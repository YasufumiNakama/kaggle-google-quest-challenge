{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Data Loading] start\n",
      "[Data Loading] done in 0 s\n",
      "[Num features] start\n",
      "[Num features] done in 1 s\n",
      "[Cat features] start\n",
      "[Cat features] done in 0 s\n",
      "[Prepare Bert config] start\n",
      "[Prepare Bert config] done in 0 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(60, 30), (6, 3), (3, 2)]\n"
     ]
    }
   ],
   "source": [
    "#===========================================================\n",
    "# Base code from https://www.kaggle.com/phoenix9032/pytorch-bert-plain\n",
    "#===========================================================\n",
    "import os\n",
    "import sys\n",
    "import gc\n",
    "import time\n",
    "import glob\n",
    "import multiprocessing\n",
    "import re\n",
    "from urllib.parse import urlparse\n",
    "from tqdm import tqdm\n",
    "from logging import getLogger, INFO, StreamHandler, FileHandler, Formatter\n",
    "from contextlib import contextmanager\n",
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "from scipy.stats import spearmanr\n",
    "import math\n",
    "from math import floor, ceil\n",
    "import random\n",
    "\n",
    "#from iterstrat.ml_stratifiers import MultilabelStratifiedShuffleSplit, MultilabelStratifiedKFold\n",
    "from sklearn.model_selection import GroupKFold\n",
    "import category_encoders as ce\n",
    "import re\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.utils import data\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\n",
    "import transformers\n",
    "from transformers import (\n",
    "    BertTokenizer, BertModel, BertForSequenceClassification, BertConfig,\n",
    "    WEIGHTS_NAME, CONFIG_NAME, AdamW, get_linear_schedule_with_warmup,\n",
    "    get_cosine_schedule_with_warmup,\n",
    ")\n",
    "from transformers.modeling_bert import BertPreTrainedModel \n",
    "\n",
    "\n",
    "#===========================================================\n",
    "# Utils\n",
    "#===========================================================\n",
    "def get_logger(filename='log'):\n",
    "    logger = getLogger(__name__)\n",
    "    logger.setLevel(INFO)\n",
    "    handler1 = StreamHandler()\n",
    "    handler1.setFormatter(Formatter(\"%(message)s\"))\n",
    "    handler2 = FileHandler(filename=f\"{filename}.log\")\n",
    "    handler2.setFormatter(Formatter(\"%(message)s\"))\n",
    "    logger.addHandler(handler1)\n",
    "    logger.addHandler(handler2)\n",
    "    return logger\n",
    "\n",
    "logger = get_logger()\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def timer(name):\n",
    "    t0 = time.time()\n",
    "    logger.info(f'[{name}] start')\n",
    "    yield\n",
    "    logger.info(f'[{name}] done in {time.time() - t0:.0f} s')\n",
    "\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "#===========================================================\n",
    "# Config\n",
    "#===========================================================\n",
    "class PipeLineConfig:\n",
    "    def __init__(self, lr, warmup, accum_steps, epochs, seed, expname, \n",
    "                 head_tail, head, freeze, question_weight, answer_weight, fold, train, cv, test):\n",
    "        self.lr = lr\n",
    "        self.warmup = warmup\n",
    "        self.accum_steps = accum_steps\n",
    "        self.epochs = epochs\n",
    "        self.seed = seed\n",
    "        self.expname = expname\n",
    "        self.head_tail = head_tail\n",
    "        self.head = head\n",
    "        self.freeze = freeze\n",
    "        self.question_weight = question_weight\n",
    "        self.answer_weight = answer_weight\n",
    "        self.fold = fold\n",
    "        self.train = train\n",
    "        self.cv = cv\n",
    "        self.test = test\n",
    "\n",
    "config = PipeLineConfig(lr=1e-4, warmup=0.1, accum_steps=1, epochs=6,\n",
    "                        seed=42, expname='uncased_6', head_tail=True, head=0.3, freeze=False,\n",
    "                        question_weight=0., answer_weight=0., fold=5, train=False, cv=False, test=True)\n",
    "\n",
    "DEBUG = False\n",
    "ID = 'qa_id'\n",
    "target_cols = ['question_asker_intent_understanding', 'question_body_critical', \n",
    "               'question_conversational', 'question_expect_short_answer', \n",
    "               'question_fact_seeking', 'question_has_commonly_accepted_answer', \n",
    "               'question_interestingness_others', 'question_interestingness_self', \n",
    "               'question_multi_intent', 'question_not_really_a_question', \n",
    "               'question_opinion_seeking', 'question_type_choice',\n",
    "               'question_type_compare', 'question_type_consequence',\n",
    "               'question_type_definition', 'question_type_entity', \n",
    "               'question_type_instructions', 'question_type_procedure', \n",
    "               'question_type_reason_explanation', 'question_type_spelling', \n",
    "               'question_well_written', 'answer_helpful',\n",
    "               'answer_level_of_information', 'answer_plausible', \n",
    "               'answer_relevance', 'answer_satisfaction', \n",
    "               'answer_type_instructions', 'answer_type_procedure', \n",
    "               'answer_type_reason_explanation', 'answer_well_written']\n",
    "NUM_FOLDS = config.fold\n",
    "ROOT = '../input/google-quest-challenge/'\n",
    "#ROOT = '../input/'\n",
    "SEED = config.seed\n",
    "seed_everything(SEED)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "MODEL_DIR = '../input/googlequestchallenge-weights1/'\n",
    "#MODEL_DIR = './'\n",
    "COMBINE_INPUT = False\n",
    "T_MAX_LEN = 30\n",
    "Q_MAX_LEN = 479 # 382\n",
    "A_MAX_LEN = 510 # 254 \n",
    "MAX_SEQUENCE_LENGTH = T_MAX_LEN + Q_MAX_LEN + A_MAX_LEN + 4\n",
    "q_max_sequence_length = T_MAX_LEN + Q_MAX_LEN + 3\n",
    "a_max_sequence_length = A_MAX_LEN + 2\n",
    "\n",
    "#===========================================================\n",
    "# Model\n",
    "#===========================================================\n",
    "class OptimizedRounder5(object):\n",
    "    def __init__(self):\n",
    "        self.coef = [0.3333, 0.5, 0.6667, 1.]\n",
    "\n",
    "    def _loss(self, X, y):\n",
    "        X_p = np.digitize(X, self.coef)\n",
    "        ll = spearmanr(y, X_p).correlation\n",
    "        return -ll\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray):\n",
    "        golden1 = 0.618\n",
    "        golden2 = 1 - golden1\n",
    "        ab_start = [(0., 0.3333), (0.3333, 0.5), (0.5, 0.6667), (0.6667, 1.)]\n",
    "        for _ in range(100):\n",
    "            search = iter(range(4))\n",
    "            for idx in search:\n",
    "                # golden section search\n",
    "                a, b = ab_start[idx]\n",
    "                # calc losses\n",
    "                self.coef[idx] = a\n",
    "                la = self._loss(X, y)\n",
    "                self.coef[idx] = b\n",
    "                lb = self._loss(X, y)\n",
    "                for it in range(4):\n",
    "                    # choose value\n",
    "                    if la > lb:\n",
    "                        a = b - (b - a) * golden1\n",
    "                        self.coef[idx] = a\n",
    "                        la = self._loss(X, y)\n",
    "                    else:\n",
    "                        b = b - (b - a) * golden2\n",
    "                        self.coef[idx] = b\n",
    "                        lb = self._loss(X, y)\n",
    "\n",
    "    def predict(self, X, coef):\n",
    "        X_p = np.digitize(X, coef)\n",
    "        return X_p\n",
    "    \n",
    "    def coefficients(self):\n",
    "        return self.coef\n",
    "\n",
    "\n",
    "def _get_masks(tokens, max_seq_length):\n",
    "    \"\"\"Mask for padding\"\"\"\n",
    "    if len(tokens)>max_seq_length:\n",
    "        print(f'len(tokens): {len(tokens)}')\n",
    "        print(f'max_seq_length: {max_seq_length}')\n",
    "        raise IndexError(\"Token length more than max seq length!\")\n",
    "    return [1]*len(tokens) + [0] * (max_seq_length - len(tokens))\n",
    "\n",
    "\n",
    "def _get_segments(tokens, max_seq_length):\n",
    "    \"\"\"Segments: 0 for the first sequence, 1 for the second\"\"\"\n",
    "    \n",
    "    if len(tokens) > max_seq_length:\n",
    "        raise IndexError(\"Token length more than max seq length!\")\n",
    "        \n",
    "    segments = []\n",
    "    first_sep = True\n",
    "    current_segment_id = 0\n",
    "    \n",
    "    for token in tokens:\n",
    "        segments.append(current_segment_id)\n",
    "        if token == \"[SEP]\":\n",
    "            if first_sep:\n",
    "                first_sep = False \n",
    "            else:\n",
    "                current_segment_id = 1\n",
    "    return segments + [0] * (max_seq_length - len(tokens))\n",
    "\n",
    "\n",
    "def _get_ids(tokens, tokenizer, max_seq_length):\n",
    "    \"\"\"Token ids from Tokenizer vocab\"\"\"\n",
    "    \n",
    "    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    input_ids = token_ids + [0] * (max_seq_length-len(token_ids))\n",
    "    return input_ids\n",
    "\n",
    "\n",
    "def _trim_input(tokenizer, title, question, answer, max_sequence_length, t_max_len, q_max_len, a_max_len):\n",
    "    \n",
    "    # 350+128+30 = 508 +4 = 512\n",
    "    \n",
    "    t = tokenizer.tokenize(title)\n",
    "    q = tokenizer.tokenize(question)\n",
    "    a = tokenizer.tokenize(answer)\n",
    "    \n",
    "    t_len = len(t)\n",
    "    q_len = len(q)\n",
    "    a_len = len(a)\n",
    "\n",
    "    if (t_len+q_len+a_len+4) > max_sequence_length:\n",
    "        \n",
    "        if t_max_len > t_len:\n",
    "            t_new_len = t_len\n",
    "            a_max_len = a_max_len + floor((t_max_len - t_len)/2)\n",
    "            q_max_len = q_max_len + ceil((t_max_len - t_len)/2)\n",
    "        else:\n",
    "            t_new_len = t_max_len\n",
    "      \n",
    "        if a_max_len > a_len:\n",
    "            a_new_len = a_len \n",
    "            q_new_len = q_max_len + (a_max_len - a_len)\n",
    "        elif q_max_len > q_len:\n",
    "            a_new_len = a_max_len + (q_max_len - q_len)\n",
    "            q_new_len = q_len\n",
    "        else:\n",
    "            a_new_len = a_max_len\n",
    "            q_new_len = q_max_len\n",
    "            \n",
    "            \n",
    "        if t_new_len+a_new_len+q_new_len+4 != max_sequence_length:\n",
    "            raise ValueError(\"New sequence length should be %d, but is %d\"%(max_sequence_length, (t_new_len + a_new_len + q_new_len + 4)))\n",
    "        # Head+Tail method \n",
    "        q_len_head = round(q_new_len * config.head)\n",
    "        q_len_tail = -1 * (q_new_len - q_len_head)\n",
    "        a_len_head = round(a_new_len * config.head)\n",
    "        a_len_tail = -1 * (a_new_len - a_len_head)\n",
    "        t_len_head = round(t_new_len * config.head)\n",
    "        t_len_tail = -1 * (t_new_len - t_len_head)  \n",
    "        #t = t[:t_new_len]\n",
    "        if config.head_tail :\n",
    "            q = q[:q_len_head]+q[q_len_tail:]\n",
    "            a = a[:a_len_head]+a[a_len_tail:]\n",
    "            #t = t[:t_len_head]+t[t_len_tail:]\n",
    "            t = t[:t_new_len]\n",
    "        else:\n",
    "            # No Head+Tail , usual processing\n",
    "            q = q[:q_new_len]\n",
    "            a = a[:a_new_len]\n",
    "            t = t[:t_new_len]\n",
    "    \n",
    "    return t, q, a\n",
    "\n",
    "\n",
    "def q_trim_input(tokenizer, title, question, q_max_sequence_length, t_max_len, q_max_len):\n",
    "\n",
    "    t = tokenizer.tokenize(title)\n",
    "    q = tokenizer.tokenize(question)\n",
    "\n",
    "    t_len = len(t)\n",
    "    q_len = len(q)\n",
    "\n",
    "    if (t_len+q_len+3) > q_max_sequence_length:\n",
    "\n",
    "        if t_max_len > t_len:\n",
    "            t_new_len = t_len\n",
    "            q_max_len = q_max_len + (t_max_len - t_len)\n",
    "        else:\n",
    "            t_new_len = t_max_len\n",
    "\n",
    "        if q_max_len > q_len:\n",
    "            q_new_len = q_len\n",
    "            t_new_len = t_max_len + (q_max_len - q_len)\n",
    "        else:\n",
    "            q_new_len = q_max_len\n",
    "\n",
    "        # Head+Tail method\n",
    "        q_len_head = round(q_new_len * config.head)\n",
    "        q_len_tail = -1 * (q_new_len - q_len_head)\n",
    "        t_len_head = round(t_new_len * config.head)\n",
    "        t_len_tail = -1 * (t_new_len - t_len_head)\n",
    "        #t = t[:t_new_len]\n",
    "        if config.head_tail :\n",
    "            q = q[:q_len_head]+q[q_len_tail:]\n",
    "            t = t[:t_len_head]+t[t_len_tail:]\n",
    "            #t = t[:t_new_len]\n",
    "        else:\n",
    "            # No Head+Tail , usual processing\n",
    "            q = q[:q_new_len]\n",
    "            t = t[:t_new_len]\n",
    "\n",
    "    return t, q\n",
    "\n",
    "\n",
    "def a_trim_input(tokenizer, answer, a_max_sequence_length, a_max_len):\n",
    "\n",
    "    a = tokenizer.tokenize(answer)\n",
    "\n",
    "    a_len = len(a)\n",
    "\n",
    "    if (a_len+2) > a_max_sequence_length:\n",
    "\n",
    "        a_new_len = a_max_len\n",
    "\n",
    "        # Head+Tail method\n",
    "        a_len_head = round(a_new_len * config.head)\n",
    "        a_len_tail = -1 * (a_new_len - a_len_head)\n",
    "        if config.head_tail :\n",
    "            a = a[:a_len_head]+a[a_len_tail:]\n",
    "        else:\n",
    "            # No Head+Tail , usual processing\n",
    "            a = a[:a_new_len]\n",
    "\n",
    "    return a\n",
    "\n",
    "\n",
    "def _convert_to_bert_inputs(title, question, answer, tokenizer, max_sequence_length):\n",
    "    \"\"\"Converts tokenized input to ids, masks and segments for BERT\"\"\"\n",
    "    if COMBINE_INPUT:\n",
    "        stoken = [\"[CLS]\"] + title + [\"[QBODY]\"] + question + [\"[ANS]\"] + answer + [\"[SEP]\"]\n",
    "        #stoken = [\"[CLS]\"] + title + [\"[SEP]\"] + question + [\"[SEP]\"] + answer + [\"[SEP]\"]\n",
    "        #stoken = [\"[CLS]\"] + title  + question  + answer + [\"[SEP]\"]\n",
    "    \n",
    "        input_ids = _get_ids(stoken, tokenizer, max_sequence_length)\n",
    "        input_masks = _get_masks(stoken, max_sequence_length)\n",
    "        input_segments = _get_segments(stoken, max_sequence_length)\n",
    "\n",
    "        return [input_ids, input_masks, input_segments]\n",
    "    else:\n",
    "        q_token = [\"[CLS]\"] + title + [\"[SEP]\"] + question + [\"[SEP\"]\n",
    "        q_input_ids = _get_ids(q_token, tokenizer, T_MAX_LEN+Q_MAX_LEN+3)\n",
    "        q_input_masks = _get_masks(q_token, T_MAX_LEN+Q_MAX_LEN+3)\n",
    "        q_input_segments = _get_segments(q_token, T_MAX_LEN+Q_MAX_LEN+3)\n",
    "        \n",
    "        a_token = [\"[CLS]\"] + answer + [\"[SEP]\"]\n",
    "        a_input_ids = _get_ids(a_token, tokenizer, A_MAX_LEN+2)\n",
    "        a_input_masks = _get_masks(a_token, A_MAX_LEN+2)\n",
    "        a_input_segments = _get_segments(a_token, A_MAX_LEN+2)\n",
    "\n",
    "        return [q_input_ids, q_input_masks, q_input_segments, a_input_ids, a_input_masks, a_input_segments]\n",
    "\n",
    "\n",
    "def compute_input_arays(df, columns, tokenizer, max_sequence_length, num_features, cat_features, \n",
    "                        t_max_len=T_MAX_LEN, q_max_len=Q_MAX_LEN, a_max_len=A_MAX_LEN):\n",
    "    if COMBINE_INPUT:\n",
    "        input_ids, input_masks, input_segments = [], [], []\n",
    "        for _, instance in df[columns].iterrows():\n",
    "            t, q, a = instance.question_title, instance.question_body, instance.answer\n",
    "            t, q, a = _trim_input(tokenizer, t, q, a, max_sequence_length, t_max_len, q_max_len, a_max_len)\n",
    "            ids, masks, segments = _convert_to_bert_inputs(t, q, a, tokenizer, max_sequence_length)\n",
    "            input_ids.append(ids)\n",
    "            input_masks.append(masks)\n",
    "            input_segments.append(segments)\n",
    "        return [\n",
    "                torch.from_numpy(np.asarray(input_ids, dtype=np.int32)).long(), \n",
    "                torch.from_numpy(np.asarray(input_masks, dtype=np.int32)).long(),\n",
    "                torch.from_numpy(np.asarray(input_segments, dtype=np.int32)).long(),\n",
    "                torch.from_numpy(np.asarray(num_features, dtype=np.float32)).float(),\n",
    "                torch.from_numpy(np.asarray(cat_features, dtype=np.int32)).long(),\n",
    "                ]\n",
    "    else:\n",
    "        q_input_ids, q_input_masks, q_input_segments = [], [], []\n",
    "        a_input_ids, a_input_masks, a_input_segments = [], [], []\n",
    "        for _, instance in df[columns].iterrows():\n",
    "            t, q, a = instance.question_title, instance.question_body, instance.answer\n",
    "            t, q = q_trim_input(tokenizer, t, q, q_max_sequence_length, t_max_len, q_max_len)\n",
    "            a = a_trim_input(tokenizer, a, a_max_sequence_length, a_max_len)\n",
    "            q_ids, q_masks, q_segments, a_ids, a_masks, a_segments = _convert_to_bert_inputs(t, q, a, tokenizer, max_sequence_length)\n",
    "            q_input_ids.append(q_ids)\n",
    "            q_input_masks.append(q_masks)\n",
    "            q_input_segments.append(q_segments)\n",
    "            a_input_ids.append(a_ids)\n",
    "            a_input_masks.append(a_masks)\n",
    "            a_input_segments.append(a_segments)\n",
    "        return [\n",
    "                torch.from_numpy(np.asarray(q_input_ids, dtype=np.int32)).long(),\n",
    "                torch.from_numpy(np.asarray(q_input_masks, dtype=np.int32)).long(),\n",
    "                torch.from_numpy(np.asarray(q_input_segments, dtype=np.int32)).long(),\n",
    "                torch.from_numpy(np.asarray(a_input_ids, dtype=np.int32)).long(),\n",
    "                torch.from_numpy(np.asarray(a_input_masks, dtype=np.int32)).long(),\n",
    "                torch.from_numpy(np.asarray(a_input_segments, dtype=np.int32)).long(),\n",
    "                torch.from_numpy(np.asarray(num_features, dtype=np.float32)).float(),\n",
    "                torch.from_numpy(np.asarray(cat_features, dtype=np.int32)).long(),\n",
    "                ]\n",
    "\n",
    "\n",
    "def compute_output_arrays(df, columns):\n",
    "    return np.asarray(df[columns])\n",
    "\n",
    "\n",
    "if COMBINE_INPUT:\n",
    "\n",
    "    class QuestDataset(torch.utils.data.Dataset):\n",
    "        def __init__(self, inputs, lengths, labels = None):\n",
    "\n",
    "            self.inputs = inputs\n",
    "            if labels is not None:\n",
    "                self.labels = labels\n",
    "            else:\n",
    "                self.labels = None\n",
    "            self.lengths = lengths\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "\n",
    "            input_ids       = self.inputs[0][idx]\n",
    "            input_masks     = self.inputs[1][idx]\n",
    "            input_segments  = self.inputs[2][idx]\n",
    "            num_features    = self.inputs[3][idx]\n",
    "            cat_features    = self.inputs[4][idx]\n",
    "            lengths         = self.lengths[idx]\n",
    "            if self.labels is not None: # targets\n",
    "                labels = self.labels[idx]\n",
    "                return input_ids, input_masks, input_segments, num_features, cat_features, labels, lengths\n",
    "            return input_ids, input_masks, input_segments, num_features, cat_features, lengths\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.inputs[0])\n",
    "\n",
    "\n",
    "    class CustomBert(BertPreTrainedModel):\n",
    "\n",
    "        def __init__(self, config, cat_dims):\n",
    "            super(CustomBert, self).__init__(config)\n",
    "            self.num_labels = config.num_labels\n",
    "            self.bert = BertModel(config)\n",
    "            self.embeddings = nn.ModuleList([\n",
    "                nn.Embedding(x, y) for x, y in cat_dims\n",
    "            ])\n",
    "            self.emb_drop = nn.Dropout(0.2)\n",
    "            n_emb_out = sum([y for x, y in cat_dims])\n",
    "            self.dropout = nn.Dropout(0.2)\n",
    "            self.classifier_final = nn.Linear(config.hidden_size+n_emb_out+4, self.config.num_labels)  # num_features=4\n",
    "\n",
    "            self.init_weights()\n",
    "\n",
    "        def forward(\n",
    "            self,\n",
    "            input_ids=None,\n",
    "            attention_mask=None,\n",
    "            token_type_ids=None,\n",
    "            num_features=None,\n",
    "            cat_features=None,\n",
    "            position_ids=None,\n",
    "            head_mask=None,\n",
    "            inputs_embeds=None,\n",
    "            labels=None,\n",
    "        ):\n",
    "\n",
    "            outputs = self.bert(\n",
    "                input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                token_type_ids=token_type_ids,\n",
    "                position_ids=position_ids,\n",
    "                head_mask=head_mask,\n",
    "                inputs_embeds=inputs_embeds,\n",
    "            )\n",
    "\n",
    "            pooled_output = outputs[1]\n",
    "            pooled_output = self.dropout(pooled_output)\n",
    "\n",
    "            emb = [\n",
    "                emb_layer(cat_features[:, j]) for j, emb_layer in enumerate(self.embeddings)\n",
    "            ]\n",
    "            emb = self.emb_drop(torch.cat(emb, 1))\n",
    "\n",
    "            pooled_output = torch.cat([pooled_output, num_features, emb], 1)\n",
    "            logits = self.classifier_final(pooled_output)\n",
    "\n",
    "            outputs = (logits,) + outputs[2:]  # add hidden states and attention if they are here\n",
    "            if labels is not None:\n",
    "                if self.num_labels == 1:\n",
    "                    #  We are doing regression\n",
    "                    loss_fct = MSELoss()\n",
    "                    loss = loss_fct(logits.view(-1), labels.view(-1))\n",
    "                else:\n",
    "                    loss_fct = CrossEntropyLoss()\n",
    "                    loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "                outputs = (loss,) + outputs\n",
    "\n",
    "            return outputs  # (loss), logits, (hidden_states), (attentions)\n",
    "\n",
    "else:\n",
    "\n",
    "    class QuestDataset(torch.utils.data.Dataset):\n",
    "        def __init__(self, inputs, lengths, labels = None):\n",
    "\n",
    "            self.inputs = inputs\n",
    "            if labels is not None:\n",
    "                self.labels = labels\n",
    "            else:\n",
    "                self.labels = None\n",
    "            self.lengths = lengths\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "\n",
    "            q_input_ids       = self.inputs[0][idx]\n",
    "            q_input_masks     = self.inputs[1][idx]\n",
    "            q_input_segments  = self.inputs[2][idx]\n",
    "            a_input_ids       = self.inputs[3][idx]\n",
    "            a_input_masks     = self.inputs[4][idx]\n",
    "            a_input_segments  = self.inputs[5][idx]\n",
    "            num_features    = self.inputs[6][idx]\n",
    "            cat_features    = self.inputs[7][idx]\n",
    "            lengths         = self.lengths[idx]\n",
    "            if self.labels is not None: # targets\n",
    "                labels = self.labels[idx]\n",
    "                return q_input_ids, q_input_masks, q_input_segments, a_input_ids, a_input_masks, a_input_segments, num_features, cat_features, labels, lengths\n",
    "            return q_input_ids, q_input_masks, q_input_segments, a_input_ids, a_input_masks, a_input_segments, num_features, cat_features, lengths\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.inputs[0])\n",
    "\n",
    "\n",
    "    class CustomBert(BertPreTrainedModel):\n",
    "\n",
    "        def __init__(self, config, cat_dims):\n",
    "            super(CustomBert, self).__init__(config)\n",
    "            self.num_labels = config.num_labels\n",
    "            self.bert = BertModel(config)\n",
    "            self.embeddings = nn.ModuleList([\n",
    "                nn.Embedding(x, y) for x, y in cat_dims\n",
    "            ])\n",
    "            self.emb_drop = nn.Dropout(0.2)\n",
    "            n_emb_out = sum([y for x, y in cat_dims])\n",
    "            self.q_dropout = nn.Dropout(0.2)\n",
    "            self.a_dropout = nn.Dropout(0.2)\n",
    "            self.classifier_final = nn.Linear(config.hidden_size*2+n_emb_out+4, self.config.num_labels)  # num_features=4\n",
    "\n",
    "            self.init_weights()\n",
    "\n",
    "        def forward(\n",
    "            self,\n",
    "            q_input_ids=None,\n",
    "            q_attention_mask=None,\n",
    "            q_token_type_ids=None,\n",
    "            a_input_ids=None,\n",
    "            a_attention_mask=None,\n",
    "            a_token_type_ids=None,\n",
    "            num_features=None,\n",
    "            cat_features=None,\n",
    "            position_ids=None,\n",
    "            head_mask=None,\n",
    "            inputs_embeds=None,\n",
    "            labels=None,\n",
    "        ):\n",
    "\n",
    "            q_outputs = self.bert(\n",
    "                q_input_ids,\n",
    "                attention_mask=q_attention_mask,\n",
    "                token_type_ids=q_token_type_ids,\n",
    "                position_ids=position_ids,\n",
    "                head_mask=head_mask,\n",
    "                inputs_embeds=inputs_embeds,\n",
    "            )\n",
    "\n",
    "            q_pooled_output = q_outputs[1]\n",
    "            q_pooled_output = self.q_dropout(q_pooled_output)\n",
    "\n",
    "            a_outputs = self.bert(\n",
    "                a_input_ids,\n",
    "                attention_mask=a_attention_mask,\n",
    "                token_type_ids=a_token_type_ids,\n",
    "                position_ids=position_ids,\n",
    "                head_mask=head_mask,\n",
    "                inputs_embeds=inputs_embeds,\n",
    "            )\n",
    "\n",
    "            a_pooled_output = a_outputs[1]\n",
    "            a_pooled_output = self.a_dropout(a_pooled_output)\n",
    "\n",
    "            emb = [\n",
    "                emb_layer(cat_features[:, j]) for j, emb_layer in enumerate(self.embeddings)\n",
    "            ]\n",
    "            emb = self.emb_drop(torch.cat(emb, 1))\n",
    "\n",
    "            pooled_output = torch.cat([q_pooled_output, a_pooled_output, num_features, emb], 1)\n",
    "            logits = self.classifier_final(pooled_output)\n",
    "\n",
    "            outputs = (logits,) + q_outputs[2:] + a_outputs[2:]  # add hidden states and attention if they are here\n",
    "            if labels is not None:\n",
    "                if self.num_labels == 1:\n",
    "                    #  We are doing regression\n",
    "                    loss_fct = MSELoss()\n",
    "                    loss = loss_fct(logits.view(-1), labels.view(-1))\n",
    "                else:\n",
    "                    loss_fct = CrossEntropyLoss()\n",
    "                    loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "                outputs = (loss,) + outputs\n",
    "\n",
    "            return outputs  # (loss), logits, (hidden_states), (attentions)\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, optimizer, criterion, scheduler, config):\n",
    "    \n",
    "    model.train()\n",
    "    avg_loss = 0.\n",
    "    avg_loss_1 = 0.\n",
    "    avg_loss_2 = 0.\n",
    "    avg_loss_3 = 0.\n",
    "    avg_loss_4 = 0.\n",
    "    avg_loss_5 = 0.\n",
    "    #tk0 = tqdm(enumerate(train_loader),total =len(train_loader))\n",
    "    optimizer.zero_grad()\n",
    "    for idx, batch in enumerate(train_loader):\n",
    "        if COMBINE_INPUT:\n",
    "            input_ids, input_masks, input_segments, num_features, cat_features, labels, _ = batch\n",
    "            input_ids, input_masks, input_segments, num_features, cat_features, labels = input_ids.to(device), input_masks.to(device), input_segments.to(device), num_features.to(device), cat_features.to(device), labels.to(device)            \n",
    "        \n",
    "            output_train = model(input_ids = input_ids.long(),\n",
    "                             labels = None,\n",
    "                             attention_mask = input_masks,\n",
    "                             token_type_ids = input_segments,\n",
    "                             num_features = num_features,\n",
    "                             cat_features = cat_features,\n",
    "                            )\n",
    "        else:\n",
    "            q_input_ids, q_input_masks, q_input_segments, a_input_ids, a_input_masks, a_input_segments, num_features, cat_features, labels, _ = batch\n",
    "            q_input_ids, q_input_masks, q_input_segments, a_input_ids, a_input_masks, a_input_segments, num_features, cat_features, labels = q_input_ids.to(device), q_input_masks.to(device), q_input_segments.to(device), a_input_ids.to(device), a_input_masks.to(device), a_input_segments.to(device), num_features.to(device), cat_features.to(device), labels.to(device)\n",
    "\n",
    "            output_train = model(q_input_ids = q_input_ids.long(),\n",
    "                             labels = None,\n",
    "                             q_attention_mask = q_input_masks,\n",
    "                             q_token_type_ids = q_input_segments,\n",
    "                             a_input_ids = a_input_ids.long(),\n",
    "                             a_attention_mask = a_input_masks,\n",
    "                             a_token_type_ids = a_input_segments,\n",
    "                             num_features = num_features,\n",
    "                             cat_features = cat_features,\n",
    "                            )\n",
    "        logits = output_train[0] #output preds\n",
    "        loss = criterion(logits, labels)\n",
    "        #loss =(config.question_weight*criterion(logits[:,0:21], labels[:,0:21]) + config.answer_weight*criterion(logits[:,21:30], labels[:,21:30]))/config.accum_steps\n",
    "        loss.backward()\n",
    "        if (idx + 1) % config.accum_steps == 0:    \n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        avg_loss += loss.item() / (len(train_loader)*config.accum_steps)\n",
    "        if COMBINE_INPUT:\n",
    "            del input_ids, input_masks, input_segments, labels\n",
    "        else:\n",
    "            del q_input_ids, q_input_masks, q_input_segments, a_input_ids, a_input_masks, a_input_segments, labels\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    return avg_loss, avg_loss_1, avg_loss_2, avg_loss_3, avg_loss_4, avg_loss_5\n",
    "\n",
    "\n",
    "def val_model(model, criterion, val_loader, val_shape, batch_size=8):\n",
    "\n",
    "    avg_val_loss = 0.\n",
    "    model.eval() # eval mode\n",
    "    \n",
    "    valid_preds = np.zeros((val_shape, len(target_cols)))\n",
    "    original = np.zeros((val_shape, len(target_cols)))\n",
    "    \n",
    "    #tk0 = tqdm(enumerate(val_loader))\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for idx, batch in enumerate(val_loader):\n",
    "            if COMBINE_INPUT:\n",
    "                input_ids, input_masks, input_segments, num_features, cat_features, labels, _ = batch\n",
    "                input_ids, input_masks, input_segments, num_features, cat_features, labels = input_ids.to(device), input_masks.to(device), input_segments.to(device), num_features.to(device), cat_features.to(device), labels.to(device)            \n",
    "            \n",
    "                output_val = model(input_ids = input_ids.long(),\n",
    "                               labels = None,\n",
    "                               attention_mask = input_masks,\n",
    "                               token_type_ids = input_segments,\n",
    "                               num_features = num_features,\n",
    "                               cat_features = cat_features,\n",
    "                              )\n",
    "            else:\n",
    "                q_input_ids, q_input_masks, q_input_segments, a_input_ids, a_input_masks, a_input_segments, num_features, cat_features, labels, _ = batch\n",
    "                q_input_ids, q_input_masks, q_input_segments, a_input_ids, a_input_masks, a_input_segments, num_features, cat_features, labels = q_input_ids.to(device), q_input_masks.to(device), q_input_segments.to(device), a_input_ids.to(device), a_input_masks.to(device), a_input_segments.to(device), num_features.to(device), cat_features.to(device), labels.to(device)\n",
    "\n",
    "                output_val = model(q_input_ids = q_input_ids.long(),\n",
    "                             labels = None,\n",
    "                             q_attention_mask = q_input_masks,\n",
    "                             q_token_type_ids = q_input_segments,\n",
    "                             a_input_ids = a_input_ids.long(),\n",
    "                             a_attention_mask = a_input_masks,\n",
    "                             a_token_type_ids = a_input_segments,\n",
    "                             num_features = num_features,\n",
    "                             cat_features = cat_features,\n",
    "                            )\n",
    "            logits = output_val[0] #output preds\n",
    "            \n",
    "            avg_val_loss += criterion(logits, labels).item() / len(val_loader)\n",
    "            valid_preds[idx*batch_size : (idx+1)*batch_size] = logits.detach().cpu().squeeze().numpy()\n",
    "            original[idx*batch_size : (idx+1)*batch_size]    = labels.detach().cpu().squeeze().numpy()\n",
    "        \n",
    "        score = 0\n",
    "        preds = torch.sigmoid(torch.tensor(valid_preds)).numpy()\n",
    "        \n",
    "        # np.save(\"preds.npy\", preds)\n",
    "        # np.save(\"actuals.npy\", original)\n",
    "        \n",
    "        rho_val = np.mean([spearmanr(original[:, i], preds[:,i]).correlation for i in range(preds.shape[1])])\n",
    "        print('\\r val_spearman-rho: %s' % (str(round(rho_val, 5))), end = 100*' '+'\\n')\n",
    "        \n",
    "        for i in range(len(target_cols)):\n",
    "            #print(i, spearmanr(original[:,i], preds[:,i]))\n",
    "            score += np.nan_to_num(spearmanr(original[:, i], preds[:, i]).correlation)\n",
    "        \n",
    "    return avg_val_loss, score/len(target_cols)\n",
    "\n",
    "\n",
    "def predict_valid_result(model, val_loader, val_length, batch_size=32):\n",
    "\n",
    "    val_preds = np.zeros((val_length, len(target_cols)))\n",
    "    original = np.zeros((val_length, len(target_cols)))\n",
    "\n",
    "    model.eval()\n",
    "    tk0 = tqdm(enumerate(val_loader))\n",
    "    for idx, batch in tk0:\n",
    "        if COMBINE_INPUT:\n",
    "            input_ids, input_masks, input_segments, num_features, cat_features, labels, _ = batch\n",
    "            input_ids, input_masks, input_segments, num_features, cat_features, labels = input_ids.to(device), input_masks.to(device), input_segments.to(device), num_features.to(device), cat_features.to(device), labels.to(device)            \n",
    "            with torch.no_grad():\n",
    "                outputs = model(input_ids = input_ids.long(),\n",
    "                            labels = None,\n",
    "                            attention_mask = input_masks,\n",
    "                            token_type_ids = input_segments,\n",
    "                            num_features = num_features,\n",
    "                            cat_features = cat_features,\n",
    "                            )\n",
    "        else:\n",
    "            q_input_ids, q_input_masks, q_input_segments, a_input_ids, a_input_masks, a_input_segments, num_features, cat_features, labels, _ = batch\n",
    "            q_input_ids, q_input_masks, q_input_segments, a_input_ids, a_input_masks, a_input_segments, num_features, cat_features, labels = q_input_ids.to(device), q_input_masks.to(device), q_input_segments.to(device), a_input_ids.to(device), a_input_masks.to(device), a_input_segments.to(device), num_features.to(device), cat_features.to(device), labels.to(device)\n",
    "            with torch.no_grad():\n",
    "                outputs = model(q_input_ids = q_input_ids.long(),\n",
    "                             labels = None,\n",
    "                             q_attention_mask = q_input_masks,\n",
    "                             q_token_type_ids = q_input_segments,\n",
    "                             a_input_ids = a_input_ids.long(),\n",
    "                             a_attention_mask = a_input_masks,\n",
    "                             a_token_type_ids = a_input_segments,\n",
    "                             num_features = num_features,\n",
    "                             cat_features = cat_features,\n",
    "                            )\n",
    "\n",
    "        predictions = outputs[0]\n",
    "        val_preds[idx*batch_size : (idx+1)*batch_size] = predictions.detach().cpu().squeeze().numpy()\n",
    "        original[idx*batch_size : (idx+1)*batch_size] = labels.detach().cpu().squeeze().numpy()\n",
    "\n",
    "    output = torch.sigmoid(torch.tensor(val_preds)).numpy()\n",
    "    return output, original\n",
    "\n",
    "\n",
    "def predict_result(model, test_loader, test_length, batch_size=32):\n",
    "\n",
    "    test_preds = np.zeros((test_length, len(target_cols)))\n",
    "\n",
    "    model.eval()\n",
    "    tk0 = tqdm(enumerate(test_loader))\n",
    "    for idx, x_batch in tk0:\n",
    "        if COMBINE_INPUT:\n",
    "            with torch.no_grad():\n",
    "                outputs = model(input_ids = x_batch[0].to(device),\n",
    "                            labels = None,\n",
    "                            attention_mask = x_batch[1].to(device),\n",
    "                            token_type_ids = x_batch[2].to(device),\n",
    "                            num_features = x_batch[3].to(device),\n",
    "                            cat_features = x_batch[4].to(device),\n",
    "                           )\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                outputs = model(q_input_ids = x_batch[0].to(device),\n",
    "                            labels = None,\n",
    "                            q_attention_mask = x_batch[1].to(device),\n",
    "                            q_token_type_ids = x_batch[2].to(device),\n",
    "                            a_input_ids = x_batch[3].to(device),\n",
    "                            a_attention_mask = x_batch[4].to(device),\n",
    "                            a_token_type_ids = x_batch[5].to(device),\n",
    "                            num_features = x_batch[6].to(device),\n",
    "                            cat_features = x_batch[7].to(device),\n",
    "                           )\n",
    "        predictions = outputs[0]\n",
    "        test_preds[idx*batch_size : (idx+1)*batch_size] = predictions.detach().cpu().squeeze().numpy()\n",
    "\n",
    "    output = torch.sigmoid(torch.tensor(test_preds)).numpy()\n",
    "    return output\n",
    "\n",
    "\n",
    "def add_features(df):\n",
    "    find = re.compile(r\"^[^.]*\")\n",
    "    df['netloc'] = df['url'].apply(lambda x: re.findall(find, urlparse(x).netloc)[0])\n",
    "    df['qa_same_user_page_flag'] = (df['question_user_page']==df['answer_user_page'])*1\n",
    "    df['question_title_num_words'] = df['question_title'].str.count('\\S+')\n",
    "    df['question_body_num_words'] = df['question_body'].str.count('\\S+')\n",
    "    df['answer_num_words'] = df['answer'].str.count('\\S+')\n",
    "    df['question_vs_answer_length'] = df['question_body_num_words']/df['answer_num_words']\n",
    "    df['question_title_num_words'] = np.log1p(df['question_title_num_words'])\n",
    "    df['question_body_num_words'] = np.log1p(df['question_body_num_words'])\n",
    "    df['answer_num_words'] = np.log1p(df['answer_num_words'])\n",
    "    df['question_vs_answer_length'] = np.log1p(df['question_vs_answer_length'])\n",
    "    return df\n",
    "\n",
    "\n",
    "def custom_loss(x, y):\n",
    "    bce_loss = nn.BCEWithLogitsLoss()(x, y)\n",
    "    return bce_loss\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "def get_bert_features(model, val_loader, val_length, batch_size=32):\n",
    "\n",
    "    features = np.zeros((val_length, 768*2))\n",
    "\n",
    "    model.eval()\n",
    "    tk0 = tqdm(enumerate(val_loader))\n",
    "    for idx, batch in tk0:\n",
    "        if COMBINE_INPUT:\n",
    "            None\n",
    "        else:\n",
    "            q_input_ids, q_input_masks, q_input_segments, a_input_ids, a_input_masks, a_input_segments, num_features, cat_features, labels, _ = batch\n",
    "            q_input_ids, q_input_masks, q_input_segments, a_input_ids, a_input_masks, a_input_segments, num_features, cat_features, labels = q_input_ids.to(device), q_input_masks.to(device), q_input_segments.to(device), a_input_ids.to(device), a_input_masks.to(device), a_input_segments.to(device), num_features.to(device), cat_features.to(device), labels.to(device)\n",
    "            with torch.no_grad():\n",
    "                q_output = model.bert(q_input_ids.long(),\n",
    "                              attention_mask=q_input_masks,\n",
    "                              token_type_ids=q_input_segments,\n",
    "                              position_ids=None,\n",
    "                              head_mask=None,\n",
    "                              inputs_embeds=None,\n",
    "                            )\n",
    "                a_output = model.bert(a_input_ids.long(),\n",
    "                              attention_mask=a_input_masks,\n",
    "                              token_type_ids=a_input_segments,\n",
    "                              position_ids=None,\n",
    "                              head_mask=None,\n",
    "                              inputs_embeds=None,\n",
    "                            )\n",
    "        q_feature = q_output[1].detach().cpu().squeeze().numpy()\n",
    "        a_feature = a_output[1].detach().cpu().squeeze().numpy()\n",
    "        features[idx*batch_size : (idx+1)*batch_size] = np.hstack([q_feature, a_feature])\n",
    "\n",
    "    return features\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def get_bert_features(model, val_loader, val_length, batch_size=32):\n",
    "\n",
    "    features = np.zeros((val_length, 768*2))\n",
    "\n",
    "    model.eval()\n",
    "    tk0 = tqdm(enumerate(val_loader))\n",
    "    for idx, batch in tk0:\n",
    "        if COMBINE_INPUT:\n",
    "            None\n",
    "        else:\n",
    "            q_input_ids, q_input_masks, q_input_segments, a_input_ids, a_input_masks, a_input_segments, num_features, cat_features, _ = batch\n",
    "            q_input_ids, q_input_masks, q_input_segments, a_input_ids, a_input_masks, a_input_segments, num_features, cat_features = q_input_ids.to(device), q_input_masks.to(device), q_input_segments.to(device), a_input_ids.to(device), a_input_masks.to(device), a_input_segments.to(device), num_features.to(device), cat_features.to(device)\n",
    "            with torch.no_grad():\n",
    "                q_output = model.bert(q_input_ids.long(),\n",
    "                              attention_mask=q_input_masks,\n",
    "                              token_type_ids=q_input_segments,\n",
    "                              position_ids=None,\n",
    "                              head_mask=None,\n",
    "                              inputs_embeds=None,\n",
    "                            )\n",
    "                a_output = model.bert(a_input_ids.long(),\n",
    "                              attention_mask=a_input_masks,\n",
    "                              token_type_ids=a_input_segments,\n",
    "                              position_ids=None,\n",
    "                              head_mask=None,\n",
    "                              inputs_embeds=None,\n",
    "                            )\n",
    "        q_feature = q_output[1].detach().cpu().squeeze().numpy()\n",
    "        a_feature = a_output[1].detach().cpu().squeeze().numpy()\n",
    "        features[idx*batch_size : (idx+1)*batch_size] = np.hstack([q_feature, a_feature])\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "#===========================================================\n",
    "# main\n",
    "#===========================================================\n",
    "if True:\n",
    "    \n",
    "    with timer('Data Loading'):\n",
    "        train = pd.read_csv(f\"{ROOT}train.csv\").fillna(\"none\")\n",
    "        y_train = train[target_cols].values\n",
    "        if config.test:\n",
    "            test = pd.read_csv(f\"{ROOT}test.csv\").fillna(\"none\")\n",
    "            submission = pd.read_csv(f\"{ROOT}sample_submission.csv\")\n",
    "    \n",
    "    with timer('Num features'):\n",
    "        train = add_features(train)\n",
    "        if config.test:\n",
    "            test = add_features(test)\n",
    "        num_features = ['question_title_num_words', 'question_body_num_words', 'answer_num_words', 'question_vs_answer_length']\n",
    "        train_num = train[num_features].values\n",
    "        if config.test:\n",
    "            test_num = test[num_features].values\n",
    "                \n",
    "    with timer('Cat features'):\n",
    "        cat_features = ['netloc', 'category', 'qa_same_user_page_flag']\n",
    "        ce_oe = ce.OrdinalEncoder(cols=cat_features, handle_unknown='return_nan')\n",
    "        ce_oe.fit(train[cat_features])\n",
    "        train_cat_df = ce_oe.transform(train[cat_features])\n",
    "        test_cat_df = ce_oe.transform(test[cat_features]).fillna(0).astype(int)\n",
    "        for c in cat_features:\n",
    "            train[c] = train_cat_df[c]\n",
    "            test[c] = test[c]\n",
    "        train_cat = train_cat_df.values\n",
    "        test_cat = test_cat_df.values\n",
    "        cat_dims = []\n",
    "        for col in cat_features:\n",
    "            dim = train[col].nunique()\n",
    "            cat_dims.append((dim+1, dim//2+1))\n",
    "        print(cat_dims)\n",
    "\n",
    "    if config.train:\n",
    "        with timer('Create folds'):\n",
    "            folds = train.copy()\n",
    "\n",
    "            kf = MultilabelStratifiedKFold(n_splits=NUM_FOLDS, random_state=SEED)\n",
    "            for fold, (train_index, val_index) in enumerate(kf.split(train.values, y_train)):\n",
    "                folds.loc[val_index, 'fold'] = int(fold)\n",
    "            folds['fold'] = folds['fold'].astype(int)\n",
    "            save_cols = [ID] + target_cols + ['fold']\n",
    "            folds[save_cols].to_csv('folds.csv', index=None)\n",
    "\n",
    "    with timer('Prepare Bert config'):\n",
    "        tokenizer = BertTokenizer.from_pretrained(\"../input/pretrained-bert-models-for-pytorch/bert-base-uncased-vocab.txt\", \n",
    "                                                  do_lower_case=True)\n",
    "        input_categories = ['question_title', 'question_body', 'answer']\n",
    "        bert_model_config = '../input/pretrained-bert-models-for-pytorch/bert-base-uncased/bert_config.json'\n",
    "        bert_config = BertConfig.from_json_file(bert_model_config)\n",
    "        bert_config.num_labels = len(target_cols)\n",
    "        bert_model = 'bert-base-uncased'\n",
    "        do_lower_case = 'uncased' in bert_model\n",
    "        output_model_file = 'bert_pytorch.bin'\n",
    "    \n",
    "    if config.train:\n",
    "\n",
    "        BATCH_SIZE = 8\n",
    "        if DEBUG:\n",
    "            epochs = 1\n",
    "        else:\n",
    "            epochs = config.epochs\n",
    "        ACCUM_STEPS = config.accum_steps\n",
    "\n",
    "        with timer('Train Bert'):\n",
    "            \n",
    "            for fold in range(NUM_FOLDS):\n",
    "\n",
    "                logger.info(f\"Current Fold: {fold}\")\n",
    "                train_index = folds[folds.fold != fold].index\n",
    "                val_index = folds[folds.fold == fold].index\n",
    "\n",
    "                train_df, val_df = train.iloc[train_index], train.iloc[val_index]\n",
    "                logger.info(f\"Train Shapes: {train_df.shape}\")\n",
    "                logger.info(f\"Valid Shapes: {val_df.shape}\")\n",
    "            \n",
    "                logger.info(\"Preparing train datasets....\")\n",
    "            \n",
    "                inputs_train = compute_input_arays(train_df, input_categories, tokenizer, max_sequence_length=MAX_SEQUENCE_LENGTH, \n",
    "                                                   num_features=train_num[train_index], cat_features=train_cat[train_index])\n",
    "                outputs_train = compute_output_arrays(train_df, columns=target_cols)\n",
    "                outputs_train = torch.tensor(outputs_train, dtype=torch.float32)\n",
    "                lengths_train = np.argmax(inputs_train[0]==0, axis=1)\n",
    "                lengths_train[lengths_train==0] = inputs_train[0].shape[1]\n",
    "            \n",
    "                logger.info(\"Preparing valid datasets....\")\n",
    "            \n",
    "                inputs_valid = compute_input_arays(val_df, input_categories, tokenizer, max_sequence_length=MAX_SEQUENCE_LENGTH, \n",
    "                                                   num_features=train_num[val_index], cat_features=train_cat[val_index])\n",
    "                outputs_valid = compute_output_arrays(val_df, columns = target_cols)\n",
    "                outputs_valid = torch.tensor(outputs_valid, dtype=torch.float32)\n",
    "                lengths_valid = np.argmax(inputs_valid[0] == 0, axis=1)\n",
    "                lengths_valid[lengths_valid == 0] = inputs_valid[0].shape[1]\n",
    "            \n",
    "                logger.info(\"Preparing Dataloaders Datasets....\")\n",
    "\n",
    "                train_set = QuestDataset(inputs=inputs_train, lengths=lengths_train, labels=outputs_train)\n",
    "                train_sampler = RandomSampler(train_set)\n",
    "                train_loader = DataLoader(train_set, batch_size=BATCH_SIZE,sampler=train_sampler)\n",
    "            \n",
    "                valid_set = QuestDataset(inputs=inputs_valid, lengths=lengths_valid, labels=outputs_valid)\n",
    "                valid_loader = DataLoader(valid_set, batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n",
    "            \n",
    "                model = CustomBert.from_pretrained('../input/pretrained-bert-models-for-pytorch/bert-base-uncased/', config=bert_config, cat_dims=cat_dims)\n",
    "                model.zero_grad()\n",
    "                model.to(device)\n",
    "                torch.cuda.empty_cache()\n",
    "                if config.freeze : ## This is basically using out of the box bert model while training only the classifier head with our data . \n",
    "                    for param in model.bert.parameters():\n",
    "                        param.requires_grad = False\n",
    "                model.train()\n",
    "            \n",
    "                i = 0\n",
    "                best_avg_loss = 100.0\n",
    "                best_score = -1.\n",
    "                best_param_loss = None\n",
    "                best_param_score = None\n",
    "                param_optimizer = list(model.named_parameters())\n",
    "                no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "                optimizer_grouped_parameters = [\n",
    "                    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "                    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "                    ]        \n",
    "                optimizer = AdamW(optimizer_grouped_parameters, lr=config.lr, eps=4e-5)\n",
    "                #optimizer = torch.optim.AdamW(model.parameters(), lr=config.lr, eps=4e-5)\n",
    "                #criterion = nn.BCEWithLogitsLoss()\n",
    "                criterion = custom_loss\n",
    "                scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=config.warmup, num_training_steps=epochs*len(train_loader)//ACCUM_STEPS)\n",
    "                logger.info(\"Training....\")\n",
    "            \n",
    "                for epoch in tqdm(range(epochs)):\n",
    "\n",
    "                    torch.cuda.empty_cache()\n",
    "                \n",
    "                    start_time   = time.time()\n",
    "                    avg_loss, avg_loss_1, avg_loss_2, avg_loss_3, avg_loss_4, avg_loss_5 = train_model(model, train_loader, optimizer, criterion, scheduler, config)\n",
    "                    avg_val_loss, score = val_model(model, criterion, valid_loader, val_shape=val_df.shape[0], batch_size=BATCH_SIZE)\n",
    "                    elapsed_time = time.time() - start_time\n",
    "\n",
    "                    logger.info('Epoch {}/{} \\t loss={:.4f} \\t val_loss={:.4f} \\t train_loss={:.4f} \\t train_loss_1={:.4f} \\t train_loss_2={:.4f} \\t train_loss_3={:.4f} \\t train_loss_4={:.4f}  \\t train_loss_5={:.4f} \\t score={:.6f} \\t time={:.2f}s'.format(\n",
    "                        epoch+1, epochs, avg_loss, avg_val_loss, avg_loss, avg_loss_1, avg_loss_2, avg_loss_3, avg_loss_4, avg_loss_5, score, elapsed_time))\n",
    "\n",
    "                    if best_avg_loss > avg_val_loss:\n",
    "                        i = 0\n",
    "                        best_avg_loss = avg_val_loss \n",
    "                        best_param_loss = model.state_dict()\n",
    "\n",
    "                    if best_score < score:\n",
    "                        best_score = score\n",
    "                        best_param_score = model.state_dict()\n",
    "                        logger.info('best_param_score_{}_{}.pt'.format(config.expname ,fold))\n",
    "                        torch.save(best_param_score, 'best_param_score_{}_{}.pt'.format(config.expname, fold))\n",
    "                    else:\n",
    "                        i += 1\n",
    "\n",
    "            del train_df, val_df, model, optimizer, criterion, scheduler\n",
    "            del valid_loader, train_loader, valid_set, train_set\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Get Bert features] start\n",
      "190it [03:30,  1.11s/it]\n",
      "190it [03:29,  1.10s/it]\n",
      "190it [03:29,  1.10s/it]\n",
      "190it [03:29,  1.10s/it]\n",
      "190it [03:29,  1.10s/it]\n",
      "[Get Bert features] done in 1598 s\n"
     ]
    }
   ],
   "source": [
    "if True:\n",
    "            \n",
    "    if True:\n",
    "        \n",
    "        with timer('Get Bert features'):\n",
    "\n",
    "            #folds = pd.read_csv(f\"{ROOT}folds.csv\")\n",
    "            #features = np.zeros((len(train), 768*2))\n",
    "\n",
    "            for fold in range(NUM_FOLDS):\n",
    "                \n",
    "                train_inputs = compute_input_arays(train, input_categories, tokenizer, max_sequence_length=MAX_SEQUENCE_LENGTH, \n",
    "                                                   num_features=train_num, cat_features=train_cat)\n",
    "                lengths_train = np.argmax(train_inputs[0] == 0, axis=1)\n",
    "                lengths_train[lengths_train == 0] = train_inputs[0].shape[1]\n",
    "                train_set = QuestDataset(inputs=train_inputs, lengths=lengths_train, labels=None)\n",
    "                train_loader  = DataLoader(train_set, batch_size=32, shuffle=False)\n",
    "                features = np.zeros((len(train), 768*2))\n",
    "                \n",
    "                model = CustomBert.from_pretrained('../input/pretrained-bert-models-for-pytorch/bert-base-uncased/', config=bert_config, cat_dims=cat_dims)\n",
    "                model.zero_grad()\n",
    "                model.to(device)\n",
    "                model.load_state_dict(torch.load(f'{MODEL_DIR}best_param_score_{config.expname}_{fold}.pt'))\n",
    "                features = get_bert_features(model, train_loader, len(train))\n",
    "                pd.DataFrame(features).to_csv(f'train_bert_features_{fold}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Get Bert features] start\n",
      "15it [00:16,  1.10s/it]\n",
      "15it [00:16,  1.09s/it]\n",
      "15it [00:16,  1.09s/it]\n",
      "15it [00:16,  1.09s/it]\n",
      "15it [00:16,  1.09s/it]\n",
      "[Get Bert features] done in 113 s\n"
     ]
    }
   ],
   "source": [
    "if True:\n",
    "                \n",
    "    if config.test:\n",
    "        \n",
    "        with timer('Get Bert features'):\n",
    "            \n",
    "            test_inputs = compute_input_arays(test, input_categories, tokenizer, max_sequence_length=MAX_SEQUENCE_LENGTH, \n",
    "                                              num_features=test_num, cat_features=test_cat)\n",
    "            lengths_test = np.argmax(test_inputs[0] == 0, axis=1)\n",
    "            lengths_test[lengths_test == 0] = test_inputs[0].shape[1]\n",
    "            test_set = QuestDataset(inputs=test_inputs, lengths=lengths_test, labels=None)\n",
    "            test_loader  = DataLoader(test_set, batch_size=32, shuffle=False)\n",
    "            #features = np.zeros((len(test), 768*2))\n",
    "\n",
    "            for fold in range(NUM_FOLDS):\n",
    "                model = CustomBert.from_pretrained('../input/pretrained-bert-models-for-pytorch/bert-base-uncased/', config=bert_config, cat_dims=cat_dims)\n",
    "                model.zero_grad()\n",
    "                model.to(device)\n",
    "                model.load_state_dict(torch.load(f'{MODEL_DIR}best_param_score_{config.expname}_{fold}.pt'))\n",
    "                features = get_bert_features(model, test_loader, len(test))\n",
    "                pd.DataFrame(features).to_csv(f'test_bert_features_{fold}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
