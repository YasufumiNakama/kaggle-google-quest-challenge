{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===========================================================\n",
    "# Base code from https://www.kaggle.com/phoenix9032/pytorch-bert-plain\n",
    "#===========================================================\n",
    "import os\n",
    "import sys\n",
    "import gc\n",
    "import time\n",
    "import glob\n",
    "import multiprocessing\n",
    "import re\n",
    "from urllib.parse import urlparse\n",
    "from tqdm import tqdm\n",
    "from logging import getLogger, INFO, StreamHandler, FileHandler, Formatter\n",
    "from contextlib import contextmanager\n",
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "from scipy.stats import spearmanr\n",
    "import math\n",
    "from math import floor, ceil\n",
    "import random\n",
    "\n",
    "#from iterstrat.ml_stratifiers import MultilabelStratifiedShuffleSplit, MultilabelStratifiedKFold\n",
    "from sklearn.model_selection import GroupKFold\n",
    "import category_encoders as ce\n",
    "import re\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.utils import data\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\n",
    "import transformers\n",
    "from transformers import (\n",
    "    BertTokenizer, BertModel, BertForSequenceClassification, BertConfig,\n",
    "    WEIGHTS_NAME, CONFIG_NAME, AdamW, get_linear_schedule_with_warmup,\n",
    "    get_cosine_schedule_with_warmup,\n",
    ")\n",
    "from transformers.modeling_bert import BertPreTrainedModel \n",
    "\n",
    "\n",
    "#===========================================================\n",
    "# Utils\n",
    "#===========================================================\n",
    "def get_logger(filename='log'):\n",
    "    logger = getLogger(__name__)\n",
    "    logger.setLevel(INFO)\n",
    "    handler1 = StreamHandler()\n",
    "    handler1.setFormatter(Formatter(\"%(message)s\"))\n",
    "    handler2 = FileHandler(filename=f\"{filename}.log\")\n",
    "    handler2.setFormatter(Formatter(\"%(message)s\"))\n",
    "    logger.addHandler(handler1)\n",
    "    logger.addHandler(handler2)\n",
    "    return logger\n",
    "\n",
    "logger = get_logger()\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def timer(name):\n",
    "    t0 = time.time()\n",
    "    logger.info(f'[{name}] start')\n",
    "    yield\n",
    "    logger.info(f'[{name}] done in {time.time() - t0:.0f} s')\n",
    "\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Data Loading] start\n",
      "[Data Loading] done in 0 s\n",
      "[Num features] start\n",
      "[Num features] done in 1 s\n",
      "[Cat features] start\n",
      "[Cat features] done in 0 s\n",
      "[Prepare Bert config] start\n",
      "[Prepare Bert config] done in 0 s\n",
      "[Inference] start\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(60, 30), (6, 3), (3, 2)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15it [00:17,  1.15s/it]\n",
      "15it [00:16,  1.09s/it]\n",
      "15it [00:16,  1.10s/it]\n",
      "15it [00:16,  1.10s/it]\n",
      "15it [00:16,  1.09s/it]\n",
      "[Inference] done in 116 s\n",
      "[Create submission.csv] start\n",
      "[Create submission.csv] done in 0 s\n",
      "[Get Bert features] start\n",
      "15it [00:16,  1.10s/it]\n",
      "15it [00:16,  1.09s/it]\n",
      "15it [00:16,  1.10s/it]\n",
      "15it [00:16,  1.09s/it]\n",
      "15it [00:16,  1.10s/it]\n",
      "[Get Bert features] done in 115 s\n",
      "[LGB] start\n",
      "[LGB] done in 170 s\n"
     ]
    }
   ],
   "source": [
    "#===========================================================\n",
    "# Config\n",
    "#===========================================================\n",
    "class PipeLineConfig:\n",
    "    def __init__(self, lr, warmup, accum_steps, epochs, seed, expname, \n",
    "                 head_tail, head, freeze, question_weight, answer_weight, fold, train, cv, test):\n",
    "        self.lr = lr\n",
    "        self.warmup = warmup\n",
    "        self.accum_steps = accum_steps\n",
    "        self.epochs = epochs\n",
    "        self.seed = seed\n",
    "        self.expname = expname\n",
    "        self.head_tail = head_tail\n",
    "        self.head = head\n",
    "        self.freeze = freeze\n",
    "        self.question_weight = question_weight\n",
    "        self.answer_weight = answer_weight\n",
    "        self.fold = fold\n",
    "        self.train = train\n",
    "        self.cv = cv\n",
    "        self.test = test\n",
    "\n",
    "config = PipeLineConfig(lr=1e-4, warmup=0.1, accum_steps=1, epochs=6,\n",
    "                        seed=42, expname='uncased_6', head_tail=True, head=0.3, freeze=False,\n",
    "                        question_weight=0., answer_weight=0., fold=5, train=False, cv=False, test=True)\n",
    "\n",
    "DEBUG = False\n",
    "ID = 'qa_id'\n",
    "target_cols = ['question_asker_intent_understanding', 'question_body_critical', \n",
    "               'question_conversational', 'question_expect_short_answer', \n",
    "               'question_fact_seeking', 'question_has_commonly_accepted_answer', \n",
    "               'question_interestingness_others', 'question_interestingness_self', \n",
    "               'question_multi_intent', 'question_not_really_a_question', \n",
    "               'question_opinion_seeking', 'question_type_choice',\n",
    "               'question_type_compare', 'question_type_consequence',\n",
    "               'question_type_definition', 'question_type_entity', \n",
    "               'question_type_instructions', 'question_type_procedure', \n",
    "               'question_type_reason_explanation', 'question_type_spelling', \n",
    "               'question_well_written', 'answer_helpful',\n",
    "               'answer_level_of_information', 'answer_plausible', \n",
    "               'answer_relevance', 'answer_satisfaction', \n",
    "               'answer_type_instructions', 'answer_type_procedure', \n",
    "               'answer_type_reason_explanation', 'answer_well_written']\n",
    "NUM_FOLDS = config.fold\n",
    "ROOT = '../input/google-quest-challenge/'\n",
    "#ROOT = '../input/'\n",
    "SEED = config.seed\n",
    "seed_everything(SEED)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "MODEL_DIR = '../input/googlequestchallenge-weights1/'\n",
    "#MODEL_DIR = './'\n",
    "COMBINE_INPUT = False\n",
    "T_MAX_LEN = 30\n",
    "Q_MAX_LEN = 479 # 382\n",
    "A_MAX_LEN = 510 # 254 \n",
    "MAX_SEQUENCE_LENGTH = T_MAX_LEN + Q_MAX_LEN + A_MAX_LEN + 4\n",
    "q_max_sequence_length = T_MAX_LEN + Q_MAX_LEN + 3\n",
    "a_max_sequence_length = A_MAX_LEN + 2\n",
    "\n",
    "#===========================================================\n",
    "# Model\n",
    "#===========================================================\n",
    "class OptimizedRounder5(object):\n",
    "    def __init__(self):\n",
    "        self.coef = [0.3333, 0.5, 0.6667, 1.]\n",
    "\n",
    "    def _loss(self, X, y):\n",
    "        X_p = np.digitize(X, self.coef)\n",
    "        ll = spearmanr(y, X_p).correlation\n",
    "        return -ll\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray):\n",
    "        golden1 = 0.618\n",
    "        golden2 = 1 - golden1\n",
    "        ab_start = [(0., 0.3333), (0.3333, 0.5), (0.5, 0.6667), (0.6667, 1.)]\n",
    "        for _ in range(100):\n",
    "            search = iter(range(4))\n",
    "            for idx in search:\n",
    "                # golden section search\n",
    "                a, b = ab_start[idx]\n",
    "                # calc losses\n",
    "                self.coef[idx] = a\n",
    "                la = self._loss(X, y)\n",
    "                self.coef[idx] = b\n",
    "                lb = self._loss(X, y)\n",
    "                for it in range(4):\n",
    "                    # choose value\n",
    "                    if la > lb:\n",
    "                        a = b - (b - a) * golden1\n",
    "                        self.coef[idx] = a\n",
    "                        la = self._loss(X, y)\n",
    "                    else:\n",
    "                        b = b - (b - a) * golden2\n",
    "                        self.coef[idx] = b\n",
    "                        lb = self._loss(X, y)\n",
    "\n",
    "    def predict(self, X, coef):\n",
    "        X_p = np.digitize(X, coef)\n",
    "        return X_p\n",
    "    \n",
    "    def coefficients(self):\n",
    "        return self.coef\n",
    "\n",
    "\n",
    "class OptimizedRounder9(object):\n",
    "    def __init__(self):\n",
    "        self.coef = [0.3333, 0.4444, 0.5, 0.5555, 0.6667, 0.7777, 0.8333, 0.8889]\n",
    "\n",
    "    def _loss(self, X, y):\n",
    "        X_p = np.digitize(X, self.coef)\n",
    "        ll = spearmanr(y, X_p).correlation\n",
    "        return -ll\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray):\n",
    "        golden1 = 0.618\n",
    "        golden2 = 1 - golden1\n",
    "        ab_start = [(0, 0.3333), (0.3333, 0.4444), (0.4444, 0.5), (0.5, 0.5555), (0.5555, 0.6667), \n",
    "                    (0.6667, 0.7777), (0.7777, 0.8333), (0.8333, 1)]\n",
    "        for _ in range(100):\n",
    "            search = iter(range(8))\n",
    "            for idx in search:\n",
    "                # golden section search\n",
    "                a, b = ab_start[idx]\n",
    "                # calc losses\n",
    "                self.coef[idx] = a\n",
    "                la = self._loss(X, y)\n",
    "                self.coef[idx] = b\n",
    "                lb = self._loss(X, y)\n",
    "                for it in range(8):\n",
    "                    # choose value\n",
    "                    if la > lb:\n",
    "                        a = b - (b - a) * golden1\n",
    "                        self.coef[idx] = a\n",
    "                        la = self._loss(X, y)\n",
    "                    else:\n",
    "                        b = b - (b - a) * golden2\n",
    "                        self.coef[idx] = b\n",
    "                        lb = self._loss(X, y)\n",
    "\n",
    "    def predict(self, X, coef):\n",
    "        X_p = np.digitize(X, coef)\n",
    "        return X_p\n",
    "    \n",
    "    def coefficients(self):\n",
    "        return self.coef\n",
    "\n",
    "    \n",
    "class OptimizedRounder3(object):\n",
    "    def __init__(self):\n",
    "        self.coef = [0.01, 0.02]\n",
    "\n",
    "    def _loss(self, X, y):\n",
    "        X_p = np.digitize(X, self.coef)\n",
    "        ll = spearmanr(y, X_p).correlation\n",
    "        return -ll\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray):\n",
    "        golden1 = 0.618\n",
    "        golden2 = 1 - golden1\n",
    "        ab_start = [(0., 0.1), (0.1, 0.3)]\n",
    "        for _ in range(100):\n",
    "            search = iter(range(2))\n",
    "            for idx in search:\n",
    "                # golden section search\n",
    "                a, b = ab_start[idx]\n",
    "                # calc losses\n",
    "                self.coef[idx] = a\n",
    "                la = self._loss(X, y)\n",
    "                self.coef[idx] = b\n",
    "                lb = self._loss(X, y)\n",
    "                for it in range(2):\n",
    "                    # choose value\n",
    "                    if la > lb:\n",
    "                        a = b - (b - a) * golden1\n",
    "                        self.coef[idx] = a\n",
    "                        la = self._loss(X, y)\n",
    "                    else:\n",
    "                        b = b - (b - a) * golden2\n",
    "                        self.coef[idx] = b\n",
    "                        lb = self._loss(X, y)\n",
    "\n",
    "    def predict(self, X, coef):\n",
    "        X_p = np.digitize(X, coef)\n",
    "        return X_p\n",
    "    \n",
    "    def coefficients(self):\n",
    "        return self.coef\n",
    "\n",
    "\n",
    "def _get_masks(tokens, max_seq_length):\n",
    "    \"\"\"Mask for padding\"\"\"\n",
    "    if len(tokens)>max_seq_length:\n",
    "        print(f'len(tokens): {len(tokens)}')\n",
    "        print(f'max_seq_length: {max_seq_length}')\n",
    "        raise IndexError(\"Token length more than max seq length!\")\n",
    "    return [1]*len(tokens) + [0] * (max_seq_length - len(tokens))\n",
    "\n",
    "\n",
    "def _get_segments(tokens, max_seq_length):\n",
    "    \"\"\"Segments: 0 for the first sequence, 1 for the second\"\"\"\n",
    "    \n",
    "    if len(tokens) > max_seq_length:\n",
    "        raise IndexError(\"Token length more than max seq length!\")\n",
    "        \n",
    "    segments = []\n",
    "    first_sep = True\n",
    "    current_segment_id = 0\n",
    "    \n",
    "    for token in tokens:\n",
    "        segments.append(current_segment_id)\n",
    "        if token == \"[SEP]\":\n",
    "            if first_sep:\n",
    "                first_sep = False \n",
    "            else:\n",
    "                current_segment_id = 1\n",
    "    return segments + [0] * (max_seq_length - len(tokens))\n",
    "\n",
    "\n",
    "def _get_ids(tokens, tokenizer, max_seq_length):\n",
    "    \"\"\"Token ids from Tokenizer vocab\"\"\"\n",
    "    \n",
    "    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    input_ids = token_ids + [0] * (max_seq_length-len(token_ids))\n",
    "    return input_ids\n",
    "\n",
    "\n",
    "def _trim_input(tokenizer, title, question, answer, max_sequence_length, t_max_len, q_max_len, a_max_len):\n",
    "    \n",
    "    # 350+128+30 = 508 +4 = 512\n",
    "    \n",
    "    t = tokenizer.tokenize(title)\n",
    "    q = tokenizer.tokenize(question)\n",
    "    a = tokenizer.tokenize(answer)\n",
    "    \n",
    "    t_len = len(t)\n",
    "    q_len = len(q)\n",
    "    a_len = len(a)\n",
    "\n",
    "    if (t_len+q_len+a_len+4) > max_sequence_length:\n",
    "        \n",
    "        if t_max_len > t_len:\n",
    "            t_new_len = t_len\n",
    "            a_max_len = a_max_len + floor((t_max_len - t_len)/2)\n",
    "            q_max_len = q_max_len + ceil((t_max_len - t_len)/2)\n",
    "        else:\n",
    "            t_new_len = t_max_len\n",
    "      \n",
    "        if a_max_len > a_len:\n",
    "            a_new_len = a_len \n",
    "            q_new_len = q_max_len + (a_max_len - a_len)\n",
    "        elif q_max_len > q_len:\n",
    "            a_new_len = a_max_len + (q_max_len - q_len)\n",
    "            q_new_len = q_len\n",
    "        else:\n",
    "            a_new_len = a_max_len\n",
    "            q_new_len = q_max_len\n",
    "            \n",
    "            \n",
    "        if t_new_len+a_new_len+q_new_len+4 != max_sequence_length:\n",
    "            raise ValueError(\"New sequence length should be %d, but is %d\"%(max_sequence_length, (t_new_len + a_new_len + q_new_len + 4)))\n",
    "        # Head+Tail method \n",
    "        q_len_head = round(q_new_len * config.head)\n",
    "        q_len_tail = -1 * (q_new_len - q_len_head)\n",
    "        a_len_head = round(a_new_len * config.head)\n",
    "        a_len_tail = -1 * (a_new_len - a_len_head)\n",
    "        t_len_head = round(t_new_len * config.head)\n",
    "        t_len_tail = -1 * (t_new_len - t_len_head)  \n",
    "        #t = t[:t_new_len]\n",
    "        if config.head_tail :\n",
    "            q = q[:q_len_head]+q[q_len_tail:]\n",
    "            a = a[:a_len_head]+a[a_len_tail:]\n",
    "            #t = t[:t_len_head]+t[t_len_tail:]\n",
    "            t = t[:t_new_len]\n",
    "        else:\n",
    "            # No Head+Tail , usual processing\n",
    "            q = q[:q_new_len]\n",
    "            a = a[:a_new_len]\n",
    "            t = t[:t_new_len]\n",
    "    \n",
    "    return t, q, a\n",
    "\n",
    "\n",
    "def q_trim_input(tokenizer, title, question, q_max_sequence_length, t_max_len, q_max_len):\n",
    "\n",
    "    t = tokenizer.tokenize(title)\n",
    "    q = tokenizer.tokenize(question)\n",
    "\n",
    "    t_len = len(t)\n",
    "    q_len = len(q)\n",
    "\n",
    "    if (t_len+q_len+3) > q_max_sequence_length:\n",
    "\n",
    "        if t_max_len > t_len:\n",
    "            t_new_len = t_len\n",
    "            q_max_len = q_max_len + (t_max_len - t_len)\n",
    "        else:\n",
    "            t_new_len = t_max_len\n",
    "\n",
    "        if q_max_len > q_len:\n",
    "            q_new_len = q_len\n",
    "            t_new_len = t_max_len + (q_max_len - q_len)\n",
    "        else:\n",
    "            q_new_len = q_max_len\n",
    "\n",
    "        # Head+Tail method\n",
    "        q_len_head = round(q_new_len * config.head)\n",
    "        q_len_tail = -1 * (q_new_len - q_len_head)\n",
    "        t_len_head = round(t_new_len * config.head)\n",
    "        t_len_tail = -1 * (t_new_len - t_len_head)\n",
    "        #t = t[:t_new_len]\n",
    "        if config.head_tail :\n",
    "            q = q[:q_len_head]+q[q_len_tail:]\n",
    "            t = t[:t_len_head]+t[t_len_tail:]\n",
    "            #t = t[:t_new_len]\n",
    "        else:\n",
    "            # No Head+Tail , usual processing\n",
    "            q = q[:q_new_len]\n",
    "            t = t[:t_new_len]\n",
    "\n",
    "    return t, q\n",
    "\n",
    "\n",
    "def a_trim_input(tokenizer, answer, a_max_sequence_length, a_max_len):\n",
    "\n",
    "    a = tokenizer.tokenize(answer)\n",
    "\n",
    "    a_len = len(a)\n",
    "\n",
    "    if (a_len+2) > a_max_sequence_length:\n",
    "\n",
    "        a_new_len = a_max_len\n",
    "\n",
    "        # Head+Tail method\n",
    "        a_len_head = round(a_new_len * config.head)\n",
    "        a_len_tail = -1 * (a_new_len - a_len_head)\n",
    "        if config.head_tail :\n",
    "            a = a[:a_len_head]+a[a_len_tail:]\n",
    "        else:\n",
    "            # No Head+Tail , usual processing\n",
    "            a = a[:a_new_len]\n",
    "\n",
    "    return a\n",
    "\n",
    "\n",
    "def _convert_to_bert_inputs(title, question, answer, tokenizer, max_sequence_length):\n",
    "    \"\"\"Converts tokenized input to ids, masks and segments for BERT\"\"\"\n",
    "    if COMBINE_INPUT:\n",
    "        stoken = [\"[CLS]\"] + title + [\"[QBODY]\"] + question + [\"[ANS]\"] + answer + [\"[SEP]\"]\n",
    "        #stoken = [\"[CLS]\"] + title + [\"[SEP]\"] + question + [\"[SEP]\"] + answer + [\"[SEP]\"]\n",
    "        #stoken = [\"[CLS]\"] + title  + question  + answer + [\"[SEP]\"]\n",
    "    \n",
    "        input_ids = _get_ids(stoken, tokenizer, max_sequence_length)\n",
    "        input_masks = _get_masks(stoken, max_sequence_length)\n",
    "        input_segments = _get_segments(stoken, max_sequence_length)\n",
    "\n",
    "        return [input_ids, input_masks, input_segments]\n",
    "    else:\n",
    "        q_token = [\"[CLS]\"] + title + [\"[SEP]\"] + question + [\"[SEP\"]\n",
    "        q_input_ids = _get_ids(q_token, tokenizer, T_MAX_LEN+Q_MAX_LEN+3)\n",
    "        q_input_masks = _get_masks(q_token, T_MAX_LEN+Q_MAX_LEN+3)\n",
    "        q_input_segments = _get_segments(q_token, T_MAX_LEN+Q_MAX_LEN+3)\n",
    "        \n",
    "        a_token = [\"[CLS]\"] + answer + [\"[SEP]\"]\n",
    "        a_input_ids = _get_ids(a_token, tokenizer, A_MAX_LEN+2)\n",
    "        a_input_masks = _get_masks(a_token, A_MAX_LEN+2)\n",
    "        a_input_segments = _get_segments(a_token, A_MAX_LEN+2)\n",
    "\n",
    "        return [q_input_ids, q_input_masks, q_input_segments, a_input_ids, a_input_masks, a_input_segments]\n",
    "\n",
    "\n",
    "def compute_input_arays(df, columns, tokenizer, max_sequence_length, num_features, cat_features, \n",
    "                        t_max_len=T_MAX_LEN, q_max_len=Q_MAX_LEN, a_max_len=A_MAX_LEN):\n",
    "    if COMBINE_INPUT:\n",
    "        input_ids, input_masks, input_segments = [], [], []\n",
    "        for _, instance in df[columns].iterrows():\n",
    "            t, q, a = instance.question_title, instance.question_body, instance.answer\n",
    "            t, q, a = _trim_input(tokenizer, t, q, a, max_sequence_length, t_max_len, q_max_len, a_max_len)\n",
    "            ids, masks, segments = _convert_to_bert_inputs(t, q, a, tokenizer, max_sequence_length)\n",
    "            input_ids.append(ids)\n",
    "            input_masks.append(masks)\n",
    "            input_segments.append(segments)\n",
    "        return [\n",
    "                torch.from_numpy(np.asarray(input_ids, dtype=np.int32)).long(), \n",
    "                torch.from_numpy(np.asarray(input_masks, dtype=np.int32)).long(),\n",
    "                torch.from_numpy(np.asarray(input_segments, dtype=np.int32)).long(),\n",
    "                torch.from_numpy(np.asarray(num_features, dtype=np.float32)).float(),\n",
    "                torch.from_numpy(np.asarray(cat_features, dtype=np.int32)).long(),\n",
    "                ]\n",
    "    else:\n",
    "        q_input_ids, q_input_masks, q_input_segments = [], [], []\n",
    "        a_input_ids, a_input_masks, a_input_segments = [], [], []\n",
    "        for _, instance in df[columns].iterrows():\n",
    "            t, q, a = instance.question_title, instance.question_body, instance.answer\n",
    "            t, q = q_trim_input(tokenizer, t, q, q_max_sequence_length, t_max_len, q_max_len)\n",
    "            a = a_trim_input(tokenizer, a, a_max_sequence_length, a_max_len)\n",
    "            q_ids, q_masks, q_segments, a_ids, a_masks, a_segments = _convert_to_bert_inputs(t, q, a, tokenizer, max_sequence_length)\n",
    "            q_input_ids.append(q_ids)\n",
    "            q_input_masks.append(q_masks)\n",
    "            q_input_segments.append(q_segments)\n",
    "            a_input_ids.append(a_ids)\n",
    "            a_input_masks.append(a_masks)\n",
    "            a_input_segments.append(a_segments)\n",
    "        return [\n",
    "                torch.from_numpy(np.asarray(q_input_ids, dtype=np.int32)).long(),\n",
    "                torch.from_numpy(np.asarray(q_input_masks, dtype=np.int32)).long(),\n",
    "                torch.from_numpy(np.asarray(q_input_segments, dtype=np.int32)).long(),\n",
    "                torch.from_numpy(np.asarray(a_input_ids, dtype=np.int32)).long(),\n",
    "                torch.from_numpy(np.asarray(a_input_masks, dtype=np.int32)).long(),\n",
    "                torch.from_numpy(np.asarray(a_input_segments, dtype=np.int32)).long(),\n",
    "                torch.from_numpy(np.asarray(num_features, dtype=np.float32)).float(),\n",
    "                torch.from_numpy(np.asarray(cat_features, dtype=np.int32)).long(),\n",
    "                ]\n",
    "\n",
    "\n",
    "def compute_output_arrays(df, columns):\n",
    "    return np.asarray(df[columns])\n",
    "\n",
    "\n",
    "if COMBINE_INPUT:\n",
    "\n",
    "    class QuestDataset(torch.utils.data.Dataset):\n",
    "        def __init__(self, inputs, lengths, labels = None):\n",
    "\n",
    "            self.inputs = inputs\n",
    "            if labels is not None:\n",
    "                self.labels = labels\n",
    "            else:\n",
    "                self.labels = None\n",
    "            self.lengths = lengths\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "\n",
    "            input_ids       = self.inputs[0][idx]\n",
    "            input_masks     = self.inputs[1][idx]\n",
    "            input_segments  = self.inputs[2][idx]\n",
    "            num_features    = self.inputs[3][idx]\n",
    "            cat_features    = self.inputs[4][idx]\n",
    "            lengths         = self.lengths[idx]\n",
    "            if self.labels is not None: # targets\n",
    "                labels = self.labels[idx]\n",
    "                return input_ids, input_masks, input_segments, num_features, cat_features, labels, lengths\n",
    "            return input_ids, input_masks, input_segments, num_features, cat_features, lengths\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.inputs[0])\n",
    "\n",
    "\n",
    "    class CustomBert(BertPreTrainedModel):\n",
    "\n",
    "        def __init__(self, config, cat_dims):\n",
    "            super(CustomBert, self).__init__(config)\n",
    "            self.num_labels = config.num_labels\n",
    "            self.bert = BertModel(config)\n",
    "            self.embeddings = nn.ModuleList([\n",
    "                nn.Embedding(x, y) for x, y in cat_dims\n",
    "            ])\n",
    "            self.emb_drop = nn.Dropout(0.2)\n",
    "            n_emb_out = sum([y for x, y in cat_dims])\n",
    "            self.dropout = nn.Dropout(0.2)\n",
    "            self.classifier_final = nn.Linear(config.hidden_size+n_emb_out+4, self.config.num_labels)  # num_features=4\n",
    "\n",
    "            self.init_weights()\n",
    "\n",
    "        def forward(\n",
    "            self,\n",
    "            input_ids=None,\n",
    "            attention_mask=None,\n",
    "            token_type_ids=None,\n",
    "            num_features=None,\n",
    "            cat_features=None,\n",
    "            position_ids=None,\n",
    "            head_mask=None,\n",
    "            inputs_embeds=None,\n",
    "            labels=None,\n",
    "        ):\n",
    "\n",
    "            outputs = self.bert(\n",
    "                input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                token_type_ids=token_type_ids,\n",
    "                position_ids=position_ids,\n",
    "                head_mask=head_mask,\n",
    "                inputs_embeds=inputs_embeds,\n",
    "            )\n",
    "\n",
    "            pooled_output = outputs[1]\n",
    "            pooled_output = self.dropout(pooled_output)\n",
    "\n",
    "            emb = [\n",
    "                emb_layer(cat_features[:, j]) for j, emb_layer in enumerate(self.embeddings)\n",
    "            ]\n",
    "            emb = self.emb_drop(torch.cat(emb, 1))\n",
    "\n",
    "            pooled_output = torch.cat([pooled_output, num_features, emb], 1)\n",
    "            logits = self.classifier_final(pooled_output)\n",
    "\n",
    "            outputs = (logits,) + outputs[2:]  # add hidden states and attention if they are here\n",
    "            if labels is not None:\n",
    "                if self.num_labels == 1:\n",
    "                    #  We are doing regression\n",
    "                    loss_fct = MSELoss()\n",
    "                    loss = loss_fct(logits.view(-1), labels.view(-1))\n",
    "                else:\n",
    "                    loss_fct = CrossEntropyLoss()\n",
    "                    loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "                outputs = (loss,) + outputs\n",
    "\n",
    "            return outputs  # (loss), logits, (hidden_states), (attentions)\n",
    "\n",
    "else:\n",
    "\n",
    "    class QuestDataset(torch.utils.data.Dataset):\n",
    "        def __init__(self, inputs, lengths, labels = None):\n",
    "\n",
    "            self.inputs = inputs\n",
    "            if labels is not None:\n",
    "                self.labels = labels\n",
    "            else:\n",
    "                self.labels = None\n",
    "            self.lengths = lengths\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "\n",
    "            q_input_ids       = self.inputs[0][idx]\n",
    "            q_input_masks     = self.inputs[1][idx]\n",
    "            q_input_segments  = self.inputs[2][idx]\n",
    "            a_input_ids       = self.inputs[3][idx]\n",
    "            a_input_masks     = self.inputs[4][idx]\n",
    "            a_input_segments  = self.inputs[5][idx]\n",
    "            num_features    = self.inputs[6][idx]\n",
    "            cat_features    = self.inputs[7][idx]\n",
    "            lengths         = self.lengths[idx]\n",
    "            if self.labels is not None: # targets\n",
    "                labels = self.labels[idx]\n",
    "                return q_input_ids, q_input_masks, q_input_segments, a_input_ids, a_input_masks, a_input_segments, num_features, cat_features, labels, lengths\n",
    "            return q_input_ids, q_input_masks, q_input_segments, a_input_ids, a_input_masks, a_input_segments, num_features, cat_features, lengths\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.inputs[0])\n",
    "\n",
    "\n",
    "    class CustomBert(BertPreTrainedModel):\n",
    "\n",
    "        def __init__(self, config, cat_dims):\n",
    "            super(CustomBert, self).__init__(config)\n",
    "            self.num_labels = config.num_labels\n",
    "            self.bert = BertModel(config)\n",
    "            self.embeddings = nn.ModuleList([\n",
    "                nn.Embedding(x, y) for x, y in cat_dims\n",
    "            ])\n",
    "            self.emb_drop = nn.Dropout(0.2)\n",
    "            n_emb_out = sum([y for x, y in cat_dims])\n",
    "            self.q_dropout = nn.Dropout(0.2)\n",
    "            self.a_dropout = nn.Dropout(0.2)\n",
    "            self.classifier_final = nn.Linear(config.hidden_size*2+n_emb_out+4, self.config.num_labels)  # num_features=4\n",
    "\n",
    "            self.init_weights()\n",
    "\n",
    "        def forward(\n",
    "            self,\n",
    "            q_input_ids=None,\n",
    "            q_attention_mask=None,\n",
    "            q_token_type_ids=None,\n",
    "            a_input_ids=None,\n",
    "            a_attention_mask=None,\n",
    "            a_token_type_ids=None,\n",
    "            num_features=None,\n",
    "            cat_features=None,\n",
    "            position_ids=None,\n",
    "            head_mask=None,\n",
    "            inputs_embeds=None,\n",
    "            labels=None,\n",
    "        ):\n",
    "\n",
    "            q_outputs = self.bert(\n",
    "                q_input_ids,\n",
    "                attention_mask=q_attention_mask,\n",
    "                token_type_ids=q_token_type_ids,\n",
    "                position_ids=position_ids,\n",
    "                head_mask=head_mask,\n",
    "                inputs_embeds=inputs_embeds,\n",
    "            )\n",
    "\n",
    "            q_pooled_output = q_outputs[1]\n",
    "            q_pooled_output = self.q_dropout(q_pooled_output)\n",
    "\n",
    "            a_outputs = self.bert(\n",
    "                a_input_ids,\n",
    "                attention_mask=a_attention_mask,\n",
    "                token_type_ids=a_token_type_ids,\n",
    "                position_ids=position_ids,\n",
    "                head_mask=head_mask,\n",
    "                inputs_embeds=inputs_embeds,\n",
    "            )\n",
    "\n",
    "            a_pooled_output = a_outputs[1]\n",
    "            a_pooled_output = self.a_dropout(a_pooled_output)\n",
    "\n",
    "            emb = [\n",
    "                emb_layer(cat_features[:, j]) for j, emb_layer in enumerate(self.embeddings)\n",
    "            ]\n",
    "            emb = self.emb_drop(torch.cat(emb, 1))\n",
    "\n",
    "            pooled_output = torch.cat([q_pooled_output, a_pooled_output, num_features, emb], 1)\n",
    "            logits = self.classifier_final(pooled_output)\n",
    "\n",
    "            outputs = (logits,) + q_outputs[2:] + a_outputs[2:]  # add hidden states and attention if they are here\n",
    "            if labels is not None:\n",
    "                if self.num_labels == 1:\n",
    "                    #  We are doing regression\n",
    "                    loss_fct = MSELoss()\n",
    "                    loss = loss_fct(logits.view(-1), labels.view(-1))\n",
    "                else:\n",
    "                    loss_fct = CrossEntropyLoss()\n",
    "                    loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "                outputs = (loss,) + outputs\n",
    "\n",
    "            return outputs  # (loss), logits, (hidden_states), (attentions)\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, optimizer, criterion, scheduler, config):\n",
    "    \n",
    "    model.train()\n",
    "    avg_loss = 0.\n",
    "    avg_loss_1 = 0.\n",
    "    avg_loss_2 = 0.\n",
    "    avg_loss_3 = 0.\n",
    "    avg_loss_4 = 0.\n",
    "    avg_loss_5 = 0.\n",
    "    #tk0 = tqdm(enumerate(train_loader),total =len(train_loader))\n",
    "    optimizer.zero_grad()\n",
    "    for idx, batch in enumerate(train_loader):\n",
    "        if COMBINE_INPUT:\n",
    "            input_ids, input_masks, input_segments, num_features, cat_features, labels, _ = batch\n",
    "            input_ids, input_masks, input_segments, num_features, cat_features, labels = input_ids.to(device), input_masks.to(device), input_segments.to(device), num_features.to(device), cat_features.to(device), labels.to(device)            \n",
    "        \n",
    "            output_train = model(input_ids = input_ids.long(),\n",
    "                             labels = None,\n",
    "                             attention_mask = input_masks,\n",
    "                             token_type_ids = input_segments,\n",
    "                             num_features = num_features,\n",
    "                             cat_features = cat_features,\n",
    "                            )\n",
    "        else:\n",
    "            q_input_ids, q_input_masks, q_input_segments, a_input_ids, a_input_masks, a_input_segments, num_features, cat_features, labels, _ = batch\n",
    "            q_input_ids, q_input_masks, q_input_segments, a_input_ids, a_input_masks, a_input_segments, num_features, cat_features, labels = q_input_ids.to(device), q_input_masks.to(device), q_input_segments.to(device), a_input_ids.to(device), a_input_masks.to(device), a_input_segments.to(device), num_features.to(device), cat_features.to(device), labels.to(device)\n",
    "\n",
    "            output_train = model(q_input_ids = q_input_ids.long(),\n",
    "                             labels = None,\n",
    "                             q_attention_mask = q_input_masks,\n",
    "                             q_token_type_ids = q_input_segments,\n",
    "                             a_input_ids = a_input_ids.long(),\n",
    "                             a_attention_mask = a_input_masks,\n",
    "                             a_token_type_ids = a_input_segments,\n",
    "                             num_features = num_features,\n",
    "                             cat_features = cat_features,\n",
    "                            )\n",
    "        logits = output_train[0] #output preds\n",
    "        \"\"\"\n",
    "        loss1 = criterion(logits[:,0:9], labels[:,0:9])\n",
    "        loss2 = criterion(logits[:,9:10], labels[:,9:10])\n",
    "        loss3 = criterion(logits[:,10:21], labels[:,10:21])\n",
    "        loss4 = criterion(logits[:,21:26], labels[:,21:26])\n",
    "        loss5 = criterion(logits[:,26:30], labels[:,26:30])\n",
    "        loss = config.question_weight*loss1+config.answer_weight*loss2+config.question_weight*loss3+config.answer_weight*loss4+config.question_weight*loss5\n",
    "            \"\"\"\n",
    "        loss = criterion(logits, labels)\n",
    "        #loss =(config.question_weight*criterion(logits[:,0:21], labels[:,0:21]) + config.answer_weight*criterion(logits[:,21:30], labels[:,21:30]))/config.accum_steps\n",
    "        loss.backward()\n",
    "        if (idx + 1) % config.accum_steps == 0:    \n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        avg_loss += loss.item() / (len(train_loader)*config.accum_steps)\n",
    "        \"\"\"\n",
    "        avg_loss_1 += loss1.item() / (len(train_loader)*config.accum_steps)\n",
    "        avg_loss_2 += loss2.item() / (len(train_loader)*config.accum_steps)\n",
    "        avg_loss_3 += loss3.item() / (len(train_loader)*config.accum_steps)\n",
    "        avg_loss_4 += loss4.item() / (len(train_loader)*config.accum_steps)\n",
    "        avg_loss_5 += loss5.item() / (len(train_loader)*config.accum_steps)\n",
    "        \"\"\"\n",
    "        if COMBINE_INPUT:\n",
    "            del input_ids, input_masks, input_segments, labels\n",
    "        else:\n",
    "            del q_input_ids, q_input_masks, q_input_segments, a_input_ids, a_input_masks, a_input_segments, labels\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    return avg_loss, avg_loss_1, avg_loss_2, avg_loss_3, avg_loss_4, avg_loss_5\n",
    "\n",
    "\n",
    "def val_model(model, criterion, val_loader, val_shape, batch_size=8):\n",
    "\n",
    "    avg_val_loss = 0.\n",
    "    model.eval() # eval mode\n",
    "    \n",
    "    valid_preds = np.zeros((val_shape, len(target_cols)))\n",
    "    original = np.zeros((val_shape, len(target_cols)))\n",
    "    \n",
    "    #tk0 = tqdm(enumerate(val_loader))\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for idx, batch in enumerate(val_loader):\n",
    "            if COMBINE_INPUT:\n",
    "                input_ids, input_masks, input_segments, num_features, cat_features, labels, _ = batch\n",
    "                input_ids, input_masks, input_segments, num_features, cat_features, labels = input_ids.to(device), input_masks.to(device), input_segments.to(device), num_features.to(device), cat_features.to(device), labels.to(device)            \n",
    "            \n",
    "                output_val = model(input_ids = input_ids.long(),\n",
    "                               labels = None,\n",
    "                               attention_mask = input_masks,\n",
    "                               token_type_ids = input_segments,\n",
    "                               num_features = num_features,\n",
    "                               cat_features = cat_features,\n",
    "                              )\n",
    "            else:\n",
    "                q_input_ids, q_input_masks, q_input_segments, a_input_ids, a_input_masks, a_input_segments, num_features, cat_features, labels, _ = batch\n",
    "                q_input_ids, q_input_masks, q_input_segments, a_input_ids, a_input_masks, a_input_segments, num_features, cat_features, labels = q_input_ids.to(device), q_input_masks.to(device), q_input_segments.to(device), a_input_ids.to(device), a_input_masks.to(device), a_input_segments.to(device), num_features.to(device), cat_features.to(device), labels.to(device)\n",
    "\n",
    "                output_val = model(q_input_ids = q_input_ids.long(),\n",
    "                             labels = None,\n",
    "                             q_attention_mask = q_input_masks,\n",
    "                             q_token_type_ids = q_input_segments,\n",
    "                             a_input_ids = a_input_ids.long(),\n",
    "                             a_attention_mask = a_input_masks,\n",
    "                             a_token_type_ids = a_input_segments,\n",
    "                             num_features = num_features,\n",
    "                             cat_features = cat_features,\n",
    "                            )\n",
    "            logits = output_val[0] #output preds\n",
    "            \n",
    "            avg_val_loss += criterion(logits, labels).item() / len(val_loader)\n",
    "            valid_preds[idx*batch_size : (idx+1)*batch_size] = logits.detach().cpu().squeeze().numpy()\n",
    "            original[idx*batch_size : (idx+1)*batch_size]    = labels.detach().cpu().squeeze().numpy()\n",
    "        \n",
    "        score = 0\n",
    "        preds = torch.sigmoid(torch.tensor(valid_preds)).numpy()\n",
    "        \n",
    "        # np.save(\"preds.npy\", preds)\n",
    "        # np.save(\"actuals.npy\", original)\n",
    "        \n",
    "        rho_val = np.mean([spearmanr(original[:, i], preds[:,i]).correlation for i in range(preds.shape[1])])\n",
    "        print('\\r val_spearman-rho: %s' % (str(round(rho_val, 5))), end = 100*' '+'\\n')\n",
    "        \n",
    "        for i in range(len(target_cols)):\n",
    "            #print(i, spearmanr(original[:,i], preds[:,i]))\n",
    "            score += np.nan_to_num(spearmanr(original[:, i], preds[:, i]).correlation)\n",
    "        \n",
    "    return avg_val_loss, score/len(target_cols)\n",
    "\n",
    "\n",
    "def predict_valid_result(model, val_loader, val_length, batch_size=32):\n",
    "\n",
    "    val_preds = np.zeros((val_length, len(target_cols)))\n",
    "    original = np.zeros((val_length, len(target_cols)))\n",
    "\n",
    "    model.eval()\n",
    "    tk0 = tqdm(enumerate(val_loader))\n",
    "    for idx, batch in tk0:\n",
    "        if COMBINE_INPUT:\n",
    "            input_ids, input_masks, input_segments, num_features, cat_features, labels, _ = batch\n",
    "            input_ids, input_masks, input_segments, num_features, cat_features, labels = input_ids.to(device), input_masks.to(device), input_segments.to(device), num_features.to(device), cat_features.to(device), labels.to(device)            \n",
    "            with torch.no_grad():\n",
    "                outputs = model(input_ids = input_ids.long(),\n",
    "                            labels = None,\n",
    "                            attention_mask = input_masks,\n",
    "                            token_type_ids = input_segments,\n",
    "                            num_features = num_features,\n",
    "                            cat_features = cat_features,\n",
    "                            )\n",
    "        else:\n",
    "            q_input_ids, q_input_masks, q_input_segments, a_input_ids, a_input_masks, a_input_segments, num_features, cat_features, labels, _ = batch\n",
    "            q_input_ids, q_input_masks, q_input_segments, a_input_ids, a_input_masks, a_input_segments, num_features, cat_features, labels = q_input_ids.to(device), q_input_masks.to(device), q_input_segments.to(device), a_input_ids.to(device), a_input_masks.to(device), a_input_segments.to(device), num_features.to(device), cat_features.to(device), labels.to(device)\n",
    "            with torch.no_grad():\n",
    "                outputs = model(q_input_ids = q_input_ids.long(),\n",
    "                             labels = None,\n",
    "                             q_attention_mask = q_input_masks,\n",
    "                             q_token_type_ids = q_input_segments,\n",
    "                             a_input_ids = a_input_ids.long(),\n",
    "                             a_attention_mask = a_input_masks,\n",
    "                             a_token_type_ids = a_input_segments,\n",
    "                             num_features = num_features,\n",
    "                             cat_features = cat_features,\n",
    "                            )\n",
    "\n",
    "        predictions = outputs[0]\n",
    "        val_preds[idx*batch_size : (idx+1)*batch_size] = predictions.detach().cpu().squeeze().numpy()\n",
    "        original[idx*batch_size : (idx+1)*batch_size] = labels.detach().cpu().squeeze().numpy()\n",
    "\n",
    "    output = torch.sigmoid(torch.tensor(val_preds)).numpy()\n",
    "    return output, original\n",
    "\n",
    "\n",
    "def predict_result(model, test_loader, test_length, batch_size=32):\n",
    "\n",
    "    test_preds = np.zeros((test_length, len(target_cols)))\n",
    "\n",
    "    model.eval()\n",
    "    tk0 = tqdm(enumerate(test_loader))\n",
    "    for idx, x_batch in tk0:\n",
    "        if COMBINE_INPUT:\n",
    "            with torch.no_grad():\n",
    "                outputs = model(input_ids = x_batch[0].to(device),\n",
    "                            labels = None,\n",
    "                            attention_mask = x_batch[1].to(device),\n",
    "                            token_type_ids = x_batch[2].to(device),\n",
    "                            num_features = x_batch[3].to(device),\n",
    "                            cat_features = x_batch[4].to(device),\n",
    "                           )\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                outputs = model(q_input_ids = x_batch[0].to(device),\n",
    "                            labels = None,\n",
    "                            q_attention_mask = x_batch[1].to(device),\n",
    "                            q_token_type_ids = x_batch[2].to(device),\n",
    "                            a_input_ids = x_batch[3].to(device),\n",
    "                            a_attention_mask = x_batch[4].to(device),\n",
    "                            a_token_type_ids = x_batch[5].to(device),\n",
    "                            num_features = x_batch[6].to(device),\n",
    "                            cat_features = x_batch[7].to(device),\n",
    "                           )\n",
    "        predictions = outputs[0]\n",
    "        test_preds[idx*batch_size : (idx+1)*batch_size] = predictions.detach().cpu().squeeze().numpy()\n",
    "\n",
    "    output = torch.sigmoid(torch.tensor(test_preds)).numpy()\n",
    "    return output\n",
    "\n",
    "\n",
    "def add_features(df):\n",
    "    find = re.compile(r\"^[^.]*\")\n",
    "    df['netloc'] = df['url'].apply(lambda x: re.findall(find, urlparse(x).netloc)[0])\n",
    "    df['qa_same_user_page_flag'] = (df['question_user_page']==df['answer_user_page'])*1\n",
    "    df['question_title_num_words'] = df['question_title'].str.count('\\S+')\n",
    "    df['question_body_num_words'] = df['question_body'].str.count('\\S+')\n",
    "    df['answer_num_words'] = df['answer'].str.count('\\S+')\n",
    "    df['question_vs_answer_length'] = df['question_body_num_words']/df['answer_num_words']\n",
    "    df['question_title_num_words'] = np.log1p(df['question_title_num_words'])\n",
    "    df['question_body_num_words'] = np.log1p(df['question_body_num_words'])\n",
    "    df['answer_num_words'] = np.log1p(df['answer_num_words'])\n",
    "    df['question_vs_answer_length'] = np.log1p(df['question_vs_answer_length'])\n",
    "    return df\n",
    "\n",
    "\n",
    "def custom_loss(x, y):\n",
    "    bce_loss = nn.BCEWithLogitsLoss()(x, y)\n",
    "    return bce_loss\n",
    "\n",
    "\n",
    "def get_bert_features(model, val_loader, val_length, batch_size=32):\n",
    "\n",
    "    features = np.zeros((val_length, 768*2))\n",
    "\n",
    "    model.eval()\n",
    "    tk0 = tqdm(enumerate(val_loader))\n",
    "    for idx, batch in tk0:\n",
    "        if COMBINE_INPUT:\n",
    "            None\n",
    "        else:\n",
    "            q_input_ids, q_input_masks, q_input_segments, a_input_ids, a_input_masks, a_input_segments, num_features, cat_features, _ = batch\n",
    "            q_input_ids, q_input_masks, q_input_segments, a_input_ids, a_input_masks, a_input_segments, num_features, cat_features = q_input_ids.to(device), q_input_masks.to(device), q_input_segments.to(device), a_input_ids.to(device), a_input_masks.to(device), a_input_segments.to(device), num_features.to(device), cat_features.to(device)\n",
    "            with torch.no_grad():\n",
    "                q_output = model.bert(q_input_ids.long(),\n",
    "                              attention_mask=q_input_masks,\n",
    "                              token_type_ids=q_input_segments,\n",
    "                              position_ids=None,\n",
    "                              head_mask=None,\n",
    "                              inputs_embeds=None,\n",
    "                            )\n",
    "                a_output = model.bert(a_input_ids.long(),\n",
    "                              attention_mask=a_input_masks,\n",
    "                              token_type_ids=a_input_segments,\n",
    "                              position_ids=None,\n",
    "                              head_mask=None,\n",
    "                              inputs_embeds=None,\n",
    "                            )\n",
    "        q_feature = q_output[1].detach().cpu().squeeze().numpy()\n",
    "        a_feature = a_output[1].detach().cpu().squeeze().numpy()\n",
    "        features[idx*batch_size : (idx+1)*batch_size] = np.hstack([q_feature, a_feature])\n",
    "\n",
    "    return features\n",
    "\n",
    "#===========================================================\n",
    "# main\n",
    "#===========================================================\n",
    "#def main():\n",
    "if True:\n",
    "    \n",
    "    with timer('Data Loading'):\n",
    "        train = pd.read_csv(f\"{ROOT}train.csv\").fillna(\"none\")\n",
    "        y_train = train[target_cols].values\n",
    "        if config.test:\n",
    "            test = pd.read_csv(f\"{ROOT}test.csv\").fillna(\"none\")\n",
    "            submission = pd.read_csv(f\"{ROOT}sample_submission.csv\")\n",
    "    \n",
    "    with timer('Num features'):\n",
    "        train = add_features(train)\n",
    "        if config.test:\n",
    "            test = add_features(test)\n",
    "        num_features = ['question_title_num_words', 'question_body_num_words', 'answer_num_words', 'question_vs_answer_length']\n",
    "        train_num = train[num_features].values\n",
    "        if config.test:\n",
    "            test_num = test[num_features].values\n",
    "                \n",
    "    with timer('Cat features'):\n",
    "        cat_features = ['netloc', 'category', 'qa_same_user_page_flag']\n",
    "        ce_oe = ce.OrdinalEncoder(cols=cat_features, handle_unknown='return_nan')\n",
    "        ce_oe.fit(train[cat_features])\n",
    "        train_cat_df = ce_oe.transform(train[cat_features])\n",
    "        test_cat_df = ce_oe.transform(test[cat_features]).fillna(0).astype(int)\n",
    "        for c in cat_features:\n",
    "            train[c] = train_cat_df[c]\n",
    "            test[c] = test_cat_df[c]\n",
    "        #cat_df = pd.concat([train_cat_df, test_cat_df])\n",
    "        train_cat = train_cat_df.values\n",
    "        test_cat = test_cat_df.values\n",
    "        cat_dims = []\n",
    "        for col in cat_features:\n",
    "            #print(cat_df[col].unique())\n",
    "            #dim = cat_df[col].nunique()\n",
    "            dim = train[col].nunique()\n",
    "            #cat_dims.append((dim, dim//2+1))\n",
    "            cat_dims.append((dim+1, dim//2+1)) # for unknown=0\n",
    "        print(cat_dims)\n",
    "\n",
    "    if config.train:\n",
    "        with timer('Create folds'):\n",
    "            folds = train.copy()\n",
    "\n",
    "            kf = MultilabelStratifiedKFold(n_splits=NUM_FOLDS, random_state=SEED)\n",
    "            for fold, (train_index, val_index) in enumerate(kf.split(train.values, y_train)):\n",
    "                folds.loc[val_index, 'fold'] = int(fold)\n",
    "            \"\"\"\n",
    "            # less gap between CV vs LB with GroupKFold\n",
    "            # https://www.kaggle.com/ratthachat/quest-cv-analysis-on-different-splitting-methods\n",
    "            kf = GroupKFold(n_splits=NUM_FOLDS)\n",
    "            for fold, (train_index, val_index) in enumerate(kf.split(X=train.question_body, groups=train.question_body)):\n",
    "                folds.loc[val_index, 'fold'] = int(fold)\n",
    "            \"\"\"\n",
    "            folds['fold'] = folds['fold'].astype(int)\n",
    "            save_cols = [ID] + target_cols + ['fold']\n",
    "            folds[save_cols].to_csv('folds.csv', index=None)\n",
    "\n",
    "    with timer('Prepare Bert config'):\n",
    "        tokenizer = BertTokenizer.from_pretrained(\"../input/pretrained-bert-models-for-pytorch/bert-base-uncased-vocab.txt\", \n",
    "                                                  do_lower_case=True)\n",
    "        input_categories = ['question_title', 'question_body', 'answer']\n",
    "        bert_model_config = '../input/pretrained-bert-models-for-pytorch/bert-base-uncased/bert_config.json'\n",
    "        bert_config = BertConfig.from_json_file(bert_model_config)\n",
    "        bert_config.num_labels = len(target_cols)\n",
    "        bert_model = 'bert-base-uncased'\n",
    "        do_lower_case = 'uncased' in bert_model\n",
    "        output_model_file = 'bert_pytorch.bin'\n",
    "    \n",
    "    if config.train:\n",
    "\n",
    "        BATCH_SIZE = 8\n",
    "        if DEBUG:\n",
    "            epochs = 1\n",
    "        else:\n",
    "            epochs = config.epochs\n",
    "        ACCUM_STEPS = config.accum_steps\n",
    "\n",
    "        with timer('Train Bert'):\n",
    "            \n",
    "            for fold in range(NUM_FOLDS):\n",
    "\n",
    "                logger.info(f\"Current Fold: {fold}\")\n",
    "                train_index = folds[folds.fold != fold].index\n",
    "                val_index = folds[folds.fold == fold].index\n",
    "\n",
    "                train_df, val_df = train.iloc[train_index], train.iloc[val_index]\n",
    "                logger.info(f\"Train Shapes: {train_df.shape}\")\n",
    "                logger.info(f\"Valid Shapes: {val_df.shape}\")\n",
    "            \n",
    "                logger.info(\"Preparing train datasets....\")\n",
    "            \n",
    "                inputs_train = compute_input_arays(train_df, input_categories, tokenizer, max_sequence_length=MAX_SEQUENCE_LENGTH, \n",
    "                                                   num_features=train_num[train_index], cat_features=train_cat[train_index])\n",
    "                outputs_train = compute_output_arrays(train_df, columns=target_cols)\n",
    "                outputs_train = torch.tensor(outputs_train, dtype=torch.float32)\n",
    "                lengths_train = np.argmax(inputs_train[0]==0, axis=1)\n",
    "                lengths_train[lengths_train==0] = inputs_train[0].shape[1]\n",
    "            \n",
    "                logger.info(\"Preparing valid datasets....\")\n",
    "            \n",
    "                inputs_valid = compute_input_arays(val_df, input_categories, tokenizer, max_sequence_length=MAX_SEQUENCE_LENGTH, \n",
    "                                                   num_features=train_num[val_index], cat_features=train_cat[val_index])\n",
    "                outputs_valid = compute_output_arrays(val_df, columns = target_cols)\n",
    "                outputs_valid = torch.tensor(outputs_valid, dtype=torch.float32)\n",
    "                lengths_valid = np.argmax(inputs_valid[0] == 0, axis=1)\n",
    "                lengths_valid[lengths_valid == 0] = inputs_valid[0].shape[1]\n",
    "            \n",
    "                logger.info(\"Preparing Dataloaders Datasets....\")\n",
    "\n",
    "                train_set = QuestDataset(inputs=inputs_train, lengths=lengths_train, labels=outputs_train)\n",
    "                train_sampler = RandomSampler(train_set)\n",
    "                train_loader = DataLoader(train_set, batch_size=BATCH_SIZE,sampler=train_sampler)\n",
    "            \n",
    "                valid_set = QuestDataset(inputs=inputs_valid, lengths=lengths_valid, labels=outputs_valid)\n",
    "                valid_loader = DataLoader(valid_set, batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n",
    "            \n",
    "                model = CustomBert.from_pretrained('../input/pretrained-bert-models-for-pytorch/bert-base-uncased/', config=bert_config, cat_dims=cat_dims)\n",
    "                model.zero_grad()\n",
    "                model.to(device)\n",
    "                torch.cuda.empty_cache()\n",
    "                if config.freeze : ## This is basically using out of the box bert model while training only the classifier head with our data . \n",
    "                    for param in model.bert.parameters():\n",
    "                        param.requires_grad = False\n",
    "                model.train()\n",
    "            \n",
    "                i = 0\n",
    "                best_avg_loss = 100.0\n",
    "                best_score = -1.\n",
    "                best_param_loss = None\n",
    "                best_param_score = None\n",
    "                param_optimizer = list(model.named_parameters())\n",
    "                no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "                optimizer_grouped_parameters = [\n",
    "                    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "                    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "                    ]        \n",
    "                optimizer = AdamW(optimizer_grouped_parameters, lr=config.lr, eps=4e-5)\n",
    "                #optimizer = torch.optim.AdamW(model.parameters(), lr=config.lr, eps=4e-5)\n",
    "                #criterion = nn.BCEWithLogitsLoss()\n",
    "                criterion = custom_loss\n",
    "                scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=config.warmup, num_training_steps=epochs*len(train_loader)//ACCUM_STEPS)\n",
    "                logger.info(\"Training....\")\n",
    "            \n",
    "                for epoch in tqdm(range(epochs)):\n",
    "\n",
    "                    torch.cuda.empty_cache()\n",
    "                \n",
    "                    start_time   = time.time()\n",
    "                    avg_loss, avg_loss_1, avg_loss_2, avg_loss_3, avg_loss_4, avg_loss_5 = train_model(model, train_loader, optimizer, criterion, scheduler, config)\n",
    "                    avg_val_loss, score = val_model(model, criterion, valid_loader, val_shape=val_df.shape[0], batch_size=BATCH_SIZE)\n",
    "                    elapsed_time = time.time() - start_time\n",
    "\n",
    "                    logger.info('Epoch {}/{} \\t loss={:.4f} \\t val_loss={:.4f} \\t train_loss={:.4f} \\t train_loss_1={:.4f} \\t train_loss_2={:.4f} \\t train_loss_3={:.4f} \\t train_loss_4={:.4f}  \\t train_loss_5={:.4f} \\t score={:.6f} \\t time={:.2f}s'.format(\n",
    "                        epoch+1, epochs, avg_loss, avg_val_loss, avg_loss, avg_loss_1, avg_loss_2, avg_loss_3, avg_loss_4, avg_loss_5, score, elapsed_time))\n",
    "\n",
    "                    if best_avg_loss > avg_val_loss:\n",
    "                        i = 0\n",
    "                        best_avg_loss = avg_val_loss \n",
    "                        best_param_loss = model.state_dict()\n",
    "\n",
    "                    if best_score < score:\n",
    "                        best_score = score\n",
    "                        best_param_score = model.state_dict()\n",
    "                        logger.info('best_param_score_{}_{}.pt'.format(config.expname ,fold))\n",
    "                        torch.save(best_param_score, 'best_param_score_{}_{}.pt'.format(config.expname, fold))\n",
    "                    else:\n",
    "                        i += 1\n",
    "\n",
    "            del train_df, val_df, model, optimizer, criterion, scheduler\n",
    "            del valid_loader, train_loader, valid_set, train_set\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "    \n",
    "    if config.cv:\n",
    "\n",
    "        with timer('CV'):\n",
    "\n",
    "            folds = pd.read_csv(f'{MODEL_DIR}folds.csv')\n",
    "            results = np.zeros((len(train), len(target_cols)))\n",
    "            logits = np.zeros((len(train), len(target_cols)))\n",
    "\n",
    "            for fold in range(NUM_FOLDS):\n",
    "                \n",
    "                #train_index = folds[folds.fold != fold].index\n",
    "                val_index = folds[folds.fold == fold].index\n",
    "                #train_df, val_df = train.iloc[train_index], train.iloc[val_index]\n",
    "                val_df = train.iloc[val_index]\n",
    "                \n",
    "                inputs_valid = compute_input_arays(val_df, input_categories, tokenizer, max_sequence_length=MAX_SEQUENCE_LENGTH, \n",
    "                                                   num_features=train_num[val_index], cat_features=train_cat[val_index])\n",
    "                outputs_valid = compute_output_arrays(val_df, columns = target_cols)\n",
    "                outputs_valid = torch.tensor(outputs_valid, dtype=torch.float32)\n",
    "                lengths_valid = np.argmax(inputs_valid[0] == 0, axis=1)\n",
    "                lengths_valid[lengths_valid == 0] = inputs_valid[0].shape[1]\n",
    "                valid_set = QuestDataset(inputs=inputs_valid, lengths=lengths_valid, labels=outputs_valid)\n",
    "                valid_loader = DataLoader(valid_set, batch_size=32, shuffle=False, drop_last=False)\n",
    "                \n",
    "                model = CustomBert.from_pretrained('../input/pretrained-bert-models-for-pytorch/bert-base-uncased/', config=bert_config, cat_dims=cat_dims)\n",
    "                model.zero_grad()\n",
    "                model.to(device)\n",
    "                model.load_state_dict(torch.load(f'{MODEL_DIR}best_param_score_{config.expname}_{fold}.pt'))\n",
    "                result, logit = predict_valid_result(model, valid_loader, len(val_df))  \n",
    "                results[val_index, :] = result\n",
    "                logits[val_index, :] = logit \n",
    "            \n",
    "            rho_val = np.mean([spearmanr(logits[:,i], results[:,i]).correlation for i in range(results.shape[1])])\n",
    "            logger.info(f'CV spearman-rho: {round(rho_val, 5)}')\n",
    "            \n",
    "            oof = pd.DataFrame()\n",
    "            for i, col in enumerate(target_cols):\n",
    "                oof[col] = results[:,i]\n",
    "            oof.to_csv('oof.csv', index=False)\n",
    "            \n",
    "    if config.cv:\n",
    "        \n",
    "        with timer('Get Bert features'):\n",
    "\n",
    "            #folds = pd.read_csv(f\"{ROOT}folds.csv\")\n",
    "            #features = np.zeros((len(train), 768*2))\n",
    "\n",
    "            for fold in range(NUM_FOLDS):\n",
    "                \n",
    "                train_inputs = compute_input_arays(train, input_categories, tokenizer, max_sequence_length=MAX_SEQUENCE_LENGTH, \n",
    "                                                   num_features=train_num, cat_features=train_cat)\n",
    "                lengths_train = np.argmax(train_inputs[0] == 0, axis=1)\n",
    "                lengths_train[lengths_train == 0] = train_inputs[0].shape[1]\n",
    "                train_set = QuestDataset(inputs=train_inputs, lengths=lengths_train, labels=None)\n",
    "                train_loader  = DataLoader(train_set, batch_size=32, shuffle=False)\n",
    "                features = np.zeros((len(train), 768*2))\n",
    "                \n",
    "                model = CustomBert.from_pretrained('../input/pretrained-bert-models-for-pytorch/bert-base-uncased/', config=bert_config, cat_dims=cat_dims)\n",
    "                model.zero_grad()\n",
    "                model.to(device)\n",
    "                model.load_state_dict(torch.load(f'{MODEL_DIR}best_param_score_{config.expname}_{fold}.pt'))\n",
    "                features = get_bert_features(model, train_loader, len(train))\n",
    "                pd.DataFrame(features).to_csv(f'train_bert_features_{fold}.csv', index=False)\n",
    "                \n",
    "    if config.test:\n",
    "\n",
    "        with timer('Inference'):\n",
    "\n",
    "            test_inputs = compute_input_arays(test, input_categories, tokenizer, max_sequence_length=MAX_SEQUENCE_LENGTH, \n",
    "                                              num_features=test_num, cat_features=test_cat)\n",
    "            lengths_test = np.argmax(test_inputs[0] == 0, axis=1)\n",
    "            lengths_test[lengths_test == 0] = test_inputs[0].shape[1]\n",
    "            test_set = QuestDataset(inputs=test_inputs, lengths=lengths_test, labels=None)\n",
    "            test_loader  = DataLoader(test_set, batch_size=32, shuffle=False)\n",
    "            result = np.zeros((len(test), len(target_cols)))\n",
    "\n",
    "            for fold in range(NUM_FOLDS):\n",
    "                model = CustomBert.from_pretrained('../input/pretrained-bert-models-for-pytorch/bert-base-uncased/', config=bert_config, cat_dims=cat_dims)\n",
    "                model.zero_grad()\n",
    "                model.to(device)\n",
    "                model.load_state_dict(torch.load(f'{MODEL_DIR}best_param_score_{config.expname}_{fold}.pt'))\n",
    "                result += predict_result(model, test_loader, len(test)) \n",
    "                if DEBUG:\n",
    "                    break\n",
    "                    \n",
    "            result /= NUM_FOLDS\n",
    "\n",
    "        with timer('Create submission.csv'):\n",
    "            submission.loc[:, 'question_asker_intent_understanding':] = result\n",
    "            submission.to_csv('submission1.csv', index=False)\n",
    "                \n",
    "    if config.test:\n",
    "        \n",
    "        with timer('Get Bert features'):\n",
    "            \n",
    "            test_inputs = compute_input_arays(test, input_categories, tokenizer, max_sequence_length=MAX_SEQUENCE_LENGTH, \n",
    "                                              num_features=test_num, cat_features=test_cat)\n",
    "            lengths_test = np.argmax(test_inputs[0] == 0, axis=1)\n",
    "            lengths_test[lengths_test == 0] = test_inputs[0].shape[1]\n",
    "            test_set = QuestDataset(inputs=test_inputs, lengths=lengths_test, labels=None)\n",
    "            test_loader  = DataLoader(test_set, batch_size=32, shuffle=False)\n",
    "            #features = np.zeros((len(test), 768*2))\n",
    "\n",
    "            for fold in range(NUM_FOLDS):\n",
    "                model = CustomBert.from_pretrained('../input/pretrained-bert-models-for-pytorch/bert-base-uncased/', config=bert_config, cat_dims=cat_dims)\n",
    "                model.zero_grad()\n",
    "                model.to(device)\n",
    "                model.load_state_dict(torch.load(f'{MODEL_DIR}best_param_score_{config.expname}_{fold}.pt'))\n",
    "                features = get_bert_features(model, test_loader, len(test))\n",
    "                pd.DataFrame(features).to_csv(f'test_bert_features_{fold}.csv', index=False)\n",
    "        \n",
    "        with timer('LGB'):\n",
    "            \n",
    "            LGB_MODEL_DICT = '../input/train-lgb-with-bert-features-xentropy/'\n",
    "\n",
    "            folds = pd.read_csv(MODEL_DIR+'folds.csv')\n",
    "            output = pd.read_csv('../input/google-quest-challenge/sample_submission.csv')\n",
    "\n",
    "            for i in range(len(target_cols)):\n",
    "\n",
    "                target_col = target_cols[i]\n",
    "                #target = train[target_col]\n",
    "\n",
    "                predictions = np.zeros(len(test))\n",
    "\n",
    "                for fold in range(NUM_FOLDS):\n",
    "\n",
    "                    train_cols = ['category', 'netloc', 'qa_same_user_page_flag', 'question_title_num_words',\n",
    "                                  'question_body_num_words', 'answer_num_words', 'question_vs_answer_length']\n",
    "                    bert_features = pd.read_csv(f'test_bert_features_{fold}.csv')\n",
    "                    df = pd.concat([test[train_cols], bert_features], axis=1)\n",
    "                    num_features = [c for c in df.columns if df.dtypes[c] != 'object']\n",
    "                    cat_features = ['netloc', 'category', 'qa_same_user_page_flag']\n",
    "                    features = num_features + cat_features\n",
    "                    drop_features = ['qa_id']\n",
    "                    features = [c for c in features if c not in drop_features]\n",
    "\n",
    "                    with open(LGB_MODEL_DICT+f'{target_col}_lightgbm_fold{fold}.pkl', 'rb') as fin:\n",
    "                        clf = pickle.load(fin)\n",
    "\n",
    "                    predictions += clf.predict(df[features], num_iteration=clf.best_iteration) / NUM_FOLDS\n",
    "\n",
    "                output[target_col] = predictions\n",
    "            \n",
    "            output.to_csv('submission_lgb.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Data Loading] start\n",
      "[Data Loading] done in 0 s\n",
      "[Num features] start\n",
      "[Num features] done in 1 s\n",
      "[Cat features] start\n",
      "[Cat features] done in 0 s\n",
      "[Prepare Bert config] start\n",
      "[Prepare Bert config] done in 0 s\n",
      "[Inference] start\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(60, 30), (6, 3), (3, 2)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15it [00:16,  1.10s/it]\n",
      "15it [00:16,  1.10s/it]\n",
      "15it [00:16,  1.09s/it]\n",
      "15it [00:16,  1.09s/it]\n",
      "15it [00:16,  1.10s/it]\n",
      "[Inference] done in 106 s\n",
      "[Create submission.csv] start\n",
      "[Create submission.csv] done in 0 s\n"
     ]
    }
   ],
   "source": [
    "#===========================================================\n",
    "# Config\n",
    "#===========================================================\n",
    "class PipeLineConfig:\n",
    "    def __init__(self, lr, warmup, accum_steps, epochs, seed, expname, \n",
    "                 head_tail, head, freeze, question_weight, answer_weight, fold, train, cv, test):\n",
    "        self.lr = lr\n",
    "        self.warmup = warmup\n",
    "        self.accum_steps = accum_steps\n",
    "        self.epochs = epochs\n",
    "        self.seed = seed\n",
    "        self.expname = expname\n",
    "        self.head_tail = head_tail\n",
    "        self.head = head\n",
    "        self.freeze = freeze\n",
    "        self.question_weight = question_weight\n",
    "        self.answer_weight = answer_weight\n",
    "        self.fold = fold\n",
    "        self.train = train\n",
    "        self.cv = cv\n",
    "        self.test = test\n",
    "\n",
    "config = PipeLineConfig(lr=1e-4, warmup=0.1, accum_steps=1, epochs=5,\n",
    "                        seed=42, expname='uncased_7', head_tail=True, head=0.5, freeze=False,\n",
    "                        question_weight=0., answer_weight=0., fold=5, train=False, cv=False, test=True)\n",
    "\n",
    "DEBUG = False\n",
    "ID = 'qa_id'\n",
    "target_cols = ['question_asker_intent_understanding', 'question_body_critical', \n",
    "               'question_conversational', 'question_expect_short_answer', \n",
    "               'question_fact_seeking', 'question_has_commonly_accepted_answer', \n",
    "               'question_interestingness_others', 'question_interestingness_self', \n",
    "               'question_multi_intent', 'question_not_really_a_question', \n",
    "               'question_opinion_seeking', 'question_type_choice',\n",
    "               'question_type_compare', 'question_type_consequence',\n",
    "               'question_type_definition', 'question_type_entity', \n",
    "               'question_type_instructions', 'question_type_procedure', \n",
    "               'question_type_reason_explanation', 'question_type_spelling', \n",
    "               'question_well_written', 'answer_helpful',\n",
    "               'answer_level_of_information', 'answer_plausible', \n",
    "               'answer_relevance', 'answer_satisfaction', \n",
    "               'answer_type_instructions', 'answer_type_procedure', \n",
    "               'answer_type_reason_explanation', 'answer_well_written']\n",
    "NUM_FOLDS = config.fold\n",
    "ROOT = '../input/google-quest-challenge/'\n",
    "#ROOT = '../input/'\n",
    "SEED = config.seed\n",
    "seed_everything(SEED)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "MODEL_DIR = '../input/googlequestchallenge-weights2/'\n",
    "#MODEL_DIR = './'\n",
    "COMBINE_INPUT = False\n",
    "T_MAX_LEN = 30\n",
    "Q_MAX_LEN = 479 # 382\n",
    "A_MAX_LEN = 479 # 254 \n",
    "MAX_SEQUENCE_LENGTH = T_MAX_LEN + Q_MAX_LEN + A_MAX_LEN + 4\n",
    "q_max_sequence_length = T_MAX_LEN + Q_MAX_LEN + 3\n",
    "a_max_sequence_length = T_MAX_LEN + A_MAX_LEN + 3\n",
    "\n",
    "#===========================================================\n",
    "# Model\n",
    "#===========================================================\n",
    "def _get_masks(tokens, max_seq_length):\n",
    "    \"\"\"Mask for padding\"\"\"\n",
    "    if len(tokens)>max_seq_length:\n",
    "        print(f'len(tokens): {len(tokens)}')\n",
    "        print(f'max_seq_length: {max_seq_length}')\n",
    "        raise IndexError(\"Token length more than max seq length!\")\n",
    "    return [1]*len(tokens) + [0] * (max_seq_length - len(tokens))\n",
    "\n",
    "\n",
    "def _get_segments(tokens, max_seq_length):\n",
    "    \"\"\"Segments: 0 for the first sequence, 1 for the second\"\"\"\n",
    "    \n",
    "    if len(tokens) > max_seq_length:\n",
    "        raise IndexError(\"Token length more than max seq length!\")\n",
    "        \n",
    "    segments = []\n",
    "    first_sep = True\n",
    "    current_segment_id = 0\n",
    "    \n",
    "    for token in tokens:\n",
    "        segments.append(current_segment_id)\n",
    "        if token == \"[SEP]\":\n",
    "            if first_sep:\n",
    "                first_sep = False \n",
    "            else:\n",
    "                current_segment_id = 1\n",
    "    return segments + [0] * (max_seq_length - len(tokens))\n",
    "\n",
    "\n",
    "def _get_ids(tokens, tokenizer, max_seq_length):\n",
    "    \"\"\"Token ids from Tokenizer vocab\"\"\"\n",
    "    \n",
    "    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    input_ids = token_ids + [0] * (max_seq_length-len(token_ids))\n",
    "    return input_ids\n",
    "\n",
    "\n",
    "def _trim_input(tokenizer, title, question, answer, max_sequence_length, t_max_len, q_max_len, a_max_len):\n",
    "    \n",
    "    # 350+128+30 = 508 +4 = 512\n",
    "    \n",
    "    t = tokenizer.tokenize(title)\n",
    "    q = tokenizer.tokenize(question)\n",
    "    a = tokenizer.tokenize(answer)\n",
    "    \n",
    "    t_len = len(t)\n",
    "    q_len = len(q)\n",
    "    a_len = len(a)\n",
    "\n",
    "    if (t_len+q_len+a_len+4) > max_sequence_length:\n",
    "        \n",
    "        if t_max_len > t_len:\n",
    "            t_new_len = t_len\n",
    "            a_max_len = a_max_len + floor((t_max_len - t_len)/2)\n",
    "            q_max_len = q_max_len + ceil((t_max_len - t_len)/2)\n",
    "        else:\n",
    "            t_new_len = t_max_len\n",
    "      \n",
    "        if a_max_len > a_len:\n",
    "            a_new_len = a_len \n",
    "            q_new_len = q_max_len + (a_max_len - a_len)\n",
    "        elif q_max_len > q_len:\n",
    "            a_new_len = a_max_len + (q_max_len - q_len)\n",
    "            q_new_len = q_len\n",
    "        else:\n",
    "            a_new_len = a_max_len\n",
    "            q_new_len = q_max_len\n",
    "            \n",
    "            \n",
    "        if t_new_len+a_new_len+q_new_len+4 != max_sequence_length:\n",
    "            raise ValueError(\"New sequence length should be %d, but is %d\"%(max_sequence_length, (t_new_len + a_new_len + q_new_len + 4)))\n",
    "        # Head+Tail method \n",
    "        q_len_head = round(q_new_len * config.head)\n",
    "        q_len_tail = -1 * (q_new_len - q_len_head)\n",
    "        a_len_head = round(a_new_len * config.head)\n",
    "        a_len_tail = -1 * (a_new_len - a_len_head)\n",
    "        t_len_head = round(t_new_len * config.head)\n",
    "        t_len_tail = -1 * (t_new_len - t_len_head)  \n",
    "        #t = t[:t_new_len]\n",
    "        if config.head_tail :\n",
    "            q = q[:q_len_head]+q[q_len_tail:]\n",
    "            a = a[:a_len_head]+a[a_len_tail:]\n",
    "            #t = t[:t_len_head]+t[t_len_tail:]\n",
    "            t = t[:t_new_len]\n",
    "        else:\n",
    "            # No Head+Tail , usual processing\n",
    "            q = q[:q_new_len]\n",
    "            a = a[:a_new_len]\n",
    "            t = t[:t_new_len]\n",
    "    \n",
    "    return t, q, a\n",
    "\n",
    "\n",
    "def q_trim_input(tokenizer, title, question, q_max_sequence_length, t_max_len, q_max_len):\n",
    "\n",
    "    t = tokenizer.tokenize(title)\n",
    "    q = tokenizer.tokenize(question)\n",
    "\n",
    "    t_len = len(t)\n",
    "    q_len = len(q)\n",
    "\n",
    "    if (t_len+q_len+3) > q_max_sequence_length:\n",
    "\n",
    "        if t_max_len > t_len:\n",
    "            t_new_len = t_len\n",
    "            q_max_len = q_max_len + (t_max_len - t_len)\n",
    "        else:\n",
    "            t_new_len = t_max_len\n",
    "\n",
    "        if q_max_len > q_len:\n",
    "            q_new_len = q_len\n",
    "            t_new_len = t_max_len + (q_max_len - q_len)\n",
    "        else:\n",
    "            q_new_len = q_max_len\n",
    "\n",
    "        # Head+Tail method\n",
    "        q_len_head = round(q_new_len * config.head)\n",
    "        q_len_tail = -1 * (q_new_len - q_len_head)\n",
    "        t_len_head = round(t_new_len * config.head)\n",
    "        t_len_tail = -1 * (t_new_len - t_len_head)\n",
    "        #t = t[:t_new_len]\n",
    "        if config.head_tail :\n",
    "            q = q[:q_len_head]+q[q_len_tail:]\n",
    "            t = t[:t_len_head]+t[t_len_tail:]\n",
    "            #t = t[:t_new_len]\n",
    "        else:\n",
    "            # No Head+Tail , usual processing\n",
    "            q = q[:q_new_len]\n",
    "            t = t[:t_new_len]\n",
    "\n",
    "    return t, q\n",
    "\n",
    "\"\"\"\n",
    "def a_trim_input(tokenizer, answer, a_max_sequence_length, a_max_len):\n",
    "\n",
    "    a = tokenizer.tokenize(answer)\n",
    "\n",
    "    a_len = len(a)\n",
    "\n",
    "    if (a_len+2) > a_max_sequence_length:\n",
    "\n",
    "        a_new_len = a_max_len\n",
    "\n",
    "        # Head+Tail method\n",
    "        a_len_head = round(a_new_len * config.head)\n",
    "        a_len_tail = -1 * (a_new_len - a_len_head)\n",
    "        if config.head_tail :\n",
    "            a = a[:a_len_head]+a[a_len_tail:]\n",
    "        else:\n",
    "            # No Head+Tail , usual processing\n",
    "            a = a[:a_new_len]\n",
    "\n",
    "    return a\n",
    "\"\"\"\n",
    "\n",
    "def a_trim_input(tokenizer, title, answer, a_max_sequence_length, t_max_len, a_max_len):\n",
    "\n",
    "    t = tokenizer.tokenize(title)\n",
    "    a = tokenizer.tokenize(answer)\n",
    "\n",
    "    t_len = len(t)\n",
    "    a_len = len(a)\n",
    "\n",
    "    if (t_len+a_len+3) > a_max_sequence_length:\n",
    "\n",
    "        if t_max_len > t_len:\n",
    "            t_new_len = t_len\n",
    "            a_max_len = a_max_len + (t_max_len - t_len)\n",
    "        else:\n",
    "            t_new_len = t_max_len\n",
    "\n",
    "        if a_max_len > a_len:\n",
    "            a_new_len = a_len\n",
    "            t_new_len = t_max_len + (a_max_len - a_len)\n",
    "        else:\n",
    "            a_new_len = a_max_len\n",
    "\n",
    "        # Head+Tail method\n",
    "        a_len_head = round(a_new_len * config.head)\n",
    "        a_len_tail = -1 * (a_new_len - a_len_head)\n",
    "        t_len_head = round(t_new_len * config.head)\n",
    "        t_len_tail = -1 * (t_new_len - t_len_head)\n",
    "        #t = t[:t_new_len]\n",
    "        if config.head_tail :\n",
    "            a = a[:a_len_head]+a[a_len_tail:]\n",
    "            t = t[:t_len_head]+t[t_len_tail:]\n",
    "            #t = t[:t_new_len]\n",
    "        else:\n",
    "            # No Head+Tail , usual processing\n",
    "            a = a[:a_new_len]\n",
    "            t = t[:t_new_len]\n",
    "\n",
    "    return t, a\n",
    "\n",
    "\n",
    "def _convert_to_bert_inputs(title_q, title_a, question, answer, tokenizer, max_sequence_length):\n",
    "    \"\"\"Converts tokenized input to ids, masks and segments for BERT\"\"\"\n",
    "    if COMBINE_INPUT:\n",
    "        stoken = [\"[CLS]\"] + title + [\"[QBODY]\"] + question + [\"[ANS]\"] + answer + [\"[SEP]\"]\n",
    "        #stoken = [\"[CLS]\"] + title + [\"[SEP]\"] + question + [\"[SEP]\"] + answer + [\"[SEP]\"]\n",
    "        #stoken = [\"[CLS]\"] + title  + question  + answer + [\"[SEP]\"]\n",
    "    \n",
    "        input_ids = _get_ids(stoken, tokenizer, max_sequence_length)\n",
    "        input_masks = _get_masks(stoken, max_sequence_length)\n",
    "        input_segments = _get_segments(stoken, max_sequence_length)\n",
    "\n",
    "        return [input_ids, input_masks, input_segments]\n",
    "    else:\n",
    "        q_token = [\"[CLS]\"] + title_q + [\"[SEP]\"] + question + [\"[SEP\"]\n",
    "        q_input_ids = _get_ids(q_token, tokenizer, T_MAX_LEN+Q_MAX_LEN+3)\n",
    "        q_input_masks = _get_masks(q_token, T_MAX_LEN+Q_MAX_LEN+3)\n",
    "        q_input_segments = _get_segments(q_token, T_MAX_LEN+Q_MAX_LEN+3)\n",
    "        \n",
    "        #a_token = [\"[CLS]\"] + answer + [\"[SEP]\"]\n",
    "        #a_input_ids = _get_ids(a_token, tokenizer, A_MAX_LEN+2)\n",
    "        #a_input_masks = _get_masks(a_token, A_MAX_LEN+2)\n",
    "        #a_input_segments = _get_segments(a_token, A_MAX_LEN+2)\n",
    "        a_token = [\"[CLS]\"] + title_a + [\"[SEP]\"] + answer + [\"[SEP\"]\n",
    "        a_input_ids = _get_ids(a_token, tokenizer, T_MAX_LEN+A_MAX_LEN+3)\n",
    "        a_input_masks = _get_masks(a_token, T_MAX_LEN+A_MAX_LEN+3)\n",
    "        a_input_segments = _get_segments(a_token, T_MAX_LEN+A_MAX_LEN+3)\n",
    "        \n",
    "        return [q_input_ids, q_input_masks, q_input_segments, a_input_ids, a_input_masks, a_input_segments]\n",
    "\n",
    "\n",
    "def compute_input_arays(df, columns, tokenizer, max_sequence_length, num_features, cat_features, \n",
    "                        t_max_len=T_MAX_LEN, q_max_len=Q_MAX_LEN, a_max_len=A_MAX_LEN):\n",
    "    if COMBINE_INPUT:\n",
    "        input_ids, input_masks, input_segments = [], [], []\n",
    "        for _, instance in df[columns].iterrows():\n",
    "            t, q, a = instance.question_title, instance.question_body, instance.answer\n",
    "            t, q, a = _trim_input(tokenizer, t, q, a, max_sequence_length, t_max_len, q_max_len, a_max_len)\n",
    "            ids, masks, segments = _convert_to_bert_inputs(t, q, a, tokenizer, max_sequence_length)\n",
    "            input_ids.append(ids)\n",
    "            input_masks.append(masks)\n",
    "            input_segments.append(segments)\n",
    "        return [\n",
    "                torch.from_numpy(np.asarray(input_ids, dtype=np.int32)).long(), \n",
    "                torch.from_numpy(np.asarray(input_masks, dtype=np.int32)).long(),\n",
    "                torch.from_numpy(np.asarray(input_segments, dtype=np.int32)).long(),\n",
    "                torch.from_numpy(np.asarray(num_features, dtype=np.float32)).float(),\n",
    "                torch.from_numpy(np.asarray(cat_features, dtype=np.int32)).long(),\n",
    "                ]\n",
    "    else:\n",
    "        q_input_ids, q_input_masks, q_input_segments = [], [], []\n",
    "        a_input_ids, a_input_masks, a_input_segments = [], [], []\n",
    "        for _, instance in df[columns].iterrows():\n",
    "            t, q, a = instance.question_title, instance.question_body, instance.answer\n",
    "            t_q, q = q_trim_input(tokenizer, t, q, q_max_sequence_length, t_max_len, q_max_len)\n",
    "            #a = a_trim_input(tokenizer, a, a_max_sequence_length, a_max_len)\n",
    "            t_a, a = a_trim_input(tokenizer, t, a, a_max_sequence_length, t_max_len, a_max_len)\n",
    "            q_ids, q_masks, q_segments, a_ids, a_masks, a_segments = _convert_to_bert_inputs(t_q, t_a, q, a, tokenizer, max_sequence_length)\n",
    "            q_input_ids.append(q_ids)\n",
    "            q_input_masks.append(q_masks)\n",
    "            q_input_segments.append(q_segments)\n",
    "            a_input_ids.append(a_ids)\n",
    "            a_input_masks.append(a_masks)\n",
    "            a_input_segments.append(a_segments)\n",
    "        return [\n",
    "                torch.from_numpy(np.asarray(q_input_ids, dtype=np.int32)).long(),\n",
    "                torch.from_numpy(np.asarray(q_input_masks, dtype=np.int32)).long(),\n",
    "                torch.from_numpy(np.asarray(q_input_segments, dtype=np.int32)).long(),\n",
    "                torch.from_numpy(np.asarray(a_input_ids, dtype=np.int32)).long(),\n",
    "                torch.from_numpy(np.asarray(a_input_masks, dtype=np.int32)).long(),\n",
    "                torch.from_numpy(np.asarray(a_input_segments, dtype=np.int32)).long(),\n",
    "                torch.from_numpy(np.asarray(num_features, dtype=np.float32)).float(),\n",
    "                torch.from_numpy(np.asarray(cat_features, dtype=np.int32)).long(),\n",
    "                ]\n",
    "\n",
    "\n",
    "def compute_output_arrays(df, columns):\n",
    "    return np.asarray(df[columns])\n",
    "\n",
    "\n",
    "if COMBINE_INPUT:\n",
    "\n",
    "    class QuestDataset(torch.utils.data.Dataset):\n",
    "        def __init__(self, inputs, lengths, labels = None):\n",
    "\n",
    "            self.inputs = inputs\n",
    "            if labels is not None:\n",
    "                self.labels = labels\n",
    "            else:\n",
    "                self.labels = None\n",
    "            self.lengths = lengths\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "\n",
    "            input_ids       = self.inputs[0][idx]\n",
    "            input_masks     = self.inputs[1][idx]\n",
    "            input_segments  = self.inputs[2][idx]\n",
    "            num_features    = self.inputs[3][idx]\n",
    "            cat_features    = self.inputs[4][idx]\n",
    "            lengths         = self.lengths[idx]\n",
    "            if self.labels is not None: # targets\n",
    "                labels = self.labels[idx]\n",
    "                return input_ids, input_masks, input_segments, num_features, cat_features, labels, lengths\n",
    "            return input_ids, input_masks, input_segments, num_features, cat_features, lengths\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.inputs[0])\n",
    "\n",
    "\n",
    "    class CustomBert(BertPreTrainedModel):\n",
    "\n",
    "        def __init__(self, config, cat_dims):\n",
    "            super(CustomBert, self).__init__(config)\n",
    "            self.num_labels = config.num_labels\n",
    "            self.bert = BertModel(config)\n",
    "            self.embeddings = nn.ModuleList([\n",
    "                nn.Embedding(x, y) for x, y in cat_dims\n",
    "            ])\n",
    "            self.emb_drop = nn.Dropout(0.2)\n",
    "            n_emb_out = sum([y for x, y in cat_dims])\n",
    "            self.dropout = nn.Dropout(0.2)\n",
    "            self.classifier_final = nn.Linear(config.hidden_size+n_emb_out+4, self.config.num_labels)  # num_features=4\n",
    "\n",
    "            self.init_weights()\n",
    "\n",
    "        def forward(\n",
    "            self,\n",
    "            input_ids=None,\n",
    "            attention_mask=None,\n",
    "            token_type_ids=None,\n",
    "            num_features=None,\n",
    "            cat_features=None,\n",
    "            position_ids=None,\n",
    "            head_mask=None,\n",
    "            inputs_embeds=None,\n",
    "            labels=None,\n",
    "        ):\n",
    "\n",
    "            outputs = self.bert(\n",
    "                input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                token_type_ids=token_type_ids,\n",
    "                position_ids=position_ids,\n",
    "                head_mask=head_mask,\n",
    "                inputs_embeds=inputs_embeds,\n",
    "            )\n",
    "\n",
    "            pooled_output = outputs[1]\n",
    "            pooled_output = self.dropout(pooled_output)\n",
    "\n",
    "            emb = [\n",
    "                emb_layer(cat_features[:, j]) for j, emb_layer in enumerate(self.embeddings)\n",
    "            ]\n",
    "            emb = self.emb_drop(torch.cat(emb, 1))\n",
    "\n",
    "            pooled_output = torch.cat([pooled_output, num_features, emb], 1)\n",
    "            logits = self.classifier_final(pooled_output)\n",
    "\n",
    "            outputs = (logits,) + outputs[2:]  # add hidden states and attention if they are here\n",
    "            if labels is not None:\n",
    "                if self.num_labels == 1:\n",
    "                    #  We are doing regression\n",
    "                    loss_fct = MSELoss()\n",
    "                    loss = loss_fct(logits.view(-1), labels.view(-1))\n",
    "                else:\n",
    "                    loss_fct = CrossEntropyLoss()\n",
    "                    loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "                outputs = (loss,) + outputs\n",
    "\n",
    "            return outputs  # (loss), logits, (hidden_states), (attentions)\n",
    "\n",
    "else:\n",
    "\n",
    "    class QuestDataset(torch.utils.data.Dataset):\n",
    "        def __init__(self, inputs, lengths, labels = None):\n",
    "\n",
    "            self.inputs = inputs\n",
    "            if labels is not None:\n",
    "                self.labels = labels\n",
    "            else:\n",
    "                self.labels = None\n",
    "            self.lengths = lengths\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "\n",
    "            q_input_ids       = self.inputs[0][idx]\n",
    "            q_input_masks     = self.inputs[1][idx]\n",
    "            q_input_segments  = self.inputs[2][idx]\n",
    "            a_input_ids       = self.inputs[3][idx]\n",
    "            a_input_masks     = self.inputs[4][idx]\n",
    "            a_input_segments  = self.inputs[5][idx]\n",
    "            num_features    = self.inputs[6][idx]\n",
    "            cat_features    = self.inputs[7][idx]\n",
    "            lengths         = self.lengths[idx]\n",
    "            if self.labels is not None: # targets\n",
    "                labels = self.labels[idx]\n",
    "                return q_input_ids, q_input_masks, q_input_segments, a_input_ids, a_input_masks, a_input_segments, num_features, cat_features, labels, lengths\n",
    "            return q_input_ids, q_input_masks, q_input_segments, a_input_ids, a_input_masks, a_input_segments, num_features, cat_features, lengths\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.inputs[0])\n",
    "\n",
    "\n",
    "    class CustomBert(BertPreTrainedModel):\n",
    "\n",
    "        def __init__(self, config, cat_dims):\n",
    "            super(CustomBert, self).__init__(config)\n",
    "            self.num_labels = config.num_labels\n",
    "            self.bert = BertModel(config)\n",
    "            self.embeddings = nn.ModuleList([\n",
    "                nn.Embedding(x, y) for x, y in cat_dims\n",
    "            ])\n",
    "            self.emb_drop = nn.Dropout(0.1)\n",
    "            n_emb_out = sum([y for x, y in cat_dims])\n",
    "            self.num_drop = nn.Dropout(0.1)\n",
    "            self.q_dropout = nn.Dropout(0.1)\n",
    "            self.a_dropout = nn.Dropout(0.1)\n",
    "            #self.dropout_all = nn.Dropout(0.2)\n",
    "            #self.dropout_a = nn.Dropout(0.2)\n",
    "            #self.dropout_q = nn.Dropout(0.2)\n",
    "            #self.classifier_all = nn.Linear(config.hidden_size*2+n_emb_out+4, 64)  # num_features=4\n",
    "            #self.classifier_all = nn.Sequential(\n",
    "            #    nn.Linear(config.hidden_size*2+n_emb_out+4, 64),\n",
    "            #    nn.ReLU(inplace=True),\n",
    "            #)\n",
    "            #self.classifier_a = nn.Linear(config.hidden_size+n_emb_out+4, 64)  # num_features=4\n",
    "            #self.classifier_a = nn.Sequential(\n",
    "            #    nn.Linear(config.hidden_size+n_emb_out+4, 64),\n",
    "            #    nn.ReLU(inplace=True),\n",
    "            #)\n",
    "            #self.classifier_q = nn.Linear(config.hidden_size+n_emb_out+4, 64)  # num_features=4\n",
    "            #self.classifier_q = nn.Sequential(\n",
    "            #    nn.Linear(config.hidden_size+n_emb_out+4, 64),\n",
    "            #    nn.ReLU(inplace=True),\n",
    "            #)\n",
    "            self.classifier_final = nn.Linear(config.hidden_size*2+n_emb_out+4, self.config.num_labels)\n",
    "            #self.classifier_final = nn.Linear(64*3, self.config.num_labels)  # num_features=4\n",
    "            #self.classifier_final = nn.Sequential(\n",
    "            #    nn.BatchNorm1d(64*3),\n",
    "            #    nn.Linear(64*3, self.config.num_labels),\n",
    "            #)\n",
    "            self.init_weights()\n",
    "\n",
    "        def forward(\n",
    "            self,\n",
    "            q_input_ids=None,\n",
    "            q_attention_mask=None,\n",
    "            q_token_type_ids=None,\n",
    "            a_input_ids=None,\n",
    "            a_attention_mask=None,\n",
    "            a_token_type_ids=None,\n",
    "            num_features=None,\n",
    "            cat_features=None,\n",
    "            position_ids=None,\n",
    "            head_mask=None,\n",
    "            inputs_embeds=None,\n",
    "            labels=None,\n",
    "        ):\n",
    "\n",
    "            q_outputs = self.bert(\n",
    "                q_input_ids,\n",
    "                attention_mask=q_attention_mask,\n",
    "                token_type_ids=q_token_type_ids,\n",
    "                position_ids=position_ids,\n",
    "                head_mask=head_mask,\n",
    "                inputs_embeds=inputs_embeds,\n",
    "            )\n",
    "\n",
    "            q_pooled_output = q_outputs[1]\n",
    "            q_pooled_output = self.q_dropout(q_pooled_output)\n",
    "\n",
    "            a_outputs = self.bert(\n",
    "                a_input_ids,\n",
    "                attention_mask=a_attention_mask,\n",
    "                token_type_ids=a_token_type_ids,\n",
    "                position_ids=position_ids,\n",
    "                head_mask=head_mask,\n",
    "                inputs_embeds=inputs_embeds,\n",
    "            )\n",
    "\n",
    "            a_pooled_output = a_outputs[1]\n",
    "            a_pooled_output = self.a_dropout(a_pooled_output)\n",
    "\n",
    "            emb = [\n",
    "                emb_layer(cat_features[:, j]) for j, emb_layer in enumerate(self.embeddings)\n",
    "            ]\n",
    "            emb = self.emb_drop(torch.cat(emb, 1))\n",
    "\n",
    "            num_features = self.num_drop(num_features)\n",
    "\n",
    "            pooled_output = torch.cat([q_pooled_output, a_pooled_output, num_features, emb], 1)\n",
    "            #all_logits = self.classifier_all(pooled_output)\n",
    "            #all_logits = self.dropout_all(all_logits)\n",
    "            logits = self.classifier_final(pooled_output)\n",
    "            \n",
    "            #a_pooled_output = torch.cat([a_pooled_output, num_features, emb], 1)\n",
    "            #a_logits = self.classifier_a(a_pooled_output)\n",
    "            #a_logits = self.dropout_a(a_logits)\n",
    "\n",
    "            #q_pooled_output = torch.cat([q_pooled_output, num_features, emb], 1)\n",
    "            #q_logits = self.classifier_q(q_pooled_output)\n",
    "            #q_logits = self.dropout_q(q_logits)\n",
    "\n",
    "            #concat_logits = torch.cat([all_logits, q_logits, a_logits], 1)\n",
    "            #logits = self.classifier_final(concat_logits)\n",
    "\n",
    "            #logits = torch.cat([q_logits, a_logits], 1)\n",
    "\n",
    "            outputs = (logits,) + q_outputs[2:] + a_outputs[2:]  # add hidden states and attention if they are here\n",
    "            if labels is not None:\n",
    "                if self.num_labels == 1:\n",
    "                    #  We are doing regression\n",
    "                    loss_fct = MSELoss()\n",
    "                    loss = loss_fct(logits.view(-1), labels.view(-1))\n",
    "                else:\n",
    "                    loss_fct = CrossEntropyLoss()\n",
    "                    loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "                outputs = (loss,) + outputs\n",
    "\n",
    "            return outputs  # (loss), logits, (hidden_states), (attentions)\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, optimizer, criterion, scheduler, config):\n",
    "    \n",
    "    model.train()\n",
    "    avg_loss = 0.\n",
    "    avg_loss_1 = 0.\n",
    "    avg_loss_2 = 0.\n",
    "    avg_loss_3 = 0.\n",
    "    avg_loss_4 = 0.\n",
    "    avg_loss_5 = 0.\n",
    "    #tk0 = tqdm(enumerate(train_loader),total =len(train_loader))\n",
    "    optimizer.zero_grad()\n",
    "    for idx, batch in enumerate(train_loader):\n",
    "        if COMBINE_INPUT:\n",
    "            input_ids, input_masks, input_segments, num_features, cat_features, labels, _ = batch\n",
    "            input_ids, input_masks, input_segments, num_features, cat_features, labels = input_ids.to(device), input_masks.to(device), input_segments.to(device), num_features.to(device), cat_features.to(device), labels.to(device)            \n",
    "        \n",
    "            output_train = model(input_ids = input_ids.long(),\n",
    "                             labels = None,\n",
    "                             attention_mask = input_masks,\n",
    "                             token_type_ids = input_segments,\n",
    "                             num_features = num_features,\n",
    "                             cat_features = cat_features,\n",
    "                            )\n",
    "        else:\n",
    "            q_input_ids, q_input_masks, q_input_segments, a_input_ids, a_input_masks, a_input_segments, num_features, cat_features, labels, _ = batch\n",
    "            q_input_ids, q_input_masks, q_input_segments, a_input_ids, a_input_masks, a_input_segments, num_features, cat_features, labels = q_input_ids.to(device), q_input_masks.to(device), q_input_segments.to(device), a_input_ids.to(device), a_input_masks.to(device), a_input_segments.to(device), num_features.to(device), cat_features.to(device), labels.to(device)\n",
    "\n",
    "            output_train = model(q_input_ids = q_input_ids.long(),\n",
    "                             labels = None,\n",
    "                             q_attention_mask = q_input_masks,\n",
    "                             q_token_type_ids = q_input_segments,\n",
    "                             a_input_ids = a_input_ids.long(),\n",
    "                             a_attention_mask = a_input_masks,\n",
    "                             a_token_type_ids = a_input_segments,\n",
    "                             num_features = num_features,\n",
    "                             cat_features = cat_features,\n",
    "                            )\n",
    "        logits = output_train[0] #output preds\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        if (idx + 1) % config.accum_steps == 0:    \n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        avg_loss += loss.item() / (len(train_loader)*config.accum_steps)\n",
    "        if COMBINE_INPUT:\n",
    "            del input_ids, input_masks, input_segments, labels\n",
    "        else:\n",
    "            del q_input_ids, q_input_masks, q_input_segments, a_input_ids, a_input_masks, a_input_segments, labels\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    return avg_loss, avg_loss_1, avg_loss_2, avg_loss_3, avg_loss_4, avg_loss_5\n",
    "\n",
    "\n",
    "def val_model(model, criterion, val_loader, val_shape, batch_size=8):\n",
    "\n",
    "    avg_val_loss = 0.\n",
    "    model.eval() # eval mode\n",
    "    \n",
    "    valid_preds = np.zeros((val_shape, len(target_cols)))\n",
    "    original = np.zeros((val_shape, len(target_cols)))\n",
    "    \n",
    "    #tk0 = tqdm(enumerate(val_loader))\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for idx, batch in enumerate(val_loader):\n",
    "            if COMBINE_INPUT:\n",
    "                input_ids, input_masks, input_segments, num_features, cat_features, labels, _ = batch\n",
    "                input_ids, input_masks, input_segments, num_features, cat_features, labels = input_ids.to(device), input_masks.to(device), input_segments.to(device), num_features.to(device), cat_features.to(device), labels.to(device)            \n",
    "            \n",
    "                output_val = model(input_ids = input_ids.long(),\n",
    "                               labels = None,\n",
    "                               attention_mask = input_masks,\n",
    "                               token_type_ids = input_segments,\n",
    "                               num_features = num_features,\n",
    "                               cat_features = cat_features,\n",
    "                              )\n",
    "            else:\n",
    "                q_input_ids, q_input_masks, q_input_segments, a_input_ids, a_input_masks, a_input_segments, num_features, cat_features, labels, _ = batch\n",
    "                q_input_ids, q_input_masks, q_input_segments, a_input_ids, a_input_masks, a_input_segments, num_features, cat_features, labels = q_input_ids.to(device), q_input_masks.to(device), q_input_segments.to(device), a_input_ids.to(device), a_input_masks.to(device), a_input_segments.to(device), num_features.to(device), cat_features.to(device), labels.to(device)\n",
    "\n",
    "                output_val = model(q_input_ids = q_input_ids.long(),\n",
    "                             labels = None,\n",
    "                             q_attention_mask = q_input_masks,\n",
    "                             q_token_type_ids = q_input_segments,\n",
    "                             a_input_ids = a_input_ids.long(),\n",
    "                             a_attention_mask = a_input_masks,\n",
    "                             a_token_type_ids = a_input_segments,\n",
    "                             num_features = num_features,\n",
    "                             cat_features = cat_features,\n",
    "                            )\n",
    "            logits = output_val[0] #output preds\n",
    "            \n",
    "            avg_val_loss += criterion(logits, labels).item() / len(val_loader)\n",
    "            valid_preds[idx*batch_size : (idx+1)*batch_size] = logits.detach().cpu().squeeze().numpy()\n",
    "            original[idx*batch_size : (idx+1)*batch_size]    = labels.detach().cpu().squeeze().numpy()\n",
    "        \n",
    "        score = 0\n",
    "        preds = torch.sigmoid(torch.tensor(valid_preds)).numpy()\n",
    "        \n",
    "        # np.save(\"preds.npy\", preds)\n",
    "        # np.save(\"actuals.npy\", original)\n",
    "        \n",
    "        rho_val = np.mean([spearmanr(original[:, i], preds[:,i]).correlation for i in range(preds.shape[1])])\n",
    "        print('\\r val_spearman-rho: %s' % (str(round(rho_val, 5))), end = 100*' '+'\\n')\n",
    "        \n",
    "        for i in range(len(target_cols)):\n",
    "            logger.info(f\"{i}, {spearmanr(original[:,i], preds[:,i])}\")\n",
    "            score += np.nan_to_num(spearmanr(original[:, i], preds[:, i]).correlation)\n",
    "        \n",
    "    return avg_val_loss, score/len(target_cols)\n",
    "\n",
    "\n",
    "def predict_valid_result(model, val_loader, val_length, batch_size=32):\n",
    "\n",
    "    val_preds = np.zeros((val_length, len(target_cols)))\n",
    "    original = np.zeros((val_length, len(target_cols)))\n",
    "\n",
    "    model.eval()\n",
    "    tk0 = tqdm(enumerate(val_loader))\n",
    "    for idx, batch in tk0:\n",
    "        if COMBINE_INPUT:\n",
    "            input_ids, input_masks, input_segments, num_features, cat_features, labels, _ = batch\n",
    "            input_ids, input_masks, input_segments, num_features, cat_features, labels = input_ids.to(device), input_masks.to(device), input_segments.to(device), num_features.to(device), cat_features.to(device), labels.to(device)            \n",
    "            with torch.no_grad():\n",
    "                outputs = model(input_ids = input_ids.long(),\n",
    "                            labels = None,\n",
    "                            attention_mask = input_masks,\n",
    "                            token_type_ids = input_segments,\n",
    "                            num_features = num_features,\n",
    "                            cat_features = cat_features,\n",
    "                            )\n",
    "        else:\n",
    "            q_input_ids, q_input_masks, q_input_segments, a_input_ids, a_input_masks, a_input_segments, num_features, cat_features, labels, _ = batch\n",
    "            q_input_ids, q_input_masks, q_input_segments, a_input_ids, a_input_masks, a_input_segments, num_features, cat_features, labels = q_input_ids.to(device), q_input_masks.to(device), q_input_segments.to(device), a_input_ids.to(device), a_input_masks.to(device), a_input_segments.to(device), num_features.to(device), cat_features.to(device), labels.to(device)\n",
    "            with torch.no_grad():\n",
    "                outputs = model(q_input_ids = q_input_ids.long(),\n",
    "                             labels = None,\n",
    "                             q_attention_mask = q_input_masks,\n",
    "                             q_token_type_ids = q_input_segments,\n",
    "                             a_input_ids = a_input_ids.long(),\n",
    "                             a_attention_mask = a_input_masks,\n",
    "                             a_token_type_ids = a_input_segments,\n",
    "                             num_features = num_features,\n",
    "                             cat_features = cat_features,\n",
    "                            )\n",
    "\n",
    "        predictions = outputs[0]\n",
    "        val_preds[idx*batch_size : (idx+1)*batch_size] = predictions.detach().cpu().squeeze().numpy()\n",
    "        original[idx*batch_size : (idx+1)*batch_size] = labels.detach().cpu().squeeze().numpy()\n",
    "\n",
    "    output = torch.sigmoid(torch.tensor(val_preds)).numpy()\n",
    "    return output, original\n",
    "\n",
    "\n",
    "def predict_result(model, test_loader, test_length, batch_size=32):\n",
    "\n",
    "    test_preds = np.zeros((test_length, len(target_cols)))\n",
    "\n",
    "    model.eval()\n",
    "    tk0 = tqdm(enumerate(test_loader))\n",
    "    for idx, x_batch in tk0:\n",
    "        if COMBINE_INPUT:\n",
    "            with torch.no_grad():\n",
    "                outputs = model(input_ids = x_batch[0].to(device),\n",
    "                            labels = None,\n",
    "                            attention_mask = x_batch[1].to(device),\n",
    "                            token_type_ids = x_batch[2].to(device),\n",
    "                            num_features = x_batch[3].to(device),\n",
    "                            cat_features = x_batch[4].to(device),\n",
    "                           )\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                outputs = model(q_input_ids = x_batch[0].to(device),\n",
    "                            labels = None,\n",
    "                            q_attention_mask = x_batch[1].to(device),\n",
    "                            q_token_type_ids = x_batch[2].to(device),\n",
    "                            a_input_ids = x_batch[3].to(device),\n",
    "                            a_attention_mask = x_batch[4].to(device),\n",
    "                            a_token_type_ids = x_batch[5].to(device),\n",
    "                            num_features = x_batch[6].to(device),\n",
    "                            cat_features = x_batch[7].to(device),\n",
    "                           )\n",
    "        predictions = outputs[0]\n",
    "        test_preds[idx*batch_size : (idx+1)*batch_size] = predictions.detach().cpu().squeeze().numpy()\n",
    "\n",
    "    output = torch.sigmoid(torch.tensor(test_preds)).numpy()\n",
    "    return output\n",
    "\n",
    "\n",
    "def add_features(df):\n",
    "    find = re.compile(r\"^[^.]*\")\n",
    "    df['netloc'] = df['url'].apply(lambda x: re.findall(find, urlparse(x).netloc)[0])\n",
    "    df['qa_same_user_page_flag'] = (df['question_user_page']==df['answer_user_page'])*1\n",
    "    df['question_title_num_words'] = df['question_title'].str.count('\\S+')\n",
    "    df['question_body_num_words'] = df['question_body'].str.count('\\S+')\n",
    "    df['answer_num_words'] = df['answer'].str.count('\\S+')\n",
    "    df['question_vs_answer_length'] = df['question_body_num_words']/df['answer_num_words']\n",
    "    df['question_title_num_words'] = np.log1p(df['question_title_num_words'])\n",
    "    df['question_body_num_words'] = np.log1p(df['question_body_num_words'])\n",
    "    df['answer_num_words'] = np.log1p(df['answer_num_words'])\n",
    "    df['question_vs_answer_length'] = np.log1p(df['question_vs_answer_length'])\n",
    "    return df\n",
    "\n",
    "\n",
    "def custom_loss(logits, labels):\n",
    "    #q_loss = nn.BCEWithLogitsLoss()(logits[:,:21], labels[:,:21])\n",
    "    #a_loss = nn.BCEWithLogitsLoss()(logits[:,21:], labels[:,21:])\n",
    "    #custom_loss = 0.5*q_loss + 0.5*a_loss\n",
    "    custom_loss = nn.BCEWithLogitsLoss()(logits, labels)\n",
    "    #loss1 = nn.BCEWithLogitsLoss()(logits[:,0:19], labels[:,0:19])\n",
    "    #loss2 = nn.BCEWithLogitsLoss()(logits[:,20:], labels[:,20:]) # except index=19\n",
    "    #custom_loss = loss1 + loss2\n",
    "    #custom_loss = 0.\n",
    "    #for i in range(len(loss_sample_weights)):\n",
    "    #    custom_loss += loss_sample_weights[i] * nn.BCEWithLogitsLoss()(logits[:,i], labels[:,i])\n",
    "    return custom_loss\n",
    "\n",
    "\n",
    "#===========================================================\n",
    "# main\n",
    "#===========================================================\n",
    "#def main():\n",
    "if True:\n",
    "    \n",
    "    with timer('Data Loading'):\n",
    "        train = pd.read_csv(f\"{ROOT}train.csv\").fillna(\"none\")\n",
    "        y_train = train[target_cols].values\n",
    "        if config.test:\n",
    "            test = pd.read_csv(f\"{ROOT}test.csv\").fillna(\"none\")\n",
    "            submission = pd.read_csv(f\"{ROOT}sample_submission.csv\")\n",
    "    \n",
    "    with timer('Num features'):\n",
    "        train = add_features(train)\n",
    "        if config.test:\n",
    "            test = add_features(test)\n",
    "        num_features = ['question_title_num_words', 'question_body_num_words', 'answer_num_words', 'question_vs_answer_length']\n",
    "        train_num = train[num_features].values\n",
    "        if config.test:\n",
    "            test_num = test[num_features].values\n",
    "                \n",
    "    with timer('Cat features'):\n",
    "        cat_features = ['netloc', 'category', 'qa_same_user_page_flag']\n",
    "        ce_oe = ce.OrdinalEncoder(cols=cat_features, handle_unknown='return_nan')\n",
    "        ce_oe.fit(train[cat_features])\n",
    "        train_cat_df = ce_oe.transform(train[cat_features])\n",
    "        test_cat_df = ce_oe.transform(test[cat_features]).fillna(0).astype(int)\n",
    "        train_cat = train_cat_df.values\n",
    "        test_cat = test_cat_df.values\n",
    "        cat_dims = []\n",
    "        for col in cat_features:\n",
    "            dim = train[col].nunique()\n",
    "            cat_dims.append((dim+1, dim//2+1)) # for unknown=0\n",
    "        print(cat_dims)\n",
    "\n",
    "    if config.train:\n",
    "        with timer('Create folds'):\n",
    "            folds = train.copy()\n",
    "\n",
    "            kf = MultilabelStratifiedKFold(n_splits=NUM_FOLDS, random_state=SEED)\n",
    "            for fold, (train_index, val_index) in enumerate(kf.split(train.values, y_train)):\n",
    "                folds.loc[val_index, 'fold'] = int(fold)\n",
    "            \"\"\"\n",
    "            # less gap between CV vs LB with GroupKFold\n",
    "            # https://www.kaggle.com/ratthachat/quest-cv-analysis-on-different-splitting-methods\n",
    "            kf = GroupKFold(n_splits=NUM_FOLDS)\n",
    "            for fold, (train_index, val_index) in enumerate(kf.split(X=train.question_body, groups=train.question_body)):\n",
    "                folds.loc[val_index, 'fold'] = int(fold)\n",
    "            \"\"\"\n",
    "            folds['fold'] = folds['fold'].astype(int)\n",
    "            save_cols = [ID] + target_cols + ['fold']\n",
    "            folds[save_cols].to_csv('folds.csv', index=None)\n",
    "\n",
    "    with timer('Prepare Bert config'):\n",
    "        tokenizer = BertTokenizer.from_pretrained(\"../input/pretrained-bert-models-for-pytorch/bert-base-uncased-vocab.txt\", \n",
    "                                                  do_lower_case=True)\n",
    "        input_categories = ['question_title', 'question_body', 'answer']\n",
    "        bert_model_config = '../input/pretrained-bert-models-for-pytorch/bert-base-uncased/bert_config.json'\n",
    "        bert_config = BertConfig.from_json_file(bert_model_config)\n",
    "        bert_config.num_labels = len(target_cols)\n",
    "        bert_model = 'bert-base-uncased'\n",
    "        do_lower_case = 'uncased' in bert_model\n",
    "        output_model_file = 'bert_pytorch.bin'\n",
    "    \n",
    "    if config.train:\n",
    "\n",
    "        BATCH_SIZE = 8\n",
    "        if DEBUG:\n",
    "            epochs = 1\n",
    "        else:\n",
    "            epochs = config.epochs\n",
    "        ACCUM_STEPS = config.accum_steps\n",
    "\n",
    "        with timer('Train Bert'):\n",
    "            \n",
    "            for fold in range(NUM_FOLDS):\n",
    "\n",
    "                logger.info(f\"Current Fold: {fold}\")\n",
    "                train_index = folds[folds.fold != fold].index\n",
    "                val_index = folds[folds.fold == fold].index\n",
    "\n",
    "                train_df, val_df = train.iloc[train_index], train.iloc[val_index]\n",
    "                logger.info(f\"Train Shapes: {train_df.shape}\")\n",
    "                logger.info(f\"Valid Shapes: {val_df.shape}\")\n",
    "            \n",
    "                logger.info(\"Preparing train datasets....\")\n",
    "            \n",
    "                inputs_train = compute_input_arays(train_df, input_categories, tokenizer, max_sequence_length=MAX_SEQUENCE_LENGTH, \n",
    "                                                   num_features=train_num[train_index], cat_features=train_cat[train_index])\n",
    "                outputs_train = compute_output_arrays(train_df, columns=target_cols)\n",
    "                outputs_train = torch.tensor(outputs_train, dtype=torch.float32)\n",
    "                lengths_train = np.argmax(inputs_train[0]==0, axis=1)\n",
    "                lengths_train[lengths_train==0] = inputs_train[0].shape[1]\n",
    "            \n",
    "                logger.info(\"Preparing valid datasets....\")\n",
    "            \n",
    "                inputs_valid = compute_input_arays(val_df, input_categories, tokenizer, max_sequence_length=MAX_SEQUENCE_LENGTH, \n",
    "                                                   num_features=train_num[val_index], cat_features=train_cat[val_index])\n",
    "                outputs_valid = compute_output_arrays(val_df, columns = target_cols)\n",
    "                outputs_valid = torch.tensor(outputs_valid, dtype=torch.float32)\n",
    "                lengths_valid = np.argmax(inputs_valid[0] == 0, axis=1)\n",
    "                lengths_valid[lengths_valid == 0] = inputs_valid[0].shape[1]\n",
    "            \n",
    "                logger.info(\"Preparing Dataloaders Datasets....\")\n",
    "\n",
    "                train_set = QuestDataset(inputs=inputs_train, lengths=lengths_train, labels=outputs_train)\n",
    "                train_sampler = RandomSampler(train_set)\n",
    "                train_loader = DataLoader(train_set, batch_size=BATCH_SIZE,sampler=train_sampler)\n",
    "            \n",
    "                valid_set = QuestDataset(inputs=inputs_valid, lengths=lengths_valid, labels=outputs_valid)\n",
    "                valid_loader = DataLoader(valid_set, batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n",
    "            \n",
    "                model = CustomBert.from_pretrained('../input/pretrained-bert-models-for-pytorch/bert-base-uncased/', config=bert_config, cat_dims=cat_dims)\n",
    "                model.zero_grad()\n",
    "                model.to(device)\n",
    "                torch.cuda.empty_cache()\n",
    "                if config.freeze : ## This is basically using out of the box bert model while training only the classifier head with our data . \n",
    "                    for param in model.bert.parameters():\n",
    "                        param.requires_grad = False\n",
    "                model.train()\n",
    "            \n",
    "                i = 0\n",
    "                best_avg_loss = 100.0\n",
    "                best_score = -1.\n",
    "                best_param_loss = None\n",
    "                best_param_score = None\n",
    "                param_optimizer = list(model.named_parameters())\n",
    "                no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "                optimizer_grouped_parameters = [\n",
    "                    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "                    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "                    ]        \n",
    "                optimizer = AdamW(optimizer_grouped_parameters, lr=config.lr, eps=4e-5)\n",
    "                #optimizer = torch.optim.AdamW(model.parameters(), lr=config.lr, eps=4e-5)\n",
    "                #criterion = nn.BCEWithLogitsLoss()\n",
    "                criterion = custom_loss\n",
    "                scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=config.warmup, num_training_steps=epochs*len(train_loader)//ACCUM_STEPS)\n",
    "                logger.info(\"Training....\")\n",
    "            \n",
    "                for epoch in tqdm(range(epochs)):\n",
    "\n",
    "                    torch.cuda.empty_cache()\n",
    "                \n",
    "                    start_time   = time.time()\n",
    "                    avg_loss, avg_loss_1, avg_loss_2, avg_loss_3, avg_loss_4, avg_loss_5 = train_model(model, train_loader, optimizer, criterion, scheduler, config)\n",
    "                    avg_val_loss, score = val_model(model, criterion, valid_loader, val_shape=val_df.shape[0], batch_size=BATCH_SIZE)\n",
    "                    elapsed_time = time.time() - start_time\n",
    "\n",
    "                    logger.info('Epoch {}/{} \\t loss={:.4f} \\t val_loss={:.4f} \\t train_loss={:.4f} \\t train_loss_1={:.4f} \\t train_loss_2={:.4f} \\t train_loss_3={:.4f} \\t train_loss_4={:.4f}  \\t train_loss_5={:.4f} \\t score={:.6f} \\t time={:.2f}s'.format(\n",
    "                        epoch+1, epochs, avg_loss, avg_val_loss, avg_loss, avg_loss_1, avg_loss_2, avg_loss_3, avg_loss_4, avg_loss_5, score, elapsed_time))\n",
    "\n",
    "                    if best_avg_loss > avg_val_loss:\n",
    "                        i = 0\n",
    "                        best_avg_loss = avg_val_loss \n",
    "                        best_param_loss = model.state_dict()\n",
    "\n",
    "                    if best_score < score:\n",
    "                        best_score = score\n",
    "                        best_param_score = model.state_dict()\n",
    "                        logger.info('best_param_score_{}_{}.pt'.format(config.expname ,fold))\n",
    "                        torch.save(best_param_score, 'best_param_score_{}_{}.pt'.format(config.expname, fold))\n",
    "                    else:\n",
    "                        i += 1\n",
    "\n",
    "            del train_df, val_df, model, optimizer, criterion, scheduler\n",
    "            del valid_loader, train_loader, valid_set, train_set\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "    \n",
    "    if config.cv:\n",
    "\n",
    "        with timer('CV'):\n",
    "\n",
    "            folds = pd.read_csv(f'{MODEL_DIR}folds.csv')\n",
    "            results = np.zeros((len(train), len(target_cols)))\n",
    "            logits = np.zeros((len(train), len(target_cols)))\n",
    "\n",
    "            for fold in range(NUM_FOLDS):\n",
    "                \n",
    "                #train_index = folds[folds.fold != fold].index\n",
    "                val_index = folds[folds.fold == fold].index\n",
    "                #train_df, val_df = train.iloc[train_index], train.iloc[val_index]\n",
    "                val_df = train.iloc[val_index]\n",
    "                \n",
    "                inputs_valid = compute_input_arays(val_df, input_categories, tokenizer, max_sequence_length=MAX_SEQUENCE_LENGTH, \n",
    "                                                   num_features=train_num[val_index], cat_features=train_cat[val_index])\n",
    "                outputs_valid = compute_output_arrays(val_df, columns = target_cols)\n",
    "                outputs_valid = torch.tensor(outputs_valid, dtype=torch.float32)\n",
    "                lengths_valid = np.argmax(inputs_valid[0] == 0, axis=1)\n",
    "                lengths_valid[lengths_valid == 0] = inputs_valid[0].shape[1]\n",
    "                valid_set = QuestDataset(inputs=inputs_valid, lengths=lengths_valid, labels=outputs_valid)\n",
    "                valid_loader = DataLoader(valid_set, batch_size=32, shuffle=False, drop_last=False)\n",
    "                \n",
    "                model = CustomBert.from_pretrained('../input/pretrained-bert-models-for-pytorch/bert-base-uncased/', config=bert_config, cat_dims=cat_dims)\n",
    "                model.zero_grad()\n",
    "                model.to(device)\n",
    "                model.load_state_dict(torch.load(f'{MODEL_DIR}best_param_score_{config.expname}_{fold}.pt'))\n",
    "                result, logit = predict_valid_result(model, valid_loader, len(val_df))  \n",
    "                results[val_index, :] = result\n",
    "                logits[val_index, :] = logit \n",
    "            \n",
    "            rho_val = np.mean([spearmanr(logits[:,i], results[:,i]).correlation for i in range(results.shape[1])])\n",
    "            logger.info(f'CV spearman-rho: {round(rho_val, 5)}')\n",
    "\n",
    "            oof = pd.DataFrame()\n",
    "            for i, col in enumerate(target_cols):\n",
    "                oof[col] = results[:,i]\n",
    "            oof.to_csv(f'oof_{config.expname}.csv', index=False)\n",
    "    \n",
    "    if config.test:\n",
    "\n",
    "        with timer('Inference'):\n",
    "\n",
    "            test_inputs = compute_input_arays(test, input_categories, tokenizer, max_sequence_length=MAX_SEQUENCE_LENGTH, \n",
    "                                              num_features=test_num, cat_features=test_cat)\n",
    "            lengths_test = np.argmax(test_inputs[0] == 0, axis=1)\n",
    "            lengths_test[lengths_test == 0] = test_inputs[0].shape[1]\n",
    "            test_set = QuestDataset(inputs=test_inputs, lengths=lengths_test, labels=None)\n",
    "            test_loader  = DataLoader(test_set, batch_size=32, shuffle=False)\n",
    "            result = np.zeros((len(test), len(target_cols)))\n",
    "\n",
    "            for fold in range(NUM_FOLDS):\n",
    "                model = CustomBert.from_pretrained('../input/pretrained-bert-models-for-pytorch/bert-base-uncased/', config=bert_config, cat_dims=cat_dims)\n",
    "                model.zero_grad()\n",
    "                model.to(device)\n",
    "                model.load_state_dict(torch.load(f'{MODEL_DIR}best_param_score_{config.expname}_{fold}.pt'))\n",
    "                result += predict_result(model, test_loader, len(test)) \n",
    "                if DEBUG:\n",
    "                    break\n",
    "                    \n",
    "            result /= NUM_FOLDS\n",
    "\n",
    "        with timer('Create submission.csv'):\n",
    "            submission.loc[:, 'question_asker_intent_understanding':] = result\n",
    "            submission.to_csv('submission2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Data Loading] start\n",
      "[Data Loading] done in 0 s\n",
      "[Num features] start\n",
      "[Num features] done in 1 s\n",
      "[Cat features] start\n",
      "[Cat features] done in 0 s\n",
      "[Prepare Bert config] start\n",
      "[Prepare Bert config] done in 0 s\n",
      "[Inference] start\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(60, 30), (6, 3), (3, 2)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15it [00:14,  1.07it/s]\n",
      "15it [00:14,  1.07it/s]\n",
      "15it [00:14,  1.07it/s]\n",
      "15it [00:14,  1.07it/s]\n",
      "15it [00:14,  1.07it/s]\n",
      "[Inference] done in 100 s\n",
      "[Create submission.csv] start\n",
      "[Create submission.csv] done in 0 s\n"
     ]
    }
   ],
   "source": [
    "#===========================================================\n",
    "# Config\n",
    "#===========================================================\n",
    "class PipeLineConfig:\n",
    "    def __init__(self, lr, warmup, accum_steps, epochs, seed, expname, \n",
    "                 head_tail, head, freeze, question_weight, answer_weight, fold, train, cv, test):\n",
    "        self.lr = lr\n",
    "        self.warmup = warmup\n",
    "        self.accum_steps = accum_steps\n",
    "        self.epochs = epochs\n",
    "        self.seed = seed\n",
    "        self.expname = expname\n",
    "        self.head_tail = head_tail\n",
    "        self.head = head\n",
    "        self.freeze = freeze\n",
    "        self.question_weight = question_weight\n",
    "        self.answer_weight = answer_weight\n",
    "        self.fold = fold\n",
    "        self.train = train\n",
    "        self.cv = cv\n",
    "        self.test = test\n",
    "\n",
    "config = PipeLineConfig(lr=5e-5, warmup=0.1, accum_steps=1, epochs=6,\n",
    "                        seed=42, expname='uncased_7', head_tail=True, head=0.5, freeze=False,\n",
    "                        question_weight=0., answer_weight=0., fold=5, train=False, cv=False, test=True)\n",
    "\n",
    "DEBUG = False\n",
    "ID = 'qa_id'\n",
    "target_cols = ['question_asker_intent_understanding', 'question_body_critical', \n",
    "               'question_conversational', 'question_expect_short_answer', \n",
    "               'question_fact_seeking', 'question_has_commonly_accepted_answer', \n",
    "               'question_interestingness_others', 'question_interestingness_self', \n",
    "               'question_multi_intent', 'question_not_really_a_question', \n",
    "               'question_opinion_seeking', 'question_type_choice',\n",
    "               'question_type_compare', 'question_type_consequence',\n",
    "               'question_type_definition', 'question_type_entity', \n",
    "               'question_type_instructions', 'question_type_procedure', \n",
    "               'question_type_reason_explanation', 'question_type_spelling', \n",
    "               'question_well_written', 'answer_helpful',\n",
    "               'answer_level_of_information', 'answer_plausible', \n",
    "               'answer_relevance', 'answer_satisfaction', \n",
    "               'answer_type_instructions', 'answer_type_procedure', \n",
    "               'answer_type_reason_explanation', 'answer_well_written']\n",
    "NUM_FOLDS = config.fold\n",
    "ROOT = '../input/google-quest-challenge/'\n",
    "#ROOT = '../input/'\n",
    "SEED = config.seed\n",
    "seed_everything(SEED)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "MODEL_DIR = '../input/googlequestchallenge-weights3/'\n",
    "#MODEL_DIR = './'\n",
    "COMBINE_INPUT = False\n",
    "T_MAX_LEN = 30\n",
    "Q_MAX_LEN = 164\n",
    "A_MAX_LEN = 254\n",
    "MAX_SEQUENCE_LENGTH = T_MAX_LEN + Q_MAX_LEN + A_MAX_LEN + 4\n",
    "q_max_sequence_length = T_MAX_LEN + Q_MAX_LEN + 3\n",
    "a_max_sequence_length = A_MAX_LEN + 2\n",
    "\n",
    "#===========================================================\n",
    "# Model\n",
    "#===========================================================\n",
    "#===========================================================\n",
    "# Model\n",
    "#===========================================================\n",
    "def _get_masks(tokens, max_seq_length):\n",
    "    \"\"\"Mask for padding\"\"\"\n",
    "    if len(tokens)>max_seq_length:\n",
    "        print(f'len(tokens): {len(tokens)}')\n",
    "        print(f'max_seq_length: {max_seq_length}')\n",
    "        raise IndexError(\"Token length more than max seq length!\")\n",
    "    return [1]*len(tokens) + [0] * (max_seq_length - len(tokens))\n",
    "\n",
    "\n",
    "def _get_segments(tokens, max_seq_length):\n",
    "    \"\"\"Segments: 0 for the first sequence, 1 for the second\"\"\"\n",
    "    \n",
    "    if len(tokens) > max_seq_length:\n",
    "        raise IndexError(\"Token length more than max seq length!\")\n",
    "        \n",
    "    segments = []\n",
    "    first_sep = True\n",
    "    current_segment_id = 0\n",
    "    \n",
    "    for token in tokens:\n",
    "        segments.append(current_segment_id)\n",
    "        if token == \"[SEP]\":\n",
    "            if first_sep:\n",
    "                first_sep = False \n",
    "            else:\n",
    "                current_segment_id = 1\n",
    "    return segments + [0] * (max_seq_length - len(tokens))\n",
    "\n",
    "\n",
    "def _get_ids(tokens, tokenizer, max_seq_length):\n",
    "    \"\"\"Token ids from Tokenizer vocab\"\"\"\n",
    "    \n",
    "    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    input_ids = token_ids + [0] * (max_seq_length-len(token_ids))\n",
    "    return input_ids\n",
    "\n",
    "\n",
    "def _trim_input(tokenizer, title, question, answer, max_sequence_length, t_max_len, q_max_len, a_max_len):\n",
    "    \n",
    "    # 350+128+30 = 508 +4 = 512\n",
    "    \n",
    "    t = tokenizer.tokenize(title)\n",
    "    q = tokenizer.tokenize(question)\n",
    "    a = tokenizer.tokenize(answer)\n",
    "    \n",
    "    t_len = len(t)\n",
    "    q_len = len(q)\n",
    "    a_len = len(a)\n",
    "\n",
    "    if (t_len+q_len+a_len+4) > max_sequence_length:\n",
    "        \n",
    "        if t_max_len > t_len:\n",
    "            t_new_len = t_len\n",
    "            a_max_len = a_max_len + floor((t_max_len - t_len)/2)\n",
    "            q_max_len = q_max_len + ceil((t_max_len - t_len)/2)\n",
    "        else:\n",
    "            t_new_len = t_max_len\n",
    "      \n",
    "        if a_max_len > a_len:\n",
    "            a_new_len = a_len \n",
    "            q_new_len = q_max_len + (a_max_len - a_len)\n",
    "        elif q_max_len > q_len:\n",
    "            a_new_len = a_max_len + (q_max_len - q_len)\n",
    "            q_new_len = q_len\n",
    "        else:\n",
    "            a_new_len = a_max_len\n",
    "            q_new_len = q_max_len\n",
    "            \n",
    "            \n",
    "        if t_new_len+a_new_len+q_new_len+4 != max_sequence_length:\n",
    "            raise ValueError(\"New sequence length should be %d, but is %d\"%(max_sequence_length, (t_new_len + a_new_len + q_new_len + 4)))\n",
    "        # Head+Tail method \n",
    "        q_len_head = round(q_new_len * config.head)\n",
    "        q_len_tail = -1 * (q_new_len - q_len_head)\n",
    "        a_len_head = round(a_new_len * config.head)\n",
    "        a_len_tail = -1 * (a_new_len - a_len_head)\n",
    "        t_len_head = round(t_new_len * config.head)\n",
    "        t_len_tail = -1 * (t_new_len - t_len_head)  \n",
    "        #t = t[:t_new_len]\n",
    "        if config.head_tail :\n",
    "            q = q[:q_len_head]+q[q_len_tail:]\n",
    "            a = a[:a_len_head]+a[a_len_tail:]\n",
    "            #t = t[:t_len_head]+t[t_len_tail:]\n",
    "            t = t[:t_new_len]\n",
    "        else:\n",
    "            # No Head+Tail , usual processing\n",
    "            q = q[:q_new_len]\n",
    "            a = a[:a_new_len]\n",
    "            t = t[:t_new_len]\n",
    "    \n",
    "    return t, q, a\n",
    "\n",
    "\n",
    "def q_trim_input(tokenizer, title, question, q_max_sequence_length, t_max_len, q_max_len):\n",
    "\n",
    "    t = tokenizer.tokenize(title)\n",
    "    q = tokenizer.tokenize(question)\n",
    "\n",
    "    t_len = len(t)\n",
    "    q_len = len(q)\n",
    "\n",
    "    if (t_len+q_len+3) > q_max_sequence_length:\n",
    "\n",
    "        if t_max_len > t_len:\n",
    "            t_new_len = t_len\n",
    "            q_max_len = q_max_len + (t_max_len - t_len)\n",
    "        else:\n",
    "            t_new_len = t_max_len\n",
    "\n",
    "        if q_max_len > q_len:\n",
    "            q_new_len = q_len\n",
    "            t_new_len = t_max_len + (q_max_len - q_len)\n",
    "        else:\n",
    "            q_new_len = q_max_len\n",
    "\n",
    "        # Head+Tail method\n",
    "        q_len_head = round(q_new_len * config.head)\n",
    "        q_len_tail = -1 * (q_new_len - q_len_head)\n",
    "        t_len_head = round(t_new_len * config.head)\n",
    "        t_len_tail = -1 * (t_new_len - t_len_head)\n",
    "        #t = t[:t_new_len]\n",
    "        if config.head_tail :\n",
    "            q = q[:q_len_head]+q[q_len_tail:]\n",
    "            t = t[:t_len_head]+t[t_len_tail:]\n",
    "            #t = t[:t_new_len]\n",
    "        else:\n",
    "            # No Head+Tail , usual processing\n",
    "            q = q[:q_new_len]\n",
    "            t = t[:t_new_len]\n",
    "\n",
    "    return t, q\n",
    "\n",
    "\n",
    "def a_trim_input(tokenizer, answer, a_max_sequence_length, a_max_len):\n",
    "\n",
    "    a = tokenizer.tokenize(answer)\n",
    "\n",
    "    a_len = len(a)\n",
    "\n",
    "    if (a_len+2) > a_max_sequence_length:\n",
    "\n",
    "        a_new_len = a_max_len\n",
    "\n",
    "        # Head+Tail method\n",
    "        a_len_head = round(a_new_len * config.head)\n",
    "        a_len_tail = -1 * (a_new_len - a_len_head)\n",
    "        if config.head_tail :\n",
    "            a = a[:a_len_head]+a[a_len_tail:]\n",
    "        else:\n",
    "            # No Head+Tail , usual processing\n",
    "            a = a[:a_new_len]\n",
    "\n",
    "    return a\n",
    "\n",
    "\n",
    "def _convert_to_bert_inputs(title, question, answer, tokenizer, max_sequence_length, combine=True):\n",
    "    \"\"\"Converts tokenized input to ids, masks and segments for BERT\"\"\"\n",
    "    if combine:\n",
    "        stoken = [\"[CLS]\"] + title + [\"[QBODY]\"] + question + [\"[ANS]\"] + answer + [\"[SEP]\"]\n",
    "        #stoken = [\"[CLS]\"] + title + [\"[SEP]\"] + question + [\"[SEP]\"] + answer + [\"[SEP]\"]\n",
    "        #stoken = [\"[CLS]\"] + title  + question  + answer + [\"[SEP]\"]\n",
    "    \n",
    "        input_ids = _get_ids(stoken, tokenizer, max_sequence_length)\n",
    "        input_masks = _get_masks(stoken, max_sequence_length)\n",
    "        input_segments = _get_segments(stoken, max_sequence_length)\n",
    "\n",
    "        return [input_ids, input_masks, input_segments]\n",
    "    else:\n",
    "        q_token = [\"[CLS]\"] + title + [\"[SEP]\"] + question + [\"[SEP\"]\n",
    "        q_input_ids = _get_ids(q_token, tokenizer, T_MAX_LEN+Q_MAX_LEN+3)\n",
    "        q_input_masks = _get_masks(q_token, T_MAX_LEN+Q_MAX_LEN+3)\n",
    "        q_input_segments = _get_segments(q_token, T_MAX_LEN+Q_MAX_LEN+3)\n",
    "        \n",
    "        a_token = [\"[CLS]\"] + answer + [\"[SEP]\"]\n",
    "        a_input_ids = _get_ids(a_token, tokenizer, A_MAX_LEN+2)\n",
    "        a_input_masks = _get_masks(a_token, A_MAX_LEN+2)\n",
    "        a_input_segments = _get_segments(a_token, A_MAX_LEN+2)\n",
    "\n",
    "        return [q_input_ids, q_input_masks, q_input_segments, a_input_ids, a_input_masks, a_input_segments]\n",
    "\n",
    "\n",
    "def compute_input_arays(df, columns, tokenizer, max_sequence_length, num_features, cat_features, \n",
    "                        t_max_len=T_MAX_LEN, q_max_len=Q_MAX_LEN, a_max_len=A_MAX_LEN):\n",
    "    if COMBINE_INPUT:\n",
    "        input_ids, input_masks, input_segments = [], [], []\n",
    "        for _, instance in df[columns].iterrows():\n",
    "            t, q, a = instance.question_title, instance.question_body, instance.answer\n",
    "            t, q, a = _trim_input(tokenizer, t, q, a, max_sequence_length, t_max_len, q_max_len, a_max_len)\n",
    "            ids, masks, segments = _convert_to_bert_inputs(t, q, a, tokenizer, max_sequence_length)\n",
    "            input_ids.append(ids)\n",
    "            input_masks.append(masks)\n",
    "            input_segments.append(segments)\n",
    "        return [\n",
    "                torch.from_numpy(np.asarray(input_ids, dtype=np.int32)).long(), \n",
    "                torch.from_numpy(np.asarray(input_masks, dtype=np.int32)).long(),\n",
    "                torch.from_numpy(np.asarray(input_segments, dtype=np.int32)).long(),\n",
    "                torch.from_numpy(np.asarray(num_features, dtype=np.float32)).float(),\n",
    "                torch.from_numpy(np.asarray(cat_features, dtype=np.int32)).long(),\n",
    "                ]\n",
    "    else:\n",
    "        input_ids, input_masks, input_segments = [], [], []\n",
    "        q_input_ids, q_input_masks, q_input_segments = [], [], []\n",
    "        a_input_ids, a_input_masks, a_input_segments = [], [], []\n",
    "        for _, instance in df[columns].iterrows():\n",
    "            # all\n",
    "            t, q, a = instance.question_title, instance.question_body, instance.answer\n",
    "            t, q, a = _trim_input(tokenizer, t, q, a, max_sequence_length, t_max_len, q_max_len, a_max_len)\n",
    "            ids, masks, segments = _convert_to_bert_inputs(t, q, a, tokenizer, max_sequence_length, combine=True)\n",
    "            input_ids.append(ids)\n",
    "            input_masks.append(masks)\n",
    "            input_segments.append(segments)\n",
    "            # q. a\n",
    "            t, q, a = instance.question_title, instance.question_body, instance.answer\n",
    "            t, q = q_trim_input(tokenizer, t, q, q_max_sequence_length, t_max_len, q_max_len)\n",
    "            a = a_trim_input(tokenizer, a, a_max_sequence_length, a_max_len)\n",
    "            q_ids, q_masks, q_segments, a_ids, a_masks, a_segments = _convert_to_bert_inputs(t, q, a, tokenizer, max_sequence_length, combine=False)\n",
    "            q_input_ids.append(q_ids)\n",
    "            q_input_masks.append(q_masks)\n",
    "            q_input_segments.append(q_segments)\n",
    "            a_input_ids.append(a_ids)\n",
    "            a_input_masks.append(a_masks)\n",
    "            a_input_segments.append(a_segments)\n",
    "        return [\n",
    "                torch.from_numpy(np.asarray(input_ids, dtype=np.int32)).long(), \n",
    "                torch.from_numpy(np.asarray(input_masks, dtype=np.int32)).long(),\n",
    "                torch.from_numpy(np.asarray(input_segments, dtype=np.int32)).long(),\n",
    "                torch.from_numpy(np.asarray(q_input_ids, dtype=np.int32)).long(),\n",
    "                torch.from_numpy(np.asarray(q_input_masks, dtype=np.int32)).long(),\n",
    "                torch.from_numpy(np.asarray(q_input_segments, dtype=np.int32)).long(),\n",
    "                torch.from_numpy(np.asarray(a_input_ids, dtype=np.int32)).long(),\n",
    "                torch.from_numpy(np.asarray(a_input_masks, dtype=np.int32)).long(),\n",
    "                torch.from_numpy(np.asarray(a_input_segments, dtype=np.int32)).long(),\n",
    "                torch.from_numpy(np.asarray(num_features, dtype=np.float32)).float(),\n",
    "                torch.from_numpy(np.asarray(cat_features, dtype=np.int32)).long(),\n",
    "                ]\n",
    "\n",
    "\n",
    "def compute_output_arrays(df, columns):\n",
    "    return np.asarray(df[columns])\n",
    "\n",
    "\n",
    "if COMBINE_INPUT:\n",
    "\n",
    "    class QuestDataset(torch.utils.data.Dataset):\n",
    "        def __init__(self, inputs, lengths, labels = None):\n",
    "\n",
    "            self.inputs = inputs\n",
    "            if labels is not None:\n",
    "                self.labels = labels\n",
    "            else:\n",
    "                self.labels = None\n",
    "            self.lengths = lengths\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "\n",
    "            input_ids       = self.inputs[0][idx]\n",
    "            input_masks     = self.inputs[1][idx]\n",
    "            input_segments  = self.inputs[2][idx]\n",
    "            num_features    = self.inputs[3][idx]\n",
    "            cat_features    = self.inputs[4][idx]\n",
    "            lengths         = self.lengths[idx]\n",
    "            if self.labels is not None: # targets\n",
    "                labels = self.labels[idx]\n",
    "                return input_ids, input_masks, input_segments, num_features, cat_features, labels, lengths\n",
    "            return input_ids, input_masks, input_segments, num_features, cat_features, lengths\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.inputs[0])\n",
    "\n",
    "\n",
    "    class CustomBert(BertPreTrainedModel):\n",
    "\n",
    "        def __init__(self, config, cat_dims):\n",
    "            super(CustomBert, self).__init__(config)\n",
    "            self.num_labels = config.num_labels\n",
    "            self.bert = BertModel(config)\n",
    "            self.embeddings = nn.ModuleList([\n",
    "                nn.Embedding(x, y) for x, y in cat_dims\n",
    "            ])\n",
    "            self.emb_drop = nn.Dropout(0.2)\n",
    "            n_emb_out = sum([y for x, y in cat_dims])\n",
    "            self.dropout = nn.Dropout(0.2)\n",
    "            self.classifier_final = nn.Linear(config.hidden_size+n_emb_out+4, self.config.num_labels)  # num_features=4\n",
    "\n",
    "            self.init_weights()\n",
    "\n",
    "        def forward(\n",
    "            self,\n",
    "            input_ids=None,\n",
    "            attention_mask=None,\n",
    "            token_type_ids=None,\n",
    "            num_features=None,\n",
    "            cat_features=None,\n",
    "            position_ids=None,\n",
    "            head_mask=None,\n",
    "            inputs_embeds=None,\n",
    "            labels=None,\n",
    "        ):\n",
    "\n",
    "            outputs = self.bert(\n",
    "                input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                token_type_ids=token_type_ids,\n",
    "                position_ids=position_ids,\n",
    "                head_mask=head_mask,\n",
    "                inputs_embeds=inputs_embeds,\n",
    "            )\n",
    "\n",
    "            pooled_output = outputs[1]\n",
    "            pooled_output = self.dropout(pooled_output)\n",
    "\n",
    "            emb = [\n",
    "                emb_layer(cat_features[:, j]) for j, emb_layer in enumerate(self.embeddings)\n",
    "            ]\n",
    "            emb = self.emb_drop(torch.cat(emb, 1))\n",
    "\n",
    "            pooled_output = torch.cat([pooled_output, num_features, emb], 1)\n",
    "            logits = self.classifier_final(pooled_output)\n",
    "\n",
    "            outputs = (logits,) + outputs[2:]  # add hidden states and attention if they are here\n",
    "            if labels is not None:\n",
    "                if self.num_labels == 1:\n",
    "                    #  We are doing regression\n",
    "                    loss_fct = MSELoss()\n",
    "                    loss = loss_fct(logits.view(-1), labels.view(-1))\n",
    "                else:\n",
    "                    loss_fct = CrossEntropyLoss()\n",
    "                    loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "                outputs = (loss,) + outputs\n",
    "\n",
    "            return outputs  # (loss), logits, (hidden_states), (attentions)\n",
    "\n",
    "else:\n",
    "\n",
    "    class QuestDataset(torch.utils.data.Dataset):\n",
    "        def __init__(self, inputs, lengths, labels = None):\n",
    "\n",
    "            self.inputs = inputs\n",
    "            if labels is not None:\n",
    "                self.labels = labels\n",
    "            else:\n",
    "                self.labels = None\n",
    "            self.lengths = lengths\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "\n",
    "            input_ids       = self.inputs[0][idx]\n",
    "            input_masks     = self.inputs[1][idx]\n",
    "            input_segments  = self.inputs[2][idx]\n",
    "            q_input_ids       = self.inputs[3][idx]\n",
    "            q_input_masks     = self.inputs[4][idx]\n",
    "            q_input_segments  = self.inputs[5][idx]\n",
    "            a_input_ids       = self.inputs[6][idx]\n",
    "            a_input_masks     = self.inputs[7][idx]\n",
    "            a_input_segments  = self.inputs[8][idx]\n",
    "            num_features    = self.inputs[9][idx]\n",
    "            cat_features    = self.inputs[10][idx]\n",
    "            lengths         = self.lengths[idx]\n",
    "            if self.labels is not None: # targets\n",
    "                labels = self.labels[idx]\n",
    "                return input_ids, input_masks, input_segments, q_input_ids, q_input_masks, q_input_segments, a_input_ids, a_input_masks, a_input_segments, num_features, cat_features, labels, lengths\n",
    "            return input_ids, input_masks, input_segments, q_input_ids, q_input_masks, q_input_segments, a_input_ids, a_input_masks, a_input_segments, num_features, cat_features, lengths\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.inputs[0])\n",
    "\n",
    "\n",
    "    class CustomBert(BertPreTrainedModel):\n",
    "\n",
    "        def __init__(self, config, cat_dims):\n",
    "            super(CustomBert, self).__init__(config)\n",
    "            self.num_labels = config.num_labels\n",
    "            self.bert = BertModel(config)\n",
    "            self.embeddings = nn.ModuleList([\n",
    "                nn.Embedding(x, y) for x, y in cat_dims\n",
    "            ])\n",
    "            self.emb_drop = nn.Dropout(0.2)\n",
    "            n_emb_out = sum([y for x, y in cat_dims])\n",
    "            self.dropout = nn.Dropout(0.2)\n",
    "            self.q_dropout = nn.Dropout(0.2)\n",
    "            self.a_dropout = nn.Dropout(0.2)\n",
    "            self.classifier_final = nn.Linear(config.hidden_size*3+n_emb_out+4, self.config.num_labels)  # num_features=4\n",
    "\n",
    "            self.init_weights()\n",
    "\n",
    "        def forward(\n",
    "            self,\n",
    "            input_ids=None,\n",
    "            attention_mask=None,\n",
    "            token_type_ids=None,\n",
    "            q_input_ids=None,\n",
    "            q_attention_mask=None,\n",
    "            q_token_type_ids=None,\n",
    "            a_input_ids=None,\n",
    "            a_attention_mask=None,\n",
    "            a_token_type_ids=None,\n",
    "            num_features=None,\n",
    "            cat_features=None,\n",
    "            position_ids=None,\n",
    "            head_mask=None,\n",
    "            inputs_embeds=None,\n",
    "            labels=None,\n",
    "        ):\n",
    "\n",
    "            outputs = self.bert(\n",
    "                input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                token_type_ids=token_type_ids,\n",
    "                position_ids=position_ids,\n",
    "                head_mask=head_mask,\n",
    "                inputs_embeds=inputs_embeds,\n",
    "            )\n",
    "\n",
    "            pooled_output = outputs[1]\n",
    "            pooled_output = self.dropout(pooled_output)\n",
    "\n",
    "            q_outputs = self.bert(\n",
    "                q_input_ids,\n",
    "                attention_mask=q_attention_mask,\n",
    "                token_type_ids=q_token_type_ids,\n",
    "                position_ids=position_ids,\n",
    "                head_mask=head_mask,\n",
    "                inputs_embeds=inputs_embeds,\n",
    "            )\n",
    "\n",
    "            q_pooled_output = q_outputs[1]\n",
    "            q_pooled_output = self.q_dropout(q_pooled_output)\n",
    "\n",
    "            a_outputs = self.bert(\n",
    "                a_input_ids,\n",
    "                attention_mask=a_attention_mask,\n",
    "                token_type_ids=a_token_type_ids,\n",
    "                position_ids=position_ids,\n",
    "                head_mask=head_mask,\n",
    "                inputs_embeds=inputs_embeds,\n",
    "            )\n",
    "\n",
    "            a_pooled_output = a_outputs[1]\n",
    "            a_pooled_output = self.a_dropout(a_pooled_output)\n",
    "\n",
    "            emb = [\n",
    "                emb_layer(cat_features[:, j]) for j, emb_layer in enumerate(self.embeddings)\n",
    "            ]\n",
    "            emb = self.emb_drop(torch.cat(emb, 1))\n",
    "\n",
    "            concat_output = torch.cat([pooled_output, q_pooled_output, a_pooled_output, num_features, emb], 1)\n",
    "            logits = self.classifier_final(concat_output)\n",
    "\n",
    "            outputs = (logits,) + q_outputs[2:] + a_outputs[2:]  # add hidden states and attention if they are here\n",
    "            if labels is not None:\n",
    "                if self.num_labels == 1:\n",
    "                    #  We are doing regression\n",
    "                    loss_fct = MSELoss()\n",
    "                    loss = loss_fct(logits.view(-1), labels.view(-1))\n",
    "                else:\n",
    "                    loss_fct = CrossEntropyLoss()\n",
    "                    loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "                outputs = (loss,) + outputs\n",
    "\n",
    "            return outputs  # (loss), logits, (hidden_states), (attentions)\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, optimizer, criterion, scheduler, config):\n",
    "    \n",
    "    model.train()\n",
    "    avg_loss = 0.\n",
    "    avg_loss_1 = 0.\n",
    "    avg_loss_2 = 0.\n",
    "    avg_loss_3 = 0.\n",
    "    avg_loss_4 = 0.\n",
    "    avg_loss_5 = 0.\n",
    "    #tk0 = tqdm(enumerate(train_loader),total =len(train_loader))\n",
    "    optimizer.zero_grad()\n",
    "    for idx, batch in enumerate(train_loader):\n",
    "        if COMBINE_INPUT:\n",
    "            input_ids, input_masks, input_segments, num_features, cat_features, labels, _ = batch\n",
    "            input_ids, input_masks, input_segments, num_features, cat_features, labels = input_ids.to(device), input_masks.to(device), input_segments.to(device), num_features.to(device), cat_features.to(device), labels.to(device)            \n",
    "        \n",
    "            output_train = model(input_ids = input_ids.long(),\n",
    "                             labels = None,\n",
    "                             attention_mask = input_masks,\n",
    "                             token_type_ids = input_segments,\n",
    "                             num_features = num_features,\n",
    "                             cat_features = cat_features,\n",
    "                            )\n",
    "        else:\n",
    "            input_ids, input_masks, input_segments, q_input_ids, q_input_masks, q_input_segments, a_input_ids, a_input_masks, a_input_segments, num_features, cat_features, labels, _ = batch\n",
    "            input_ids, input_masks, input_segments, q_input_ids, q_input_masks, q_input_segments, a_input_ids, a_input_masks, a_input_segments, num_features, cat_features, labels = \\\n",
    "            input_ids.to(device), input_masks.to(device), input_segments.to(device), q_input_ids.to(device), q_input_masks.to(device), q_input_segments.to(device), a_input_ids.to(device), a_input_masks.to(device), a_input_segments.to(device), num_features.to(device), cat_features.to(device), labels.to(device)\n",
    "\n",
    "            output_train = model(q_input_ids = q_input_ids.long(),\n",
    "                             labels = None,\n",
    "                             q_attention_mask = q_input_masks,\n",
    "                             q_token_type_ids = q_input_segments,\n",
    "                             a_input_ids = a_input_ids.long(),\n",
    "                             a_attention_mask = a_input_masks,\n",
    "                             a_token_type_ids = a_input_segments,\n",
    "                             input_ids = input_ids.long(),\n",
    "                             attention_mask = input_masks,\n",
    "                             token_type_ids = input_segments,\n",
    "                             num_features = num_features,\n",
    "                             cat_features = cat_features,\n",
    "                            )\n",
    "        logits = output_train[0] #output preds\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        if (idx + 1) % config.accum_steps == 0:    \n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "        avg_loss += loss.item() / (len(train_loader)*config.accum_steps)\n",
    "        if COMBINE_INPUT:\n",
    "            del input_ids, input_masks, input_segments, labels\n",
    "        else:\n",
    "            del q_input_ids, q_input_masks, q_input_segments, a_input_ids, a_input_masks, a_input_segments, labels\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    return avg_loss, avg_loss_1, avg_loss_2, avg_loss_3, avg_loss_4, avg_loss_5\n",
    "\n",
    "\n",
    "def val_model(model, criterion, val_loader, val_shape, batch_size=8):\n",
    "\n",
    "    avg_val_loss = 0.\n",
    "    model.eval() # eval mode\n",
    "    \n",
    "    valid_preds = np.zeros((val_shape, len(target_cols)))\n",
    "    original = np.zeros((val_shape, len(target_cols)))\n",
    "    \n",
    "    #tk0 = tqdm(enumerate(val_loader))\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for idx, batch in enumerate(val_loader):\n",
    "            if COMBINE_INPUT:\n",
    "                input_ids, input_masks, input_segments, num_features, cat_features, labels, _ = batch\n",
    "                input_ids, input_masks, input_segments, num_features, cat_features, labels = input_ids.to(device), input_masks.to(device), input_segments.to(device), num_features.to(device), cat_features.to(device), labels.to(device)            \n",
    "            \n",
    "                output_val = model(input_ids = input_ids.long(),\n",
    "                               labels = None,\n",
    "                               attention_mask = input_masks,\n",
    "                               token_type_ids = input_segments,\n",
    "                               num_features = num_features,\n",
    "                               cat_features = cat_features,\n",
    "                              )\n",
    "            else:\n",
    "                input_ids, input_masks, input_segments, q_input_ids, q_input_masks, q_input_segments, a_input_ids, a_input_masks, a_input_segments, num_features, cat_features, labels, _ = batch\n",
    "                input_ids, input_masks, input_segments, q_input_ids, q_input_masks, q_input_segments, a_input_ids, a_input_masks, a_input_segments, num_features, cat_features, labels = \\\n",
    "                input_ids.to(device), input_masks.to(device), input_segments.to(device), q_input_ids.to(device), q_input_masks.to(device), q_input_segments.to(device), a_input_ids.to(device), a_input_masks.to(device), a_input_segments.to(device), num_features.to(device), cat_features.to(device), labels.to(device)\n",
    "\n",
    "                output_val = model(q_input_ids = q_input_ids.long(),\n",
    "                             labels = None,\n",
    "                             q_attention_mask = q_input_masks,\n",
    "                             q_token_type_ids = q_input_segments,\n",
    "                             a_input_ids = a_input_ids.long(),\n",
    "                             a_attention_mask = a_input_masks,\n",
    "                             a_token_type_ids = a_input_segments,\n",
    "                             input_ids = input_ids.long(),\n",
    "                             attention_mask = input_masks,\n",
    "                             token_type_ids = input_segments,\n",
    "                             num_features = num_features,\n",
    "                             cat_features = cat_features,\n",
    "                            )\n",
    "            logits = output_val[0] #output preds\n",
    "            \n",
    "            avg_val_loss += criterion(logits, labels).item() / len(val_loader)\n",
    "            valid_preds[idx*batch_size : (idx+1)*batch_size] = logits.detach().cpu().squeeze().numpy()\n",
    "            original[idx*batch_size : (idx+1)*batch_size]    = labels.detach().cpu().squeeze().numpy()\n",
    "        \n",
    "        score = 0\n",
    "        preds = torch.sigmoid(torch.tensor(valid_preds)).numpy()\n",
    "        \n",
    "        # np.save(\"preds.npy\", preds)\n",
    "        # np.save(\"actuals.npy\", original)\n",
    "        \n",
    "        rho_val = np.mean([spearmanr(original[:, i], preds[:,i]).correlation for i in range(preds.shape[1])])\n",
    "        print('\\r val_spearman-rho: %s' % (str(round(rho_val, 5))), end = 100*' '+'\\n')\n",
    "        \n",
    "        for i in range(len(target_cols)):\n",
    "            #print(i, spearmanr(original[:,i], preds[:,i]))\n",
    "            score += np.nan_to_num(spearmanr(original[:, i], preds[:, i]).correlation)\n",
    "        \n",
    "    return avg_val_loss, score/len(target_cols)\n",
    "\n",
    "\n",
    "def predict_valid_result(model, val_loader, val_length, batch_size=32):\n",
    "\n",
    "    val_preds = np.zeros((val_length, len(target_cols)))\n",
    "    original = np.zeros((val_length, len(target_cols)))\n",
    "\n",
    "    model.eval()\n",
    "    tk0 = tqdm(enumerate(val_loader))\n",
    "    for idx, batch in tk0:\n",
    "        if COMBINE_INPUT:\n",
    "            input_ids, input_masks, input_segments, num_features, cat_features, labels, _ = batch\n",
    "            input_ids, input_masks, input_segments, num_features, cat_features, labels = input_ids.to(device), input_masks.to(device), input_segments.to(device), num_features.to(device), cat_features.to(device), labels.to(device)            \n",
    "            with torch.no_grad():\n",
    "                outputs = model(input_ids = input_ids.long(),\n",
    "                            labels = None,\n",
    "                            attention_mask = input_masks,\n",
    "                            token_type_ids = input_segments,\n",
    "                            num_features = num_features,\n",
    "                            cat_features = cat_features,\n",
    "                            )\n",
    "        else:\n",
    "            input_ids, input_masks, input_segments, q_input_ids, q_input_masks, q_input_segments, a_input_ids, a_input_masks, a_input_segments, num_features, cat_features, labels, _ = batch\n",
    "            input_ids, input_masks, input_segments, q_input_ids, q_input_masks, q_input_segments, a_input_ids, a_input_masks, a_input_segments, num_features, cat_features, labels = \\\n",
    "            input_ids.to(device), input_masks.to(device), input_segments.to(device), q_input_ids.to(device), q_input_masks.to(device), q_input_segments.to(device), a_input_ids.to(device), a_input_masks.to(device), a_input_segments.to(device), num_features.to(device), cat_features.to(device), labels.to(device)\n",
    "            with torch.no_grad():\n",
    "                outputs = model(q_input_ids = q_input_ids.long(),\n",
    "                             labels = None,\n",
    "                             q_attention_mask = q_input_masks,\n",
    "                             q_token_type_ids = q_input_segments,\n",
    "                             a_input_ids = a_input_ids.long(),\n",
    "                             a_attention_mask = a_input_masks,\n",
    "                             a_token_type_ids = a_input_segments,\n",
    "                             input_ids = input_ids.long(),\n",
    "                             attention_mask = input_masks,\n",
    "                             token_type_ids = input_segments,\n",
    "                             num_features = num_features,\n",
    "                             cat_features = cat_features,\n",
    "                            )\n",
    "\n",
    "        predictions = outputs[0]\n",
    "        val_preds[idx*batch_size : (idx+1)*batch_size] = predictions.detach().cpu().squeeze().numpy()\n",
    "        original[idx*batch_size : (idx+1)*batch_size] = labels.detach().cpu().squeeze().numpy()\n",
    "\n",
    "    output = torch.sigmoid(torch.tensor(val_preds)).numpy()\n",
    "    return output, original\n",
    "\n",
    "\n",
    "def predict_result(model, test_loader, test_length, batch_size=32):\n",
    "\n",
    "    test_preds = np.zeros((test_length, len(target_cols)))\n",
    "\n",
    "    model.eval()\n",
    "    tk0 = tqdm(enumerate(test_loader))\n",
    "    for idx, x_batch in tk0:\n",
    "        if COMBINE_INPUT:\n",
    "            with torch.no_grad():\n",
    "                outputs = model(input_ids = x_batch[0].to(device),\n",
    "                            labels = None,\n",
    "                            attention_mask = x_batch[1].to(device),\n",
    "                            token_type_ids = x_batch[2].to(device),\n",
    "                            num_features = x_batch[3].to(device),\n",
    "                            cat_features = x_batch[4].to(device),\n",
    "                           )\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                outputs = model(q_input_ids = x_batch[3].to(device),\n",
    "                            labels = None,\n",
    "                            q_attention_mask = x_batch[4].to(device),\n",
    "                            q_token_type_ids = x_batch[5].to(device),\n",
    "                            a_input_ids = x_batch[6].to(device),\n",
    "                            a_attention_mask = x_batch[7].to(device),\n",
    "                            a_token_type_ids = x_batch[8].to(device),\n",
    "                            input_ids = x_batch[0].to(device),\n",
    "                            attention_mask = x_batch[1].to(device),\n",
    "                            token_type_ids = x_batch[2].to(device),\n",
    "                            num_features = x_batch[9].to(device),\n",
    "                            cat_features = x_batch[10].to(device),\n",
    "                           )\n",
    "        predictions = outputs[0]\n",
    "        test_preds[idx*batch_size : (idx+1)*batch_size] = predictions.detach().cpu().squeeze().numpy()\n",
    "\n",
    "    output = torch.sigmoid(torch.tensor(test_preds)).numpy()\n",
    "    return output\n",
    "\n",
    "\n",
    "def add_features(df):\n",
    "    find = re.compile(r\"^[^.]*\")\n",
    "    df['netloc'] = df['url'].apply(lambda x: re.findall(find, urlparse(x).netloc)[0])\n",
    "    df['qa_same_user_page_flag'] = (df['question_user_page']==df['answer_user_page'])*1\n",
    "    df['question_title_num_words'] = df['question_title'].str.count('\\S+')\n",
    "    df['question_body_num_words'] = df['question_body'].str.count('\\S+')\n",
    "    df['answer_num_words'] = df['answer'].str.count('\\S+')\n",
    "    df['question_vs_answer_length'] = df['question_body_num_words']/df['answer_num_words']\n",
    "    df['question_title_num_words'] = np.log1p(df['question_title_num_words'])\n",
    "    df['question_body_num_words'] = np.log1p(df['question_body_num_words'])\n",
    "    df['answer_num_words'] = np.log1p(df['answer_num_words'])\n",
    "    df['question_vs_answer_length'] = np.log1p(df['question_vs_answer_length'])\n",
    "    return df\n",
    "\n",
    "\n",
    "def custom_loss(x, y):\n",
    "    #vx = x - torch.mean(x)\n",
    "    #vy = y - torch.mean(y)\n",
    "    #pearson_loss = 1 - (torch.sum(vx * vy) / (torch.sqrt(torch.sum(vx ** 2)) * torch.sqrt(torch.sum(vy ** 2))))\n",
    "    bce_loss = nn.BCEWithLogitsLoss()(x, y)\n",
    "    #loss = 0.1*pearson_loss + 0.9*bce_loss\n",
    "    return bce_loss # loss\n",
    "\n",
    "#===========================================================\n",
    "# main\n",
    "#===========================================================\n",
    "#def main():\n",
    "if True:\n",
    "    \n",
    "    with timer('Data Loading'):\n",
    "        train = pd.read_csv(f\"{ROOT}train.csv\").fillna(\"none\")\n",
    "        y_train = train[target_cols].values\n",
    "        if config.test:\n",
    "            test = pd.read_csv(f\"{ROOT}test.csv\").fillna(\"none\")\n",
    "            submission = pd.read_csv(f\"{ROOT}sample_submission.csv\")\n",
    "    \n",
    "    with timer('Num features'):\n",
    "        train = add_features(train)\n",
    "        if config.test:\n",
    "            test = add_features(test)\n",
    "        num_features = ['question_title_num_words', 'question_body_num_words', 'answer_num_words', 'question_vs_answer_length']\n",
    "        train_num = train[num_features].values\n",
    "        if config.test:\n",
    "            test_num = test[num_features].values\n",
    "                \n",
    "    with timer('Cat features'):\n",
    "        cat_features = ['netloc', 'category', 'qa_same_user_page_flag']\n",
    "        \"\"\"\n",
    "        ce_ohe = ce.OneHotEncoder(cols=features, handle_unknown='impute')\n",
    "        ce_ohe.fit(train[features])\n",
    "        #train_ohe = ce_ohe.transform(train[features]).values\n",
    "        #test_ohe = ce_ohe.transform(test[features]).values\n",
    "        train_ohe = pd.concat([_train_ohe, ce_ohe.transform(train[features])], axis=1).values\n",
    "        test_ohe = pd.concat([_test_ohe, ce_ohe.transform(test[features])], axis=1).values\n",
    "        \"\"\"\n",
    "        ce_oe = ce.OrdinalEncoder(cols=cat_features, handle_unknown='return_nan')\n",
    "        ce_oe.fit(train[cat_features])\n",
    "        train_cat_df = ce_oe.transform(train[cat_features])\n",
    "        test_cat_df = ce_oe.transform(test[cat_features]).fillna(0).astype(int)\n",
    "        #cat_df = pd.concat([train_cat_df, test_cat_df])\n",
    "        train_cat = train_cat_df.values\n",
    "        test_cat = test_cat_df.values\n",
    "        cat_dims = []\n",
    "        for col in cat_features:\n",
    "            #print(cat_df[col].unique())\n",
    "            #dim = cat_df[col].nunique()\n",
    "            dim = train[col].nunique()\n",
    "            #cat_dims.append((dim, dim//2+1))\n",
    "            cat_dims.append((dim+1, dim//2+1)) # for unknown=0\n",
    "        print(cat_dims)\n",
    "\n",
    "    if config.train:\n",
    "        with timer('Create folds'):\n",
    "            folds = train.copy()\n",
    "\n",
    "            kf = MultilabelStratifiedKFold(n_splits=NUM_FOLDS, random_state=SEED)\n",
    "            for fold, (train_index, val_index) in enumerate(kf.split(train.values, y_train)):\n",
    "                folds.loc[val_index, 'fold'] = int(fold)\n",
    "            \"\"\"\n",
    "            # less gap between CV vs LB with GroupKFold\n",
    "            # https://www.kaggle.com/ratthachat/quest-cv-analysis-on-different-splitting-methods\n",
    "            kf = GroupKFold(n_splits=NUM_FOLDS)\n",
    "            for fold, (train_index, val_index) in enumerate(kf.split(X=train.question_body, groups=train.question_body)):\n",
    "                folds.loc[val_index, 'fold'] = int(fold)\n",
    "            \"\"\"\n",
    "            folds['fold'] = folds['fold'].astype(int)\n",
    "            save_cols = [ID] + target_cols + ['fold']\n",
    "            folds[save_cols].to_csv('folds.csv', index=None)\n",
    "\n",
    "    with timer('Prepare Bert config'):\n",
    "        tokenizer = BertTokenizer.from_pretrained(\"../input/pretrained-bert-models-for-pytorch/bert-base-uncased-vocab.txt\", \n",
    "                                                  do_lower_case=True)\n",
    "        input_categories = ['question_title', 'question_body', 'answer']\n",
    "        bert_model_config = '../input/pretrained-bert-models-for-pytorch/bert-base-uncased/bert_config.json'\n",
    "        bert_config = BertConfig.from_json_file(bert_model_config)\n",
    "        bert_config.num_labels = len(target_cols)\n",
    "        bert_model = 'bert-base-uncased'\n",
    "        do_lower_case = 'uncased' in bert_model\n",
    "        output_model_file = 'bert_pytorch.bin'\n",
    "    \n",
    "    if config.train:\n",
    "\n",
    "        BATCH_SIZE = 4\n",
    "        if DEBUG:\n",
    "            epochs = 1\n",
    "        else:\n",
    "            epochs = config.epochs\n",
    "        ACCUM_STEPS = config.accum_steps\n",
    "\n",
    "        with timer('Train Bert'):\n",
    "            \n",
    "            for fold in range(NUM_FOLDS):\n",
    "\n",
    "                logger.info(f\"Current Fold: {fold}\")\n",
    "                train_index = folds[folds.fold != fold].index\n",
    "                val_index = folds[folds.fold == fold].index\n",
    "\n",
    "                train_df, val_df = train.iloc[train_index], train.iloc[val_index]\n",
    "                logger.info(f\"Train Shapes: {train_df.shape}\")\n",
    "                logger.info(f\"Valid Shapes: {val_df.shape}\")\n",
    "            \n",
    "                logger.info(\"Preparing train datasets....\")\n",
    "            \n",
    "                inputs_train = compute_input_arays(train_df, input_categories, tokenizer, max_sequence_length=MAX_SEQUENCE_LENGTH, \n",
    "                                                   num_features=train_num[train_index], cat_features=train_cat[train_index])\n",
    "                outputs_train = compute_output_arrays(train_df, columns=target_cols)\n",
    "                outputs_train = torch.tensor(outputs_train, dtype=torch.float32)\n",
    "                lengths_train = np.argmax(inputs_train[0]==0, axis=1)\n",
    "                lengths_train[lengths_train==0] = inputs_train[0].shape[1]\n",
    "            \n",
    "                logger.info(\"Preparing valid datasets....\")\n",
    "            \n",
    "                inputs_valid = compute_input_arays(val_df, input_categories, tokenizer, max_sequence_length=MAX_SEQUENCE_LENGTH, \n",
    "                                                   num_features=train_num[val_index], cat_features=train_cat[val_index])\n",
    "                outputs_valid = compute_output_arrays(val_df, columns = target_cols)\n",
    "                outputs_valid = torch.tensor(outputs_valid, dtype=torch.float32)\n",
    "                lengths_valid = np.argmax(inputs_valid[0] == 0, axis=1)\n",
    "                lengths_valid[lengths_valid == 0] = inputs_valid[0].shape[1]\n",
    "            \n",
    "                logger.info(\"Preparing Dataloaders Datasets....\")\n",
    "\n",
    "                train_set = QuestDataset(inputs=inputs_train, lengths=lengths_train, labels=outputs_train)\n",
    "                train_sampler = RandomSampler(train_set)\n",
    "                train_loader = DataLoader(train_set, batch_size=BATCH_SIZE,sampler=train_sampler)\n",
    "            \n",
    "                valid_set = QuestDataset(inputs=inputs_valid, lengths=lengths_valid, labels=outputs_valid)\n",
    "                valid_loader = DataLoader(valid_set, batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n",
    "            \n",
    "                model = CustomBert.from_pretrained('../input/pretrained-bert-models-for-pytorch/bert-base-uncased/', config=bert_config, cat_dims=cat_dims)\n",
    "                model.zero_grad()\n",
    "                model.to(device)\n",
    "                torch.cuda.empty_cache()\n",
    "                if config.freeze : ## This is basically using out of the box bert model while training only the classifier head with our data . \n",
    "                    for param in model.bert.parameters():\n",
    "                        param.requires_grad = False\n",
    "                model.train()\n",
    "            \n",
    "                i = 0\n",
    "                best_avg_loss = 100.0\n",
    "                best_score = -1.\n",
    "                best_param_loss = None\n",
    "                best_param_score = None\n",
    "                param_optimizer = list(model.named_parameters())\n",
    "                no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "                optimizer_grouped_parameters = [\n",
    "                    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "                    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "                    ]        \n",
    "                optimizer = AdamW(optimizer_grouped_parameters, lr=config.lr, eps=4e-5)\n",
    "                #optimizer = torch.optim.AdamW(model.parameters(), lr=config.lr, eps=4e-5)\n",
    "                #criterion = nn.BCEWithLogitsLoss()\n",
    "                criterion = custom_loss\n",
    "                scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=config.warmup, num_training_steps=epochs*len(train_loader)//ACCUM_STEPS)\n",
    "                logger.info(\"Training....\")\n",
    "            \n",
    "                for epoch in tqdm(range(epochs)):\n",
    "\n",
    "                    torch.cuda.empty_cache()\n",
    "                \n",
    "                    start_time   = time.time()\n",
    "                    avg_loss, avg_loss_1, avg_loss_2, avg_loss_3, avg_loss_4, avg_loss_5 = train_model(model, train_loader, optimizer, criterion, scheduler, config)\n",
    "                    avg_val_loss, score = val_model(model, criterion, valid_loader, val_shape=val_df.shape[0], batch_size=BATCH_SIZE)\n",
    "                    elapsed_time = time.time() - start_time\n",
    "\n",
    "                    logger.info('Epoch {}/{} \\t loss={:.4f} \\t val_loss={:.4f} \\t train_loss={:.4f} \\t train_loss_1={:.4f} \\t train_loss_2={:.4f} \\t train_loss_3={:.4f} \\t train_loss_4={:.4f}  \\t train_loss_5={:.4f} \\t score={:.6f} \\t time={:.2f}s'.format(\n",
    "                        epoch+1, epochs, avg_loss, avg_val_loss, avg_loss, avg_loss_1, avg_loss_2, avg_loss_3, avg_loss_4, avg_loss_5, score, elapsed_time))\n",
    "\n",
    "                    if best_avg_loss > avg_val_loss:\n",
    "                        i = 0\n",
    "                        best_avg_loss = avg_val_loss \n",
    "                        best_param_loss = model.state_dict()\n",
    "\n",
    "                    if best_score < score:\n",
    "                        best_score = score\n",
    "                        best_param_score = model.state_dict()\n",
    "                        logger.info('best_param_score_{}_{}.pt'.format(config.expname ,fold))\n",
    "                        torch.save(best_param_score, 'best_param_score_{}_{}.pt'.format(config.expname, fold))\n",
    "                    else:\n",
    "                        i += 1\n",
    "\n",
    "            del train_df, val_df, model, optimizer, criterion, scheduler\n",
    "            del valid_loader, train_loader, valid_set, train_set\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "    \n",
    "    if config.cv:\n",
    "\n",
    "        with timer('CV'):\n",
    "\n",
    "            folds = pd.read_csv(f'{MODEL_DIR}folds.csv')\n",
    "            results = np.zeros((len(train), len(target_cols)))\n",
    "            logits = np.zeros((len(train), len(target_cols)))\n",
    "\n",
    "            for fold in range(NUM_FOLDS):\n",
    "                \n",
    "                #train_index = folds[folds.fold != fold].index\n",
    "                val_index = folds[folds.fold == fold].index\n",
    "                #train_df, val_df = train.iloc[train_index], train.iloc[val_index]\n",
    "                val_df = train.iloc[val_index]\n",
    "                \n",
    "                inputs_valid = compute_input_arays(val_df, input_categories, tokenizer, max_sequence_length=MAX_SEQUENCE_LENGTH, \n",
    "                                                   num_features=train_num[val_index], cat_features=train_cat[val_index])\n",
    "                outputs_valid = compute_output_arrays(val_df, columns = target_cols)\n",
    "                outputs_valid = torch.tensor(outputs_valid, dtype=torch.float32)\n",
    "                lengths_valid = np.argmax(inputs_valid[0] == 0, axis=1)\n",
    "                lengths_valid[lengths_valid == 0] = inputs_valid[0].shape[1]\n",
    "                valid_set = QuestDataset(inputs=inputs_valid, lengths=lengths_valid, labels=outputs_valid)\n",
    "                valid_loader = DataLoader(valid_set, batch_size=32, shuffle=False, drop_last=False)\n",
    "                \n",
    "                model = CustomBert.from_pretrained('../input/pretrained-bert-models-for-pytorch/bert-base-uncased/', config=bert_config, cat_dims=cat_dims)\n",
    "                model.zero_grad()\n",
    "                model.to(device)\n",
    "                model.load_state_dict(torch.load(f'{MODEL_DIR}best_param_score_{config.expname}_{fold}.pt'))\n",
    "                result, logit = predict_valid_result(model, valid_loader, len(val_df))  \n",
    "                results[val_index, :] = result\n",
    "                logits[val_index, :] = logit \n",
    "            \n",
    "            rho_val = np.mean([spearmanr(logits[:,i], results[:,i]).correlation for i in range(results.shape[1])])\n",
    "            logger.info(f'CV spearman-rho: {round(rho_val, 5)}')\n",
    "\n",
    "            oof = pd.DataFrame()\n",
    "            for i, col in enumerate(target_cols):\n",
    "                oof[col] = results[:,i]\n",
    "            oof.to_csv('oof_{config.expname}.csv', index=False)\n",
    "    \n",
    "    if config.test:\n",
    "\n",
    "        with timer('Inference'):\n",
    "\n",
    "            test_inputs = compute_input_arays(test, input_categories, tokenizer, max_sequence_length=MAX_SEQUENCE_LENGTH, \n",
    "                                              num_features=test_num, cat_features=test_cat)\n",
    "            lengths_test = np.argmax(test_inputs[0] == 0, axis=1)\n",
    "            lengths_test[lengths_test == 0] = test_inputs[0].shape[1]\n",
    "            test_set = QuestDataset(inputs=test_inputs, lengths=lengths_test, labels=None)\n",
    "            test_loader  = DataLoader(test_set, batch_size=32, shuffle=False)\n",
    "            result = np.zeros((len(test), len(target_cols)))\n",
    "\n",
    "            for fold in range(NUM_FOLDS):\n",
    "                model = CustomBert.from_pretrained('../input/pretrained-bert-models-for-pytorch/bert-base-uncased/', config=bert_config, cat_dims=cat_dims)\n",
    "                model.zero_grad()\n",
    "                model.to(device)\n",
    "                model.load_state_dict(torch.load(f'{MODEL_DIR}best_param_score_{config.expname}_{fold}.pt'))\n",
    "                result += predict_result(model, test_loader, len(test)) \n",
    "                if DEBUG:\n",
    "                    break\n",
    "                    \n",
    "            result /= NUM_FOLDS\n",
    "\n",
    "        with timer('Create submission.csv'):\n",
    "            submission.loc[:, 'question_asker_intent_understanding':] = result\n",
    "            submission.to_csv('submission3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Data Loading] start\n",
      "[Data Loading] done in 0 s\n",
      "[Num features] start\n",
      "[Num features] done in 1 s\n",
      "[Cat features] start\n",
      "[Cat features] done in 0 s\n",
      "[Prepare Bert config] start\n",
      "[Prepare Bert config] done in 0 s\n",
      "[Inference] start\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(60, 30), (6, 3), (3, 2)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15it [00:16,  1.09s/it]\n",
      "15it [00:16,  1.10s/it]\n",
      "15it [00:16,  1.09s/it]\n",
      "15it [00:16,  1.09s/it]\n",
      "15it [00:16,  1.10s/it]\n",
      "[Inference] done in 106 s\n",
      "[Create submission.csv] start\n",
      "[Create submission.csv] done in 0 s\n"
     ]
    }
   ],
   "source": [
    "#===========================================================\n",
    "# Config\n",
    "#===========================================================\n",
    "class PipeLineConfig:\n",
    "    def __init__(self, lr, warmup, accum_steps, epochs, seed, expname, \n",
    "                 head_tail, head, freeze, question_weight, answer_weight, fold, train, cv, test):\n",
    "        self.lr = lr\n",
    "        self.warmup = warmup\n",
    "        self.accum_steps = accum_steps\n",
    "        self.epochs = epochs\n",
    "        self.seed = seed\n",
    "        self.expname = expname\n",
    "        self.head_tail = head_tail\n",
    "        self.head = head\n",
    "        self.freeze = freeze\n",
    "        self.question_weight = question_weight\n",
    "        self.answer_weight = answer_weight\n",
    "        self.fold = fold\n",
    "        self.train = train\n",
    "        self.cv = cv\n",
    "        self.test = test\n",
    "\n",
    "config = PipeLineConfig(lr=5e-5, warmup=0.1, accum_steps=1, epochs=6,\n",
    "                        seed=42, expname='cased_1', head_tail=True, head=0.5, freeze=False,\n",
    "                        question_weight=0., answer_weight=0., fold=5, train=False, cv=False, test=True)\n",
    "\n",
    "DEBUG = False\n",
    "ID = 'qa_id'\n",
    "target_cols = ['question_asker_intent_understanding', 'question_body_critical', \n",
    "               'question_conversational', 'question_expect_short_answer', \n",
    "               'question_fact_seeking', 'question_has_commonly_accepted_answer', \n",
    "               'question_interestingness_others', 'question_interestingness_self', \n",
    "               'question_multi_intent', 'question_not_really_a_question', \n",
    "               'question_opinion_seeking', 'question_type_choice',\n",
    "               'question_type_compare', 'question_type_consequence',\n",
    "               'question_type_definition', 'question_type_entity', \n",
    "               'question_type_instructions', 'question_type_procedure', \n",
    "               'question_type_reason_explanation', 'question_type_spelling', \n",
    "               'question_well_written', 'answer_helpful',\n",
    "               'answer_level_of_information', 'answer_plausible', \n",
    "               'answer_relevance', 'answer_satisfaction', \n",
    "               'answer_type_instructions', 'answer_type_procedure', \n",
    "               'answer_type_reason_explanation', 'answer_well_written']\n",
    "NUM_FOLDS = config.fold\n",
    "ROOT = '../input/google-quest-challenge/'\n",
    "#ROOT = '../input/'\n",
    "SEED = config.seed\n",
    "seed_everything(SEED)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "MODEL_DIR = '../input/googlequestchallenge-weights4/'\n",
    "#MODEL_DIR = './'\n",
    "COMBINE_INPUT = False\n",
    "T_MAX_LEN = 30\n",
    "Q_MAX_LEN = 479 # 382\n",
    "A_MAX_LEN = 510 # 254 \n",
    "MAX_SEQUENCE_LENGTH = T_MAX_LEN + Q_MAX_LEN + A_MAX_LEN + 4\n",
    "q_max_sequence_length = T_MAX_LEN + Q_MAX_LEN + 3\n",
    "a_max_sequence_length = A_MAX_LEN + 2\n",
    "\n",
    "#===========================================================\n",
    "# Model\n",
    "#===========================================================\n",
    "def _get_masks(tokens, max_seq_length):\n",
    "    \"\"\"Mask for padding\"\"\"\n",
    "    if len(tokens)>max_seq_length:\n",
    "        print(f'len(tokens): {len(tokens)}')\n",
    "        print(f'max_seq_length: {max_seq_length}')\n",
    "        raise IndexError(\"Token length more than max seq length!\")\n",
    "    return [1]*len(tokens) + [0] * (max_seq_length - len(tokens))\n",
    "\n",
    "\n",
    "def _get_segments(tokens, max_seq_length):\n",
    "    \"\"\"Segments: 0 for the first sequence, 1 for the second\"\"\"\n",
    "    \n",
    "    if len(tokens) > max_seq_length:\n",
    "        raise IndexError(\"Token length more than max seq length!\")\n",
    "        \n",
    "    segments = []\n",
    "    first_sep = True\n",
    "    current_segment_id = 0\n",
    "    \n",
    "    for token in tokens:\n",
    "        segments.append(current_segment_id)\n",
    "        if token == \"[SEP]\":\n",
    "            if first_sep:\n",
    "                first_sep = False \n",
    "            else:\n",
    "                current_segment_id = 1\n",
    "    return segments + [0] * (max_seq_length - len(tokens))\n",
    "\n",
    "\n",
    "def _get_ids(tokens, tokenizer, max_seq_length):\n",
    "    \"\"\"Token ids from Tokenizer vocab\"\"\"\n",
    "    \n",
    "    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    input_ids = token_ids + [0] * (max_seq_length-len(token_ids))\n",
    "    return input_ids\n",
    "\n",
    "\n",
    "def _trim_input(tokenizer, title, question, answer, max_sequence_length, t_max_len, q_max_len, a_max_len):\n",
    "    \n",
    "    # 350+128+30 = 508 +4 = 512\n",
    "    \n",
    "    t = tokenizer.tokenize(title)\n",
    "    q = tokenizer.tokenize(question)\n",
    "    a = tokenizer.tokenize(answer)\n",
    "    \n",
    "    t_len = len(t)\n",
    "    q_len = len(q)\n",
    "    a_len = len(a)\n",
    "\n",
    "    if (t_len+q_len+a_len+4) > max_sequence_length:\n",
    "        \n",
    "        if t_max_len > t_len:\n",
    "            t_new_len = t_len\n",
    "            a_max_len = a_max_len + floor((t_max_len - t_len)/2)\n",
    "            q_max_len = q_max_len + ceil((t_max_len - t_len)/2)\n",
    "        else:\n",
    "            t_new_len = t_max_len\n",
    "      \n",
    "        if a_max_len > a_len:\n",
    "            a_new_len = a_len \n",
    "            q_new_len = q_max_len + (a_max_len - a_len)\n",
    "        elif q_max_len > q_len:\n",
    "            a_new_len = a_max_len + (q_max_len - q_len)\n",
    "            q_new_len = q_len\n",
    "        else:\n",
    "            a_new_len = a_max_len\n",
    "            q_new_len = q_max_len\n",
    "            \n",
    "            \n",
    "        if t_new_len+a_new_len+q_new_len+4 != max_sequence_length:\n",
    "            raise ValueError(\"New sequence length should be %d, but is %d\"%(max_sequence_length, (t_new_len + a_new_len + q_new_len + 4)))\n",
    "        # Head+Tail method \n",
    "        q_len_head = round(q_new_len * config.head)\n",
    "        q_len_tail = -1 * (q_new_len - q_len_head)\n",
    "        a_len_head = round(a_new_len * config.head)\n",
    "        a_len_tail = -1 * (a_new_len - a_len_head)\n",
    "        t_len_head = round(t_new_len * config.head)\n",
    "        t_len_tail = -1 * (t_new_len - t_len_head)  \n",
    "        #t = t[:t_new_len]\n",
    "        if config.head_tail :\n",
    "            q = q[:q_len_head]+q[q_len_tail:]\n",
    "            a = a[:a_len_head]+a[a_len_tail:]\n",
    "            #t = t[:t_len_head]+t[t_len_tail:]\n",
    "            t = t[:t_new_len]\n",
    "        else:\n",
    "            # No Head+Tail , usual processing\n",
    "            q = q[:q_new_len]\n",
    "            a = a[:a_new_len]\n",
    "            t = t[:t_new_len]\n",
    "    \n",
    "    return t, q, a\n",
    "\n",
    "\n",
    "def q_trim_input(tokenizer, title, question, q_max_sequence_length, t_max_len, q_max_len):\n",
    "\n",
    "    t = tokenizer.tokenize(title)\n",
    "    q = tokenizer.tokenize(question)\n",
    "\n",
    "    t_len = len(t)\n",
    "    q_len = len(q)\n",
    "\n",
    "    if (t_len+q_len+3) > q_max_sequence_length:\n",
    "\n",
    "        if t_max_len > t_len:\n",
    "            t_new_len = t_len\n",
    "            q_max_len = q_max_len + (t_max_len - t_len)\n",
    "        else:\n",
    "            t_new_len = t_max_len\n",
    "\n",
    "        if q_max_len > q_len:\n",
    "            q_new_len = q_len\n",
    "            t_new_len = t_max_len + (q_max_len - q_len)\n",
    "        else:\n",
    "            q_new_len = q_max_len\n",
    "\n",
    "        # Head+Tail method\n",
    "        q_len_head = round(q_new_len * config.head)\n",
    "        q_len_tail = -1 * (q_new_len - q_len_head)\n",
    "        t_len_head = round(t_new_len * config.head)\n",
    "        t_len_tail = -1 * (t_new_len - t_len_head)\n",
    "        #t = t[:t_new_len]\n",
    "        if config.head_tail :\n",
    "            q = q[:q_len_head]+q[q_len_tail:]\n",
    "            t = t[:t_len_head]+t[t_len_tail:]\n",
    "            #t = t[:t_new_len]\n",
    "        else:\n",
    "            # No Head+Tail , usual processing\n",
    "            q = q[:q_new_len]\n",
    "            t = t[:t_new_len]\n",
    "\n",
    "    return t, q\n",
    "\n",
    "\n",
    "def a_trim_input(tokenizer, answer, a_max_sequence_length, a_max_len):\n",
    "\n",
    "    a = tokenizer.tokenize(answer)\n",
    "\n",
    "    a_len = len(a)\n",
    "\n",
    "    if (a_len+2) > a_max_sequence_length:\n",
    "\n",
    "        a_new_len = a_max_len\n",
    "\n",
    "        # Head+Tail method\n",
    "        a_len_head = round(a_new_len * config.head)\n",
    "        a_len_tail = -1 * (a_new_len - a_len_head)\n",
    "        if config.head_tail :\n",
    "            a = a[:a_len_head]+a[a_len_tail:]\n",
    "        else:\n",
    "            # No Head+Tail , usual processing\n",
    "            a = a[:a_new_len]\n",
    "\n",
    "    return a\n",
    "\n",
    "\n",
    "def _convert_to_bert_inputs(title, question, answer, tokenizer, max_sequence_length):\n",
    "    \"\"\"Converts tokenized input to ids, masks and segments for BERT\"\"\"\n",
    "    if COMBINE_INPUT:\n",
    "        stoken = [\"[CLS]\"] + title + [\"[QBODY]\"] + question + [\"[ANS]\"] + answer + [\"[SEP]\"]\n",
    "        #stoken = [\"[CLS]\"] + title + [\"[SEP]\"] + question + [\"[SEP]\"] + answer + [\"[SEP]\"]\n",
    "        #stoken = [\"[CLS]\"] + title  + question  + answer + [\"[SEP]\"]\n",
    "    \n",
    "        input_ids = _get_ids(stoken, tokenizer, max_sequence_length)\n",
    "        input_masks = _get_masks(stoken, max_sequence_length)\n",
    "        input_segments = _get_segments(stoken, max_sequence_length)\n",
    "\n",
    "        return [input_ids, input_masks, input_segments]\n",
    "    else:\n",
    "        q_token = [\"[CLS]\"] + title + [\"[SEP]\"] + question + [\"[SEP\"]\n",
    "        q_input_ids = _get_ids(q_token, tokenizer, T_MAX_LEN+Q_MAX_LEN+3)\n",
    "        q_input_masks = _get_masks(q_token, T_MAX_LEN+Q_MAX_LEN+3)\n",
    "        q_input_segments = _get_segments(q_token, T_MAX_LEN+Q_MAX_LEN+3)\n",
    "        \n",
    "        a_token = [\"[CLS]\"] + answer + [\"[SEP]\"]\n",
    "        a_input_ids = _get_ids(a_token, tokenizer, A_MAX_LEN+2)\n",
    "        a_input_masks = _get_masks(a_token, A_MAX_LEN+2)\n",
    "        a_input_segments = _get_segments(a_token, A_MAX_LEN+2)\n",
    "\n",
    "        return [q_input_ids, q_input_masks, q_input_segments, a_input_ids, a_input_masks, a_input_segments]\n",
    "\n",
    "\n",
    "def compute_input_arays(df, columns, tokenizer, max_sequence_length, num_features, cat_features, \n",
    "                        t_max_len=T_MAX_LEN, q_max_len=Q_MAX_LEN, a_max_len=A_MAX_LEN):\n",
    "    if COMBINE_INPUT:\n",
    "        input_ids, input_masks, input_segments = [], [], []\n",
    "        for _, instance in df[columns].iterrows():\n",
    "            t, q, a = instance.question_title, instance.question_body, instance.answer\n",
    "            t, q, a = _trim_input(tokenizer, t, q, a, max_sequence_length, t_max_len, q_max_len, a_max_len)\n",
    "            ids, masks, segments = _convert_to_bert_inputs(t, q, a, tokenizer, max_sequence_length)\n",
    "            input_ids.append(ids)\n",
    "            input_masks.append(masks)\n",
    "            input_segments.append(segments)\n",
    "        return [\n",
    "                torch.from_numpy(np.asarray(input_ids, dtype=np.int32)).long(), \n",
    "                torch.from_numpy(np.asarray(input_masks, dtype=np.int32)).long(),\n",
    "                torch.from_numpy(np.asarray(input_segments, dtype=np.int32)).long(),\n",
    "                torch.from_numpy(np.asarray(num_features, dtype=np.float32)).float(),\n",
    "                torch.from_numpy(np.asarray(cat_features, dtype=np.int32)).long(),\n",
    "                ]\n",
    "    else:\n",
    "        q_input_ids, q_input_masks, q_input_segments = [], [], []\n",
    "        a_input_ids, a_input_masks, a_input_segments = [], [], []\n",
    "        for _, instance in df[columns].iterrows():\n",
    "            t, q, a = instance.question_title, instance.question_body, instance.answer\n",
    "            t, q = q_trim_input(tokenizer, t, q, q_max_sequence_length, t_max_len, q_max_len)\n",
    "            a = a_trim_input(tokenizer, a, a_max_sequence_length, a_max_len)\n",
    "            q_ids, q_masks, q_segments, a_ids, a_masks, a_segments = _convert_to_bert_inputs(t, q, a, tokenizer, max_sequence_length)\n",
    "            q_input_ids.append(q_ids)\n",
    "            q_input_masks.append(q_masks)\n",
    "            q_input_segments.append(q_segments)\n",
    "            a_input_ids.append(a_ids)\n",
    "            a_input_masks.append(a_masks)\n",
    "            a_input_segments.append(a_segments)\n",
    "        return [\n",
    "                torch.from_numpy(np.asarray(q_input_ids, dtype=np.int32)).long(),\n",
    "                torch.from_numpy(np.asarray(q_input_masks, dtype=np.int32)).long(),\n",
    "                torch.from_numpy(np.asarray(q_input_segments, dtype=np.int32)).long(),\n",
    "                torch.from_numpy(np.asarray(a_input_ids, dtype=np.int32)).long(),\n",
    "                torch.from_numpy(np.asarray(a_input_masks, dtype=np.int32)).long(),\n",
    "                torch.from_numpy(np.asarray(a_input_segments, dtype=np.int32)).long(),\n",
    "                torch.from_numpy(np.asarray(num_features, dtype=np.float32)).float(),\n",
    "                torch.from_numpy(np.asarray(cat_features, dtype=np.int32)).long(),\n",
    "                ]\n",
    "\n",
    "\n",
    "def compute_output_arrays(df, columns):\n",
    "    return np.asarray(df[columns])\n",
    "\n",
    "\n",
    "if COMBINE_INPUT:\n",
    "\n",
    "    class QuestDataset(torch.utils.data.Dataset):\n",
    "        def __init__(self, inputs, lengths, labels = None):\n",
    "\n",
    "            self.inputs = inputs\n",
    "            if labels is not None:\n",
    "                self.labels = labels\n",
    "            else:\n",
    "                self.labels = None\n",
    "            self.lengths = lengths\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "\n",
    "            input_ids       = self.inputs[0][idx]\n",
    "            input_masks     = self.inputs[1][idx]\n",
    "            input_segments  = self.inputs[2][idx]\n",
    "            num_features    = self.inputs[3][idx]\n",
    "            cat_features    = self.inputs[4][idx]\n",
    "            lengths         = self.lengths[idx]\n",
    "            if self.labels is not None: # targets\n",
    "                labels = self.labels[idx]\n",
    "                return input_ids, input_masks, input_segments, num_features, cat_features, labels, lengths\n",
    "            return input_ids, input_masks, input_segments, num_features, cat_features, lengths\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.inputs[0])\n",
    "\n",
    "\n",
    "    class CustomBert(BertPreTrainedModel):\n",
    "\n",
    "        def __init__(self, config, cat_dims):\n",
    "            super(CustomBert, self).__init__(config)\n",
    "            self.num_labels = config.num_labels\n",
    "            self.bert = BertModel(config)\n",
    "            self.embeddings = nn.ModuleList([\n",
    "                nn.Embedding(x, y) for x, y in cat_dims\n",
    "            ])\n",
    "            self.emb_drop = nn.Dropout(0.2)\n",
    "            n_emb_out = sum([y for x, y in cat_dims])\n",
    "            self.dropout = nn.Dropout(0.2)\n",
    "            self.classifier_final = nn.Linear(config.hidden_size+n_emb_out+4, self.config.num_labels)  # num_features=4\n",
    "\n",
    "            self.init_weights()\n",
    "\n",
    "        def forward(\n",
    "            self,\n",
    "            input_ids=None,\n",
    "            attention_mask=None,\n",
    "            token_type_ids=None,\n",
    "            num_features=None,\n",
    "            cat_features=None,\n",
    "            position_ids=None,\n",
    "            head_mask=None,\n",
    "            inputs_embeds=None,\n",
    "            labels=None,\n",
    "        ):\n",
    "\n",
    "            outputs = self.bert(\n",
    "                input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                token_type_ids=token_type_ids,\n",
    "                position_ids=position_ids,\n",
    "                head_mask=head_mask,\n",
    "                inputs_embeds=inputs_embeds,\n",
    "            )\n",
    "\n",
    "            pooled_output = outputs[1]\n",
    "            pooled_output = self.dropout(pooled_output)\n",
    "\n",
    "            emb = [\n",
    "                emb_layer(cat_features[:, j]) for j, emb_layer in enumerate(self.embeddings)\n",
    "            ]\n",
    "            emb = self.emb_drop(torch.cat(emb, 1))\n",
    "\n",
    "            pooled_output = torch.cat([pooled_output, num_features, emb], 1)\n",
    "            logits = self.classifier_final(pooled_output)\n",
    "\n",
    "            outputs = (logits,) + outputs[2:]  # add hidden states and attention if they are here\n",
    "            if labels is not None:\n",
    "                if self.num_labels == 1:\n",
    "                    #  We are doing regression\n",
    "                    loss_fct = MSELoss()\n",
    "                    loss = loss_fct(logits.view(-1), labels.view(-1))\n",
    "                else:\n",
    "                    loss_fct = CrossEntropyLoss()\n",
    "                    loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "                outputs = (loss,) + outputs\n",
    "\n",
    "            return outputs  # (loss), logits, (hidden_states), (attentions)\n",
    "\n",
    "else:\n",
    "\n",
    "    class QuestDataset(torch.utils.data.Dataset):\n",
    "        def __init__(self, inputs, lengths, labels = None):\n",
    "\n",
    "            self.inputs = inputs\n",
    "            if labels is not None:\n",
    "                self.labels = labels\n",
    "            else:\n",
    "                self.labels = None\n",
    "            self.lengths = lengths\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "\n",
    "            q_input_ids       = self.inputs[0][idx]\n",
    "            q_input_masks     = self.inputs[1][idx]\n",
    "            q_input_segments  = self.inputs[2][idx]\n",
    "            a_input_ids       = self.inputs[3][idx]\n",
    "            a_input_masks     = self.inputs[4][idx]\n",
    "            a_input_segments  = self.inputs[5][idx]\n",
    "            num_features    = self.inputs[6][idx]\n",
    "            cat_features    = self.inputs[7][idx]\n",
    "            lengths         = self.lengths[idx]\n",
    "            if self.labels is not None: # targets\n",
    "                labels = self.labels[idx]\n",
    "                return q_input_ids, q_input_masks, q_input_segments, a_input_ids, a_input_masks, a_input_segments, num_features, cat_features, labels, lengths\n",
    "            return q_input_ids, q_input_masks, q_input_segments, a_input_ids, a_input_masks, a_input_segments, num_features, cat_features, lengths\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.inputs[0])\n",
    "\n",
    "\n",
    "    class CustomBert(BertPreTrainedModel):\n",
    "\n",
    "        def __init__(self, config, cat_dims):\n",
    "            super(CustomBert, self).__init__(config)\n",
    "            self.num_labels = config.num_labels\n",
    "            self.bert = BertModel(config)\n",
    "            self.embeddings = nn.ModuleList([\n",
    "                nn.Embedding(x, y) for x, y in cat_dims\n",
    "            ])\n",
    "            self.emb_drop = nn.Dropout(0.1)\n",
    "            n_emb_out = sum([y for x, y in cat_dims])\n",
    "            self.num_drop = nn.Dropout(0.1)\n",
    "            self.q_dropout = nn.Dropout(0.1)\n",
    "            self.a_dropout = nn.Dropout(0.1)\n",
    "            #self.dropout_all = nn.Dropout(0.2)\n",
    "            #self.dropout_a = nn.Dropout(0.2)\n",
    "            #self.dropout_q = nn.Dropout(0.2)\n",
    "            #self.classifier_all = nn.Linear(config.hidden_size*2+n_emb_out+4, 64)  # num_features=4\n",
    "            #self.classifier_all = nn.Sequential(\n",
    "            #    nn.Linear(config.hidden_size*2+n_emb_out+4, 64),\n",
    "            #    nn.ReLU(inplace=True),\n",
    "            #)\n",
    "            #self.classifier_a = nn.Linear(config.hidden_size+n_emb_out+4, 64)  # num_features=4\n",
    "            #self.classifier_a = nn.Sequential(\n",
    "            #    nn.Linear(config.hidden_size+n_emb_out+4, 64),\n",
    "            #    nn.ReLU(inplace=True),\n",
    "            #)\n",
    "            #self.classifier_q = nn.Linear(config.hidden_size+n_emb_out+4, 64)  # num_features=4\n",
    "            #self.classifier_q = nn.Sequential(\n",
    "            #    nn.Linear(config.hidden_size+n_emb_out+4, 64),\n",
    "            #    nn.ReLU(inplace=True),\n",
    "            #)\n",
    "            self.classifier_final = nn.Linear(config.hidden_size*2+n_emb_out+4, self.config.num_labels)\n",
    "            #self.classifier_final = nn.Linear(64*3, self.config.num_labels)  # num_features=4\n",
    "            #self.classifier_final = nn.Sequential(\n",
    "            #    nn.BatchNorm1d(64*3),\n",
    "            #    nn.Linear(64*3, self.config.num_labels),\n",
    "            #)\n",
    "            self.init_weights()\n",
    "\n",
    "        def forward(\n",
    "            self,\n",
    "            q_input_ids=None,\n",
    "            q_attention_mask=None,\n",
    "            q_token_type_ids=None,\n",
    "            a_input_ids=None,\n",
    "            a_attention_mask=None,\n",
    "            a_token_type_ids=None,\n",
    "            num_features=None,\n",
    "            cat_features=None,\n",
    "            position_ids=None,\n",
    "            head_mask=None,\n",
    "            inputs_embeds=None,\n",
    "            labels=None,\n",
    "        ):\n",
    "\n",
    "            q_outputs = self.bert(\n",
    "                q_input_ids,\n",
    "                attention_mask=q_attention_mask,\n",
    "                token_type_ids=q_token_type_ids,\n",
    "                position_ids=position_ids,\n",
    "                head_mask=head_mask,\n",
    "                inputs_embeds=inputs_embeds,\n",
    "            )\n",
    "\n",
    "            q_pooled_output = q_outputs[1]\n",
    "            q_pooled_output = self.q_dropout(q_pooled_output)\n",
    "\n",
    "            a_outputs = self.bert(\n",
    "                a_input_ids,\n",
    "                attention_mask=a_attention_mask,\n",
    "                token_type_ids=a_token_type_ids,\n",
    "                position_ids=position_ids,\n",
    "                head_mask=head_mask,\n",
    "                inputs_embeds=inputs_embeds,\n",
    "            )\n",
    "\n",
    "            a_pooled_output = a_outputs[1]\n",
    "            a_pooled_output = self.a_dropout(a_pooled_output)\n",
    "\n",
    "            emb = [\n",
    "                emb_layer(cat_features[:, j]) for j, emb_layer in enumerate(self.embeddings)\n",
    "            ]\n",
    "            emb = self.emb_drop(torch.cat(emb, 1))\n",
    "\n",
    "            num_features = self.num_drop(num_features)\n",
    "\n",
    "            pooled_output = torch.cat([q_pooled_output, a_pooled_output, num_features, emb], 1)\n",
    "            #all_logits = self.classifier_all(pooled_output)\n",
    "            #all_logits = self.dropout_all(all_logits)\n",
    "            logits = self.classifier_final(pooled_output)\n",
    "            \n",
    "            #a_pooled_output = torch.cat([a_pooled_output, num_features, emb], 1)\n",
    "            #a_logits = self.classifier_a(a_pooled_output)\n",
    "            #a_logits = self.dropout_a(a_logits)\n",
    "\n",
    "            #q_pooled_output = torch.cat([q_pooled_output, num_features, emb], 1)\n",
    "            #q_logits = self.classifier_q(q_pooled_output)\n",
    "            #q_logits = self.dropout_q(q_logits)\n",
    "\n",
    "            #concat_logits = torch.cat([all_logits, q_logits, a_logits], 1)\n",
    "            #logits = self.classifier_final(concat_logits)\n",
    "\n",
    "            #logits = torch.cat([q_logits, a_logits], 1)\n",
    "\n",
    "            outputs = (logits,) + q_outputs[2:] + a_outputs[2:]  # add hidden states and attention if they are here\n",
    "            if labels is not None:\n",
    "                if self.num_labels == 1:\n",
    "                    #  We are doing regression\n",
    "                    loss_fct = MSELoss()\n",
    "                    loss = loss_fct(logits.view(-1), labels.view(-1))\n",
    "                else:\n",
    "                    loss_fct = CrossEntropyLoss()\n",
    "                    loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "                outputs = (loss,) + outputs\n",
    "\n",
    "            return outputs  # (loss), logits, (hidden_states), (attentions)\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, optimizer, criterion, scheduler, config):\n",
    "    \n",
    "    model.train()\n",
    "    avg_loss = 0.\n",
    "    avg_loss_1 = 0.\n",
    "    avg_loss_2 = 0.\n",
    "    avg_loss_3 = 0.\n",
    "    avg_loss_4 = 0.\n",
    "    avg_loss_5 = 0.\n",
    "    #tk0 = tqdm(enumerate(train_loader),total =len(train_loader))\n",
    "    optimizer.zero_grad()\n",
    "    for idx, batch in enumerate(train_loader):\n",
    "        if COMBINE_INPUT:\n",
    "            input_ids, input_masks, input_segments, num_features, cat_features, labels, _ = batch\n",
    "            input_ids, input_masks, input_segments, num_features, cat_features, labels = input_ids.to(device), input_masks.to(device), input_segments.to(device), num_features.to(device), cat_features.to(device), labels.to(device)            \n",
    "        \n",
    "            output_train = model(input_ids = input_ids.long(),\n",
    "                             labels = None,\n",
    "                             attention_mask = input_masks,\n",
    "                             token_type_ids = input_segments,\n",
    "                             num_features = num_features,\n",
    "                             cat_features = cat_features,\n",
    "                            )\n",
    "        else:\n",
    "            q_input_ids, q_input_masks, q_input_segments, a_input_ids, a_input_masks, a_input_segments, num_features, cat_features, labels, _ = batch\n",
    "            q_input_ids, q_input_masks, q_input_segments, a_input_ids, a_input_masks, a_input_segments, num_features, cat_features, labels = q_input_ids.to(device), q_input_masks.to(device), q_input_segments.to(device), a_input_ids.to(device), a_input_masks.to(device), a_input_segments.to(device), num_features.to(device), cat_features.to(device), labels.to(device)\n",
    "\n",
    "            output_train = model(q_input_ids = q_input_ids.long(),\n",
    "                             labels = None,\n",
    "                             q_attention_mask = q_input_masks,\n",
    "                             q_token_type_ids = q_input_segments,\n",
    "                             a_input_ids = a_input_ids.long(),\n",
    "                             a_attention_mask = a_input_masks,\n",
    "                             a_token_type_ids = a_input_segments,\n",
    "                             num_features = num_features,\n",
    "                             cat_features = cat_features,\n",
    "                            )\n",
    "        logits = output_train[0] #output preds\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        if (idx + 1) % config.accum_steps == 0:    \n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        avg_loss += loss.item() / (len(train_loader)*config.accum_steps)\n",
    "        if COMBINE_INPUT:\n",
    "            del input_ids, input_masks, input_segments, labels\n",
    "        else:\n",
    "            del q_input_ids, q_input_masks, q_input_segments, a_input_ids, a_input_masks, a_input_segments, labels\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    return avg_loss, avg_loss_1, avg_loss_2, avg_loss_3, avg_loss_4, avg_loss_5\n",
    "\n",
    "\n",
    "def val_model(model, criterion, val_loader, val_shape, batch_size=8):\n",
    "\n",
    "    avg_val_loss = 0.\n",
    "    model.eval() # eval mode\n",
    "    \n",
    "    valid_preds = np.zeros((val_shape, len(target_cols)))\n",
    "    original = np.zeros((val_shape, len(target_cols)))\n",
    "    \n",
    "    #tk0 = tqdm(enumerate(val_loader))\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for idx, batch in enumerate(val_loader):\n",
    "            if COMBINE_INPUT:\n",
    "                input_ids, input_masks, input_segments, num_features, cat_features, labels, _ = batch\n",
    "                input_ids, input_masks, input_segments, num_features, cat_features, labels = input_ids.to(device), input_masks.to(device), input_segments.to(device), num_features.to(device), cat_features.to(device), labels.to(device)            \n",
    "            \n",
    "                output_val = model(input_ids = input_ids.long(),\n",
    "                               labels = None,\n",
    "                               attention_mask = input_masks,\n",
    "                               token_type_ids = input_segments,\n",
    "                               num_features = num_features,\n",
    "                               cat_features = cat_features,\n",
    "                              )\n",
    "            else:\n",
    "                q_input_ids, q_input_masks, q_input_segments, a_input_ids, a_input_masks, a_input_segments, num_features, cat_features, labels, _ = batch\n",
    "                q_input_ids, q_input_masks, q_input_segments, a_input_ids, a_input_masks, a_input_segments, num_features, cat_features, labels = q_input_ids.to(device), q_input_masks.to(device), q_input_segments.to(device), a_input_ids.to(device), a_input_masks.to(device), a_input_segments.to(device), num_features.to(device), cat_features.to(device), labels.to(device)\n",
    "\n",
    "                output_val = model(q_input_ids = q_input_ids.long(),\n",
    "                             labels = None,\n",
    "                             q_attention_mask = q_input_masks,\n",
    "                             q_token_type_ids = q_input_segments,\n",
    "                             a_input_ids = a_input_ids.long(),\n",
    "                             a_attention_mask = a_input_masks,\n",
    "                             a_token_type_ids = a_input_segments,\n",
    "                             num_features = num_features,\n",
    "                             cat_features = cat_features,\n",
    "                            )\n",
    "            logits = output_val[0] #output preds\n",
    "            \n",
    "            avg_val_loss += criterion(logits, labels).item() / len(val_loader)\n",
    "            valid_preds[idx*batch_size : (idx+1)*batch_size] = logits.detach().cpu().squeeze().numpy()\n",
    "            original[idx*batch_size : (idx+1)*batch_size]    = labels.detach().cpu().squeeze().numpy()\n",
    "        \n",
    "        score = 0\n",
    "        preds = torch.sigmoid(torch.tensor(valid_preds)).numpy()\n",
    "        \n",
    "        # np.save(\"preds.npy\", preds)\n",
    "        # np.save(\"actuals.npy\", original)\n",
    "        \n",
    "        rho_val = np.mean([spearmanr(original[:, i], preds[:,i]).correlation for i in range(preds.shape[1])])\n",
    "        print('\\r val_spearman-rho: %s' % (str(round(rho_val, 5))), end = 100*' '+'\\n')\n",
    "        \n",
    "        for i in range(len(target_cols)):\n",
    "            logger.info(f\"{i}, {spearmanr(original[:,i], preds[:,i])}\")\n",
    "            score += np.nan_to_num(spearmanr(original[:, i], preds[:, i]).correlation)\n",
    "        \n",
    "    return avg_val_loss, score/len(target_cols)\n",
    "\n",
    "\n",
    "def predict_valid_result(model, val_loader, val_length, batch_size=32):\n",
    "\n",
    "    val_preds = np.zeros((val_length, len(target_cols)))\n",
    "    original = np.zeros((val_length, len(target_cols)))\n",
    "\n",
    "    model.eval()\n",
    "    tk0 = tqdm(enumerate(val_loader))\n",
    "    for idx, batch in tk0:\n",
    "        if COMBINE_INPUT:\n",
    "            input_ids, input_masks, input_segments, num_features, cat_features, labels, _ = batch\n",
    "            input_ids, input_masks, input_segments, num_features, cat_features, labels = input_ids.to(device), input_masks.to(device), input_segments.to(device), num_features.to(device), cat_features.to(device), labels.to(device)            \n",
    "            with torch.no_grad():\n",
    "                outputs = model(input_ids = input_ids.long(),\n",
    "                            labels = None,\n",
    "                            attention_mask = input_masks,\n",
    "                            token_type_ids = input_segments,\n",
    "                            num_features = num_features,\n",
    "                            cat_features = cat_features,\n",
    "                            )\n",
    "        else:\n",
    "            q_input_ids, q_input_masks, q_input_segments, a_input_ids, a_input_masks, a_input_segments, num_features, cat_features, labels, _ = batch\n",
    "            q_input_ids, q_input_masks, q_input_segments, a_input_ids, a_input_masks, a_input_segments, num_features, cat_features, labels = q_input_ids.to(device), q_input_masks.to(device), q_input_segments.to(device), a_input_ids.to(device), a_input_masks.to(device), a_input_segments.to(device), num_features.to(device), cat_features.to(device), labels.to(device)\n",
    "            with torch.no_grad():\n",
    "                outputs = model(q_input_ids = q_input_ids.long(),\n",
    "                             labels = None,\n",
    "                             q_attention_mask = q_input_masks,\n",
    "                             q_token_type_ids = q_input_segments,\n",
    "                             a_input_ids = a_input_ids.long(),\n",
    "                             a_attention_mask = a_input_masks,\n",
    "                             a_token_type_ids = a_input_segments,\n",
    "                             num_features = num_features,\n",
    "                             cat_features = cat_features,\n",
    "                            )\n",
    "\n",
    "        predictions = outputs[0]\n",
    "        val_preds[idx*batch_size : (idx+1)*batch_size] = predictions.detach().cpu().squeeze().numpy()\n",
    "        original[idx*batch_size : (idx+1)*batch_size] = labels.detach().cpu().squeeze().numpy()\n",
    "\n",
    "    output = torch.sigmoid(torch.tensor(val_preds)).numpy()\n",
    "    return output, original\n",
    "\n",
    "\n",
    "def predict_result(model, test_loader, test_length, batch_size=32):\n",
    "\n",
    "    test_preds = np.zeros((test_length, len(target_cols)))\n",
    "\n",
    "    model.eval()\n",
    "    tk0 = tqdm(enumerate(test_loader))\n",
    "    for idx, x_batch in tk0:\n",
    "        if COMBINE_INPUT:\n",
    "            with torch.no_grad():\n",
    "                outputs = model(input_ids = x_batch[0].to(device),\n",
    "                            labels = None,\n",
    "                            attention_mask = x_batch[1].to(device),\n",
    "                            token_type_ids = x_batch[2].to(device),\n",
    "                            num_features = x_batch[3].to(device),\n",
    "                            cat_features = x_batch[4].to(device),\n",
    "                           )\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                outputs = model(q_input_ids = x_batch[0].to(device),\n",
    "                            labels = None,\n",
    "                            q_attention_mask = x_batch[1].to(device),\n",
    "                            q_token_type_ids = x_batch[2].to(device),\n",
    "                            a_input_ids = x_batch[3].to(device),\n",
    "                            a_attention_mask = x_batch[4].to(device),\n",
    "                            a_token_type_ids = x_batch[5].to(device),\n",
    "                            num_features = x_batch[6].to(device),\n",
    "                            cat_features = x_batch[7].to(device),\n",
    "                           )\n",
    "        predictions = outputs[0]\n",
    "        test_preds[idx*batch_size : (idx+1)*batch_size] = predictions.detach().cpu().squeeze().numpy()\n",
    "\n",
    "    output = torch.sigmoid(torch.tensor(test_preds)).numpy()\n",
    "    return output\n",
    "\n",
    "\n",
    "def add_features(df):\n",
    "    find = re.compile(r\"^[^.]*\")\n",
    "    df['netloc'] = df['url'].apply(lambda x: re.findall(find, urlparse(x).netloc)[0])\n",
    "    df['qa_same_user_page_flag'] = (df['question_user_page']==df['answer_user_page'])*1\n",
    "    df['question_title_num_words'] = df['question_title'].str.count('\\S+')\n",
    "    df['question_body_num_words'] = df['question_body'].str.count('\\S+')\n",
    "    df['answer_num_words'] = df['answer'].str.count('\\S+')\n",
    "    df['question_vs_answer_length'] = df['question_body_num_words']/df['answer_num_words']\n",
    "    df['question_title_num_words'] = np.log1p(df['question_title_num_words'])\n",
    "    df['question_body_num_words'] = np.log1p(df['question_body_num_words'])\n",
    "    df['answer_num_words'] = np.log1p(df['answer_num_words'])\n",
    "    df['question_vs_answer_length'] = np.log1p(df['question_vs_answer_length'])\n",
    "    return df\n",
    "\n",
    "\n",
    "def custom_loss(logits, labels):\n",
    "    #q_loss = nn.BCEWithLogitsLoss()(logits[:,:21], labels[:,:21])\n",
    "    #a_loss = nn.BCEWithLogitsLoss()(logits[:,21:], labels[:,21:])\n",
    "    #custom_loss = 0.5*q_loss + 0.5*a_loss\n",
    "    custom_loss = nn.BCEWithLogitsLoss()(logits, labels)\n",
    "    #loss1 = nn.BCEWithLogitsLoss()(logits[:,0:19], labels[:,0:19])\n",
    "    #loss2 = nn.BCEWithLogitsLoss()(logits[:,20:], labels[:,20:]) # except index=19\n",
    "    #custom_loss = loss1 + loss2\n",
    "    #custom_loss = 0.\n",
    "    #for i in range(len(loss_sample_weights)):\n",
    "    #    custom_loss += loss_sample_weights[i] * nn.BCEWithLogitsLoss()(logits[:,i], labels[:,i])\n",
    "    return custom_loss\n",
    "\n",
    "\n",
    "#===========================================================\n",
    "# main\n",
    "#===========================================================\n",
    "#def main():\n",
    "if True:\n",
    "    \n",
    "    with timer('Data Loading'):\n",
    "        train = pd.read_csv(f\"{ROOT}train.csv\").fillna(\"none\")\n",
    "        y_train = train[target_cols].values\n",
    "        if config.test:\n",
    "            test = pd.read_csv(f\"{ROOT}test.csv\").fillna(\"none\")\n",
    "            submission = pd.read_csv(f\"{ROOT}sample_submission.csv\")\n",
    "    \n",
    "    with timer('Num features'):\n",
    "        train = add_features(train)\n",
    "        if config.test:\n",
    "            test = add_features(test)\n",
    "        num_features = ['question_title_num_words', 'question_body_num_words', 'answer_num_words', 'question_vs_answer_length']\n",
    "        train_num = train[num_features].values\n",
    "        if config.test:\n",
    "            test_num = test[num_features].values\n",
    "                \n",
    "    with timer('Cat features'):\n",
    "        cat_features = ['netloc', 'category', 'qa_same_user_page_flag']\n",
    "        \"\"\"\n",
    "        ce_ohe = ce.OneHotEncoder(cols=features, handle_unknown='impute')\n",
    "        ce_ohe.fit(train[features])\n",
    "        #train_ohe = ce_ohe.transform(train[features]).values\n",
    "        #test_ohe = ce_ohe.transform(test[features]).values\n",
    "        train_ohe = pd.concat([_train_ohe, ce_ohe.transform(train[features])], axis=1).values\n",
    "        test_ohe = pd.concat([_test_ohe, ce_ohe.transform(test[features])], axis=1).values\n",
    "        \"\"\"\n",
    "        ce_oe = ce.OrdinalEncoder(cols=cat_features, handle_unknown='return_nan')\n",
    "        ce_oe.fit(train[cat_features])\n",
    "        train_cat_df = ce_oe.transform(train[cat_features])\n",
    "        test_cat_df = ce_oe.transform(test[cat_features]).fillna(0).astype(int)\n",
    "        #cat_df = pd.concat([train_cat_df, test_cat_df])\n",
    "        train_cat = train_cat_df.values\n",
    "        test_cat = test_cat_df.values\n",
    "        cat_dims = []\n",
    "        for col in cat_features:\n",
    "            #print(cat_df[col].unique())\n",
    "            #dim = cat_df[col].nunique()\n",
    "            dim = train[col].nunique()\n",
    "            #cat_dims.append((dim, dim//2+1))\n",
    "            cat_dims.append((dim+1, dim//2+1)) # for unknown=0\n",
    "        print(cat_dims)\n",
    "\n",
    "    if config.train:\n",
    "        with timer('Create folds'):\n",
    "            folds = train.copy()\n",
    "\n",
    "            kf = MultilabelStratifiedKFold(n_splits=NUM_FOLDS, random_state=SEED)\n",
    "            for fold, (train_index, val_index) in enumerate(kf.split(train.values, y_train)):\n",
    "                folds.loc[val_index, 'fold'] = int(fold)\n",
    "            \"\"\"\n",
    "            # less gap between CV vs LB with GroupKFold\n",
    "            # https://www.kaggle.com/ratthachat/quest-cv-analysis-on-different-splitting-methods\n",
    "            kf = GroupKFold(n_splits=NUM_FOLDS)\n",
    "            for fold, (train_index, val_index) in enumerate(kf.split(X=train.question_body, groups=train.question_body)):\n",
    "                folds.loc[val_index, 'fold'] = int(fold)\n",
    "            \"\"\"\n",
    "            folds['fold'] = folds['fold'].astype(int)\n",
    "            save_cols = [ID] + target_cols + ['fold']\n",
    "            folds[save_cols].to_csv('folds.csv', index=None)\n",
    "\n",
    "    with timer('Prepare Bert config'):\n",
    "        tokenizer = BertTokenizer.from_pretrained(\"../input/pretrained-bert-models-for-pytorch/bert-base-cased-vocab.txt\", \n",
    "                                                  do_lower_case=True)\n",
    "        input_categories = ['question_title', 'question_body', 'answer']\n",
    "        bert_model_config = '../input/pretrained-bert-models-for-pytorch/bert-base-cased/bert_config.json'\n",
    "        bert_config = BertConfig.from_json_file(bert_model_config)\n",
    "        bert_config.num_labels = len(target_cols)\n",
    "        bert_model = 'bert-base-cased'\n",
    "        do_lower_case = 'cased' in bert_model\n",
    "        output_model_file = 'bert_pytorch.bin'\n",
    "    \n",
    "    if config.train:\n",
    "\n",
    "        BATCH_SIZE = 8\n",
    "        if DEBUG:\n",
    "            epochs = 1\n",
    "        else:\n",
    "            epochs = config.epochs\n",
    "        ACCUM_STEPS = config.accum_steps\n",
    "\n",
    "        with timer('Train Bert'):\n",
    "            \n",
    "            for fold in range(NUM_FOLDS):\n",
    "\n",
    "                logger.info(f\"Current Fold: {fold}\")\n",
    "                train_index = folds[folds.fold != fold].index\n",
    "                val_index = folds[folds.fold == fold].index\n",
    "\n",
    "                train_df, val_df = train.iloc[train_index], train.iloc[val_index]\n",
    "                logger.info(f\"Train Shapes: {train_df.shape}\")\n",
    "                logger.info(f\"Valid Shapes: {val_df.shape}\")\n",
    "            \n",
    "                logger.info(\"Preparing train datasets....\")\n",
    "            \n",
    "                inputs_train = compute_input_arays(train_df, input_categories, tokenizer, max_sequence_length=MAX_SEQUENCE_LENGTH, \n",
    "                                                   num_features=train_num[train_index], cat_features=train_cat[train_index])\n",
    "                outputs_train = compute_output_arrays(train_df, columns=target_cols)\n",
    "                outputs_train = torch.tensor(outputs_train, dtype=torch.float32)\n",
    "                lengths_train = np.argmax(inputs_train[0]==0, axis=1)\n",
    "                lengths_train[lengths_train==0] = inputs_train[0].shape[1]\n",
    "            \n",
    "                logger.info(\"Preparing valid datasets....\")\n",
    "            \n",
    "                inputs_valid = compute_input_arays(val_df, input_categories, tokenizer, max_sequence_length=MAX_SEQUENCE_LENGTH, \n",
    "                                                   num_features=train_num[val_index], cat_features=train_cat[val_index])\n",
    "                outputs_valid = compute_output_arrays(val_df, columns = target_cols)\n",
    "                outputs_valid = torch.tensor(outputs_valid, dtype=torch.float32)\n",
    "                lengths_valid = np.argmax(inputs_valid[0] == 0, axis=1)\n",
    "                lengths_valid[lengths_valid == 0] = inputs_valid[0].shape[1]\n",
    "            \n",
    "                logger.info(\"Preparing Dataloaders Datasets....\")\n",
    "\n",
    "                train_set = QuestDataset(inputs=inputs_train, lengths=lengths_train, labels=outputs_train)\n",
    "                train_sampler = RandomSampler(train_set)\n",
    "                train_loader = DataLoader(train_set, batch_size=BATCH_SIZE,sampler=train_sampler)\n",
    "            \n",
    "                valid_set = QuestDataset(inputs=inputs_valid, lengths=lengths_valid, labels=outputs_valid)\n",
    "                valid_loader = DataLoader(valid_set, batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n",
    "            \n",
    "                model = CustomBert.from_pretrained('../input/pretrained-bert-models-for-pytorch/bert-base-cased/', config=bert_config, cat_dims=cat_dims)\n",
    "                model.zero_grad()\n",
    "                model.to(device)\n",
    "                torch.cuda.empty_cache()\n",
    "                if config.freeze : ## This is basically using out of the box bert model while training only the classifier head with our data . \n",
    "                    for param in model.bert.parameters():\n",
    "                        param.requires_grad = False\n",
    "                model.train()\n",
    "            \n",
    "                i = 0\n",
    "                best_avg_loss = 100.0\n",
    "                best_score = -1.\n",
    "                best_param_loss = None\n",
    "                best_param_score = None\n",
    "                param_optimizer = list(model.named_parameters())\n",
    "                no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "                optimizer_grouped_parameters = [\n",
    "                    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "                    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "                    ]        \n",
    "                optimizer = AdamW(optimizer_grouped_parameters, lr=config.lr, eps=4e-5)\n",
    "                #optimizer = torch.optim.AdamW(model.parameters(), lr=config.lr, eps=4e-5)\n",
    "                #criterion = nn.BCEWithLogitsLoss()\n",
    "                criterion = custom_loss\n",
    "                scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=config.warmup, num_training_steps=epochs*len(train_loader)//ACCUM_STEPS)\n",
    "                logger.info(\"Training....\")\n",
    "            \n",
    "                for epoch in tqdm(range(epochs)):\n",
    "\n",
    "                    torch.cuda.empty_cache()\n",
    "                \n",
    "                    start_time   = time.time()\n",
    "                    avg_loss, avg_loss_1, avg_loss_2, avg_loss_3, avg_loss_4, avg_loss_5 = train_model(model, train_loader, optimizer, criterion, scheduler, config)\n",
    "                    avg_val_loss, score = val_model(model, criterion, valid_loader, val_shape=val_df.shape[0], batch_size=BATCH_SIZE)\n",
    "                    elapsed_time = time.time() - start_time\n",
    "\n",
    "                    logger.info('Epoch {}/{} \\t loss={:.4f} \\t val_loss={:.4f} \\t train_loss={:.4f} \\t train_loss_1={:.4f} \\t train_loss_2={:.4f} \\t train_loss_3={:.4f} \\t train_loss_4={:.4f}  \\t train_loss_5={:.4f} \\t score={:.6f} \\t time={:.2f}s'.format(\n",
    "                        epoch+1, epochs, avg_loss, avg_val_loss, avg_loss, avg_loss_1, avg_loss_2, avg_loss_3, avg_loss_4, avg_loss_5, score, elapsed_time))\n",
    "\n",
    "                    if best_avg_loss > avg_val_loss:\n",
    "                        i = 0\n",
    "                        best_avg_loss = avg_val_loss \n",
    "                        best_param_loss = model.state_dict()\n",
    "\n",
    "                    if best_score < score:\n",
    "                        best_score = score\n",
    "                        best_param_score = model.state_dict()\n",
    "                        logger.info('best_param_score_{}_{}.pt'.format(config.expname ,fold))\n",
    "                        torch.save(best_param_score, 'best_param_score_{}_{}.pt'.format(config.expname, fold))\n",
    "                    else:\n",
    "                        i += 1\n",
    "\n",
    "            del train_df, val_df, model, optimizer, criterion, scheduler\n",
    "            del valid_loader, train_loader, valid_set, train_set\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "    \n",
    "    if config.cv:\n",
    "\n",
    "        with timer('CV'):\n",
    "\n",
    "            folds = pd.read_csv(f'{MODEL_DIR}folds.csv')\n",
    "            results = np.zeros((len(train), len(target_cols)))\n",
    "            logits = np.zeros((len(train), len(target_cols)))\n",
    "\n",
    "            for fold in range(NUM_FOLDS):\n",
    "                \n",
    "                #train_index = folds[folds.fold != fold].index\n",
    "                val_index = folds[folds.fold == fold].index\n",
    "                #train_df, val_df = train.iloc[train_index], train.iloc[val_index]\n",
    "                val_df = train.iloc[val_index]\n",
    "                \n",
    "                inputs_valid = compute_input_arays(val_df, input_categories, tokenizer, max_sequence_length=MAX_SEQUENCE_LENGTH, \n",
    "                                                   num_features=train_num[val_index], cat_features=train_cat[val_index])\n",
    "                outputs_valid = compute_output_arrays(val_df, columns = target_cols)\n",
    "                outputs_valid = torch.tensor(outputs_valid, dtype=torch.float32)\n",
    "                lengths_valid = np.argmax(inputs_valid[0] == 0, axis=1)\n",
    "                lengths_valid[lengths_valid == 0] = inputs_valid[0].shape[1]\n",
    "                valid_set = QuestDataset(inputs=inputs_valid, lengths=lengths_valid, labels=outputs_valid)\n",
    "                valid_loader = DataLoader(valid_set, batch_size=32, shuffle=False, drop_last=False)\n",
    "                \n",
    "                model = CustomBert.from_pretrained('../input/pretrained-bert-models-for-pytorch/bert-base-cased/', config=bert_config, cat_dims=cat_dims)\n",
    "                model.zero_grad()\n",
    "                model.to(device)\n",
    "                model.load_state_dict(torch.load(f'{MODEL_DIR}best_param_score_{config.expname}_{fold}.pt'))\n",
    "                result, logit = predict_valid_result(model, valid_loader, len(val_df))  \n",
    "                results[val_index, :] = result\n",
    "                logits[val_index, :] = logit \n",
    "            \n",
    "            rho_val = np.mean([spearmanr(logits[:,i], results[:,i]).correlation for i in range(results.shape[1])])\n",
    "            logger.info(f'CV spearman-rho: {round(rho_val, 5)}')\n",
    "            \n",
    "            oof = pd.DataFrame()\n",
    "            for i, col in enumerate(target_cols):\n",
    "                oof[col] = results[:,i]\n",
    "            oof.to_csv('oof.csv', index=False)\n",
    "    \n",
    "    if config.test:\n",
    "\n",
    "        with timer('Inference'):\n",
    "\n",
    "            test_inputs = compute_input_arays(test, input_categories, tokenizer, max_sequence_length=MAX_SEQUENCE_LENGTH, \n",
    "                                              num_features=test_num, cat_features=test_cat)\n",
    "            lengths_test = np.argmax(test_inputs[0] == 0, axis=1)\n",
    "            lengths_test[lengths_test == 0] = test_inputs[0].shape[1]\n",
    "            test_set = QuestDataset(inputs=test_inputs, lengths=lengths_test, labels=None)\n",
    "            test_loader  = DataLoader(test_set, batch_size=32, shuffle=False)\n",
    "            result = np.zeros((len(test), len(target_cols)))\n",
    "\n",
    "            for fold in range(NUM_FOLDS):\n",
    "                model = CustomBert.from_pretrained('../input/pretrained-bert-models-for-pytorch/bert-base-cased/', config=bert_config, cat_dims=cat_dims)\n",
    "                model.zero_grad()\n",
    "                model.to(device)\n",
    "                model.load_state_dict(torch.load(f'{MODEL_DIR}best_param_score_{config.expname}_{fold}.pt'))\n",
    "                result += predict_result(model, test_loader, len(test)) \n",
    "                if DEBUG:\n",
    "                    break\n",
    "                    \n",
    "            result /= NUM_FOLDS\n",
    "\n",
    "        with timer('Create submission.csv'):\n",
    "            submission.loc[:, 'question_asker_intent_understanding':] = result\n",
    "            submission.to_csv('submission4.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Data Loading] start\n",
      "[Data Loading] done in 0 s\n",
      "[Num features] start\n",
      "[Num features] done in 1 s\n",
      "[Cat features] start\n",
      "[Cat features] done in 0 s\n",
      "[Prepare Bert config] start\n",
      "[Prepare Bert config] done in 0 s\n",
      "[Inference] start\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(60, 30), (6, 3), (3, 2)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15it [00:16,  1.09s/it]\n",
      "15it [00:16,  1.10s/it]\n",
      "15it [00:16,  1.09s/it]\n",
      "15it [00:16,  1.09s/it]\n",
      "15it [00:16,  1.09s/it]\n",
      "[Inference] done in 108 s\n",
      "[Create submission.csv] start\n",
      "[Create submission.csv] done in 0 s\n"
     ]
    }
   ],
   "source": [
    "#===========================================================\n",
    "# Config\n",
    "#===========================================================\n",
    "class PipeLineConfig:\n",
    "    def __init__(self, lr, warmup, accum_steps, epochs, seed, expname, \n",
    "                 head_tail, head, freeze, question_weight, answer_weight, fold, train, cv, test):\n",
    "        self.lr = lr\n",
    "        self.warmup = warmup\n",
    "        self.accum_steps = accum_steps\n",
    "        self.epochs = epochs\n",
    "        self.seed = seed\n",
    "        self.expname = expname\n",
    "        self.head_tail = head_tail\n",
    "        self.head = head\n",
    "        self.freeze = freeze\n",
    "        self.question_weight = question_weight\n",
    "        self.answer_weight = answer_weight\n",
    "        self.fold = fold\n",
    "        self.train = train\n",
    "        self.cv = cv\n",
    "        self.test = test\n",
    "\n",
    "config = PipeLineConfig(lr=1e-4, warmup=0.1, accum_steps=1, epochs=6,\n",
    "                        seed=42, expname='uncased_8', head_tail=True, head=0.5, freeze=False,\n",
    "                        question_weight=0., answer_weight=0., fold=5, train=False, cv=False, test=True)\n",
    "\n",
    "DEBUG = False\n",
    "ID = 'qa_id'\n",
    "target_cols = ['question_asker_intent_understanding', 'question_body_critical', \n",
    "               'question_conversational', 'question_expect_short_answer', \n",
    "               'question_fact_seeking', 'question_has_commonly_accepted_answer', \n",
    "               'question_interestingness_others', 'question_interestingness_self', \n",
    "               'question_multi_intent', 'question_not_really_a_question', \n",
    "               'question_opinion_seeking', 'question_type_choice',\n",
    "               'question_type_compare', 'question_type_consequence',\n",
    "               'question_type_definition', 'question_type_entity', \n",
    "               'question_type_instructions', 'question_type_procedure', \n",
    "               'question_type_reason_explanation', 'question_type_spelling', \n",
    "               'question_well_written', 'answer_helpful',\n",
    "               'answer_level_of_information', 'answer_plausible', \n",
    "               'answer_relevance', 'answer_satisfaction', \n",
    "               'answer_type_instructions', 'answer_type_procedure', \n",
    "               'answer_type_reason_explanation', 'answer_well_written']\n",
    "NUM_FOLDS = config.fold\n",
    "ROOT = '../input/google-quest-challenge/'\n",
    "#ROOT = '../input/'\n",
    "SEED = config.seed\n",
    "seed_everything(SEED)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "MODEL_DIR = '../input/googlequestchallenge-weights5/'\n",
    "#MODEL_DIR = './'\n",
    "COMBINE_INPUT = False\n",
    "T_MAX_LEN = 30\n",
    "Q_MAX_LEN = 479 # 382\n",
    "A_MAX_LEN = 479 # 254 \n",
    "MAX_SEQUENCE_LENGTH = T_MAX_LEN + Q_MAX_LEN + A_MAX_LEN + 4\n",
    "q_max_sequence_length = T_MAX_LEN + Q_MAX_LEN + 3\n",
    "a_max_sequence_length = T_MAX_LEN + A_MAX_LEN + 3\n",
    "\n",
    "#===========================================================\n",
    "# Model\n",
    "#===========================================================\n",
    "def _get_masks(tokens, max_seq_length):\n",
    "    \"\"\"Mask for padding\"\"\"\n",
    "    if len(tokens)>max_seq_length:\n",
    "        print(f'len(tokens): {len(tokens)}')\n",
    "        print(f'max_seq_length: {max_seq_length}')\n",
    "        raise IndexError(\"Token length more than max seq length!\")\n",
    "    return [1]*len(tokens) + [0] * (max_seq_length - len(tokens))\n",
    "\n",
    "\n",
    "def _get_segments(tokens, max_seq_length):\n",
    "    \"\"\"Segments: 0 for the first sequence, 1 for the second\"\"\"\n",
    "    \n",
    "    if len(tokens) > max_seq_length:\n",
    "        raise IndexError(\"Token length more than max seq length!\")\n",
    "        \n",
    "    segments = []\n",
    "    first_sep = True\n",
    "    current_segment_id = 0\n",
    "    \n",
    "    for token in tokens:\n",
    "        segments.append(current_segment_id)\n",
    "        if token == \"[SEP]\":\n",
    "            if first_sep:\n",
    "                first_sep = False \n",
    "            else:\n",
    "                current_segment_id = 1\n",
    "    return segments + [0] * (max_seq_length - len(tokens))\n",
    "\n",
    "\n",
    "def _get_ids(tokens, tokenizer, max_seq_length):\n",
    "    \"\"\"Token ids from Tokenizer vocab\"\"\"\n",
    "    \n",
    "    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    input_ids = token_ids + [0] * (max_seq_length-len(token_ids))\n",
    "    return input_ids\n",
    "\n",
    "\n",
    "def _trim_input(tokenizer, title, question, answer, max_sequence_length, t_max_len, q_max_len, a_max_len):\n",
    "    \n",
    "    # 350+128+30 = 508 +4 = 512\n",
    "    \n",
    "    t = tokenizer.tokenize(title)\n",
    "    q = tokenizer.tokenize(question)\n",
    "    a = tokenizer.tokenize(answer)\n",
    "    \n",
    "    t_len = len(t)\n",
    "    q_len = len(q)\n",
    "    a_len = len(a)\n",
    "\n",
    "    if (t_len+q_len+a_len+4) > max_sequence_length:\n",
    "        \n",
    "        if t_max_len > t_len:\n",
    "            t_new_len = t_len\n",
    "            a_max_len = a_max_len + floor((t_max_len - t_len)/2)\n",
    "            q_max_len = q_max_len + ceil((t_max_len - t_len)/2)\n",
    "        else:\n",
    "            t_new_len = t_max_len\n",
    "      \n",
    "        if a_max_len > a_len:\n",
    "            a_new_len = a_len \n",
    "            q_new_len = q_max_len + (a_max_len - a_len)\n",
    "        elif q_max_len > q_len:\n",
    "            a_new_len = a_max_len + (q_max_len - q_len)\n",
    "            q_new_len = q_len\n",
    "        else:\n",
    "            a_new_len = a_max_len\n",
    "            q_new_len = q_max_len\n",
    "            \n",
    "            \n",
    "        if t_new_len+a_new_len+q_new_len+4 != max_sequence_length:\n",
    "            raise ValueError(\"New sequence length should be %d, but is %d\"%(max_sequence_length, (t_new_len + a_new_len + q_new_len + 4)))\n",
    "        # Head+Tail method \n",
    "        q_len_head = round(q_new_len * config.head)\n",
    "        q_len_tail = -1 * (q_new_len - q_len_head)\n",
    "        a_len_head = round(a_new_len * config.head)\n",
    "        a_len_tail = -1 * (a_new_len - a_len_head)\n",
    "        t_len_head = round(t_new_len * config.head)\n",
    "        t_len_tail = -1 * (t_new_len - t_len_head)  \n",
    "        #t = t[:t_new_len]\n",
    "        if config.head_tail :\n",
    "            q = q[:q_len_head]+q[q_len_tail:]\n",
    "            a = a[:a_len_head]+a[a_len_tail:]\n",
    "            #t = t[:t_len_head]+t[t_len_tail:]\n",
    "            t = t[:t_new_len]\n",
    "        else:\n",
    "            # No Head+Tail , usual processing\n",
    "            q = q[:q_new_len]\n",
    "            a = a[:a_new_len]\n",
    "            t = t[:t_new_len]\n",
    "    \n",
    "    return t, q, a\n",
    "\n",
    "\n",
    "def q_trim_input(tokenizer, title, question, q_max_sequence_length, t_max_len, q_max_len):\n",
    "\n",
    "    t = tokenizer.tokenize(title)\n",
    "    q = tokenizer.tokenize(question)\n",
    "\n",
    "    t_len = len(t)\n",
    "    q_len = len(q)\n",
    "\n",
    "    if (t_len+q_len+3) > q_max_sequence_length:\n",
    "\n",
    "        if t_max_len > t_len:\n",
    "            t_new_len = t_len\n",
    "            q_max_len = q_max_len + (t_max_len - t_len)\n",
    "        else:\n",
    "            t_new_len = t_max_len\n",
    "\n",
    "        if q_max_len > q_len:\n",
    "            q_new_len = q_len\n",
    "            t_new_len = t_max_len + (q_max_len - q_len)\n",
    "        else:\n",
    "            q_new_len = q_max_len\n",
    "\n",
    "        # Head+Tail method\n",
    "        q_len_head = round(q_new_len * config.head)\n",
    "        q_len_tail = -1 * (q_new_len - q_len_head)\n",
    "        t_len_head = round(t_new_len * config.head)\n",
    "        t_len_tail = -1 * (t_new_len - t_len_head)\n",
    "        #t = t[:t_new_len]\n",
    "        if config.head_tail :\n",
    "            q = q[:q_len_head]+q[q_len_tail:]\n",
    "            t = t[:t_len_head]+t[t_len_tail:]\n",
    "            #t = t[:t_new_len]\n",
    "        else:\n",
    "            # No Head+Tail , usual processing\n",
    "            q = q[:q_new_len]\n",
    "            t = t[:t_new_len]\n",
    "\n",
    "    return t, q\n",
    "\n",
    "\"\"\"\n",
    "def a_trim_input(tokenizer, answer, a_max_sequence_length, a_max_len):\n",
    "\n",
    "    a = tokenizer.tokenize(answer)\n",
    "\n",
    "    a_len = len(a)\n",
    "\n",
    "    if (a_len+2) > a_max_sequence_length:\n",
    "\n",
    "        a_new_len = a_max_len\n",
    "\n",
    "        # Head+Tail method\n",
    "        a_len_head = round(a_new_len * config.head)\n",
    "        a_len_tail = -1 * (a_new_len - a_len_head)\n",
    "        if config.head_tail :\n",
    "            a = a[:a_len_head]+a[a_len_tail:]\n",
    "        else:\n",
    "            # No Head+Tail , usual processing\n",
    "            a = a[:a_new_len]\n",
    "\n",
    "    return a\n",
    "\"\"\"\n",
    "\n",
    "def a_trim_input(tokenizer, title, answer, a_max_sequence_length, t_max_len, a_max_len):\n",
    "\n",
    "    t = tokenizer.tokenize(title)\n",
    "    a = tokenizer.tokenize(answer)\n",
    "\n",
    "    t_len = len(t)\n",
    "    a_len = len(a)\n",
    "\n",
    "    if (t_len+a_len+3) > a_max_sequence_length:\n",
    "\n",
    "        if t_max_len > t_len:\n",
    "            t_new_len = t_len\n",
    "            a_max_len = a_max_len + (t_max_len - t_len)\n",
    "        else:\n",
    "            t_new_len = t_max_len\n",
    "\n",
    "        if a_max_len > a_len:\n",
    "            a_new_len = a_len\n",
    "            t_new_len = t_max_len + (a_max_len - a_len)\n",
    "        else:\n",
    "            a_new_len = a_max_len\n",
    "\n",
    "        # Head+Tail method\n",
    "        a_len_head = round(a_new_len * config.head)\n",
    "        a_len_tail = -1 * (a_new_len - a_len_head)\n",
    "        t_len_head = round(t_new_len * config.head)\n",
    "        t_len_tail = -1 * (t_new_len - t_len_head)\n",
    "        #t = t[:t_new_len]\n",
    "        if config.head_tail :\n",
    "            a = a[:a_len_head]+a[a_len_tail:]\n",
    "            t = t[:t_len_head]+t[t_len_tail:]\n",
    "            #t = t[:t_new_len]\n",
    "        else:\n",
    "            # No Head+Tail , usual processing\n",
    "            a = a[:a_new_len]\n",
    "            t = t[:t_new_len]\n",
    "\n",
    "    return t, a\n",
    "\n",
    "\n",
    "def _convert_to_bert_inputs(title_q, title_a, question, answer, tokenizer, max_sequence_length):\n",
    "    \"\"\"Converts tokenized input to ids, masks and segments for BERT\"\"\"\n",
    "    if COMBINE_INPUT:\n",
    "        stoken = [\"[CLS]\"] + title + [\"[QBODY]\"] + question + [\"[ANS]\"] + answer + [\"[SEP]\"]\n",
    "        #stoken = [\"[CLS]\"] + title + [\"[SEP]\"] + question + [\"[SEP]\"] + answer + [\"[SEP]\"]\n",
    "        #stoken = [\"[CLS]\"] + title  + question  + answer + [\"[SEP]\"]\n",
    "    \n",
    "        input_ids = _get_ids(stoken, tokenizer, max_sequence_length)\n",
    "        input_masks = _get_masks(stoken, max_sequence_length)\n",
    "        input_segments = _get_segments(stoken, max_sequence_length)\n",
    "\n",
    "        return [input_ids, input_masks, input_segments]\n",
    "    else:\n",
    "        q_token = [\"[CLS]\"] + title_q + [\"[SEP]\"] + question + [\"[SEP\"]\n",
    "        q_input_ids = _get_ids(q_token, tokenizer, T_MAX_LEN+Q_MAX_LEN+3)\n",
    "        q_input_masks = _get_masks(q_token, T_MAX_LEN+Q_MAX_LEN+3)\n",
    "        q_input_segments = _get_segments(q_token, T_MAX_LEN+Q_MAX_LEN+3)\n",
    "        \n",
    "        #a_token = [\"[CLS]\"] + answer + [\"[SEP]\"]\n",
    "        #a_input_ids = _get_ids(a_token, tokenizer, A_MAX_LEN+2)\n",
    "        #a_input_masks = _get_masks(a_token, A_MAX_LEN+2)\n",
    "        #a_input_segments = _get_segments(a_token, A_MAX_LEN+2)\n",
    "        a_token = [\"[CLS]\"] + title_a + [\"[SEP]\"] + answer + [\"[SEP\"]\n",
    "        a_input_ids = _get_ids(a_token, tokenizer, T_MAX_LEN+A_MAX_LEN+3)\n",
    "        a_input_masks = _get_masks(a_token, T_MAX_LEN+A_MAX_LEN+3)\n",
    "        a_input_segments = _get_segments(a_token, T_MAX_LEN+A_MAX_LEN+3)\n",
    "        \n",
    "        return [q_input_ids, q_input_masks, q_input_segments, a_input_ids, a_input_masks, a_input_segments]\n",
    "\n",
    "\n",
    "def compute_input_arays(df, columns, tokenizer, max_sequence_length, num_features, cat_features, \n",
    "                        t_max_len=T_MAX_LEN, q_max_len=Q_MAX_LEN, a_max_len=A_MAX_LEN):\n",
    "    if COMBINE_INPUT:\n",
    "        input_ids, input_masks, input_segments = [], [], []\n",
    "        for _, instance in df[columns].iterrows():\n",
    "            t, q, a = instance.question_title, instance.question_body, instance.answer\n",
    "            t, q, a = _trim_input(tokenizer, t, q, a, max_sequence_length, t_max_len, q_max_len, a_max_len)\n",
    "            ids, masks, segments = _convert_to_bert_inputs(t, q, a, tokenizer, max_sequence_length)\n",
    "            input_ids.append(ids)\n",
    "            input_masks.append(masks)\n",
    "            input_segments.append(segments)\n",
    "        return [\n",
    "                torch.from_numpy(np.asarray(input_ids, dtype=np.int32)).long(), \n",
    "                torch.from_numpy(np.asarray(input_masks, dtype=np.int32)).long(),\n",
    "                torch.from_numpy(np.asarray(input_segments, dtype=np.int32)).long(),\n",
    "                torch.from_numpy(np.asarray(num_features, dtype=np.float32)).float(),\n",
    "                torch.from_numpy(np.asarray(cat_features, dtype=np.int32)).long(),\n",
    "                ]\n",
    "    else:\n",
    "        q_input_ids, q_input_masks, q_input_segments = [], [], []\n",
    "        a_input_ids, a_input_masks, a_input_segments = [], [], []\n",
    "        for _, instance in df[columns].iterrows():\n",
    "            t, q, a = instance.question_title, instance.question_body, instance.answer\n",
    "            t_q, q = q_trim_input(tokenizer, t, q, q_max_sequence_length, t_max_len, q_max_len)\n",
    "            #a = a_trim_input(tokenizer, a, a_max_sequence_length, a_max_len)\n",
    "            t_a, a = a_trim_input(tokenizer, t, a, a_max_sequence_length, t_max_len, a_max_len)\n",
    "            q_ids, q_masks, q_segments, a_ids, a_masks, a_segments = _convert_to_bert_inputs(t_q, t_a, q, a, tokenizer, max_sequence_length)\n",
    "            q_input_ids.append(q_ids)\n",
    "            q_input_masks.append(q_masks)\n",
    "            q_input_segments.append(q_segments)\n",
    "            a_input_ids.append(a_ids)\n",
    "            a_input_masks.append(a_masks)\n",
    "            a_input_segments.append(a_segments)\n",
    "        return [\n",
    "                torch.from_numpy(np.asarray(q_input_ids, dtype=np.int32)).long(),\n",
    "                torch.from_numpy(np.asarray(q_input_masks, dtype=np.int32)).long(),\n",
    "                torch.from_numpy(np.asarray(q_input_segments, dtype=np.int32)).long(),\n",
    "                torch.from_numpy(np.asarray(a_input_ids, dtype=np.int32)).long(),\n",
    "                torch.from_numpy(np.asarray(a_input_masks, dtype=np.int32)).long(),\n",
    "                torch.from_numpy(np.asarray(a_input_segments, dtype=np.int32)).long(),\n",
    "                torch.from_numpy(np.asarray(num_features, dtype=np.float32)).float(),\n",
    "                torch.from_numpy(np.asarray(cat_features, dtype=np.int32)).long(),\n",
    "                ]\n",
    "\n",
    "\n",
    "def compute_output_arrays(df, columns):\n",
    "    return np.asarray(df[columns])\n",
    "\n",
    "\n",
    "if COMBINE_INPUT:\n",
    "\n",
    "    class QuestDataset(torch.utils.data.Dataset):\n",
    "        def __init__(self, inputs, lengths, labels = None):\n",
    "\n",
    "            self.inputs = inputs\n",
    "            if labels is not None:\n",
    "                self.labels = labels\n",
    "            else:\n",
    "                self.labels = None\n",
    "            self.lengths = lengths\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "\n",
    "            input_ids       = self.inputs[0][idx]\n",
    "            input_masks     = self.inputs[1][idx]\n",
    "            input_segments  = self.inputs[2][idx]\n",
    "            num_features    = self.inputs[3][idx]\n",
    "            cat_features    = self.inputs[4][idx]\n",
    "            lengths         = self.lengths[idx]\n",
    "            if self.labels is not None: # targets\n",
    "                labels = self.labels[idx]\n",
    "                return input_ids, input_masks, input_segments, num_features, cat_features, labels, lengths\n",
    "            return input_ids, input_masks, input_segments, num_features, cat_features, lengths\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.inputs[0])\n",
    "\n",
    "\n",
    "    class CustomBert(BertPreTrainedModel):\n",
    "\n",
    "        def __init__(self, config, cat_dims):\n",
    "            super(CustomBert, self).__init__(config)\n",
    "            self.num_labels = config.num_labels\n",
    "            self.bert = BertModel(config)\n",
    "            self.embeddings = nn.ModuleList([\n",
    "                nn.Embedding(x, y) for x, y in cat_dims\n",
    "            ])\n",
    "            self.emb_drop = nn.Dropout(0.2)\n",
    "            n_emb_out = sum([y for x, y in cat_dims])\n",
    "            self.dropout = nn.Dropout(0.2)\n",
    "            self.classifier_final = nn.Linear(config.hidden_size+n_emb_out+4, self.config.num_labels)  # num_features=4\n",
    "\n",
    "            self.init_weights()\n",
    "\n",
    "        def forward(\n",
    "            self,\n",
    "            input_ids=None,\n",
    "            attention_mask=None,\n",
    "            token_type_ids=None,\n",
    "            num_features=None,\n",
    "            cat_features=None,\n",
    "            position_ids=None,\n",
    "            head_mask=None,\n",
    "            inputs_embeds=None,\n",
    "            labels=None,\n",
    "        ):\n",
    "\n",
    "            outputs = self.bert(\n",
    "                input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                token_type_ids=token_type_ids,\n",
    "                position_ids=position_ids,\n",
    "                head_mask=head_mask,\n",
    "                inputs_embeds=inputs_embeds,\n",
    "            )\n",
    "\n",
    "            pooled_output = outputs[1]\n",
    "            pooled_output = self.dropout(pooled_output)\n",
    "\n",
    "            emb = [\n",
    "                emb_layer(cat_features[:, j]) for j, emb_layer in enumerate(self.embeddings)\n",
    "            ]\n",
    "            emb = self.emb_drop(torch.cat(emb, 1))\n",
    "\n",
    "            pooled_output = torch.cat([pooled_output, num_features, emb], 1)\n",
    "            logits = self.classifier_final(pooled_output)\n",
    "\n",
    "            outputs = (logits,) + outputs[2:]  # add hidden states and attention if they are here\n",
    "            if labels is not None:\n",
    "                if self.num_labels == 1:\n",
    "                    #  We are doing regression\n",
    "                    loss_fct = MSELoss()\n",
    "                    loss = loss_fct(logits.view(-1), labels.view(-1))\n",
    "                else:\n",
    "                    loss_fct = CrossEntropyLoss()\n",
    "                    loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "                outputs = (loss,) + outputs\n",
    "\n",
    "            return outputs  # (loss), logits, (hidden_states), (attentions)\n",
    "\n",
    "else:\n",
    "\n",
    "    class QuestDataset(torch.utils.data.Dataset):\n",
    "        def __init__(self, inputs, lengths, labels = None):\n",
    "\n",
    "            self.inputs = inputs\n",
    "            if labels is not None:\n",
    "                self.labels = labels\n",
    "            else:\n",
    "                self.labels = None\n",
    "            self.lengths = lengths\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "\n",
    "            q_input_ids       = self.inputs[0][idx]\n",
    "            q_input_masks     = self.inputs[1][idx]\n",
    "            q_input_segments  = self.inputs[2][idx]\n",
    "            a_input_ids       = self.inputs[3][idx]\n",
    "            a_input_masks     = self.inputs[4][idx]\n",
    "            a_input_segments  = self.inputs[5][idx]\n",
    "            num_features    = self.inputs[6][idx]\n",
    "            cat_features    = self.inputs[7][idx]\n",
    "            lengths         = self.lengths[idx]\n",
    "            if self.labels is not None: # targets\n",
    "                labels = self.labels[idx]\n",
    "                return q_input_ids, q_input_masks, q_input_segments, a_input_ids, a_input_masks, a_input_segments, num_features, cat_features, labels, lengths\n",
    "            return q_input_ids, q_input_masks, q_input_segments, a_input_ids, a_input_masks, a_input_segments, num_features, cat_features, lengths\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.inputs[0])\n",
    "\n",
    "\n",
    "    class CustomBert(BertPreTrainedModel):\n",
    "\n",
    "        def __init__(self, config, cat_dims):\n",
    "            super(CustomBert, self).__init__(config)\n",
    "            self.num_labels = config.num_labels\n",
    "            self.bert = BertModel(config)\n",
    "            self.embeddings = nn.ModuleList([\n",
    "                nn.Embedding(x, y) for x, y in cat_dims\n",
    "            ])\n",
    "            self.emb_drop = nn.Dropout(0.1)\n",
    "            n_emb_out = sum([y for x, y in cat_dims])\n",
    "            self.num_drop = nn.Dropout(0.1)\n",
    "            self.q_dropout = nn.Dropout(0.1)\n",
    "            self.a_dropout = nn.Dropout(0.1)\n",
    "            #self.dropout_all = nn.Dropout(0.2)\n",
    "            #self.dropout_a = nn.Dropout(0.2)\n",
    "            #self.dropout_q = nn.Dropout(0.2)\n",
    "            #self.classifier_all = nn.Linear(config.hidden_size*2+n_emb_out+4, 64)  # num_features=4\n",
    "            #self.classifier_all = nn.Sequential(\n",
    "            #    nn.Linear(config.hidden_size*2+n_emb_out+4, 64),\n",
    "            #    nn.ReLU(inplace=True),\n",
    "            #)\n",
    "            #self.classifier_a = nn.Linear(config.hidden_size+n_emb_out+4, 64)  # num_features=4\n",
    "            #self.classifier_a = nn.Sequential(\n",
    "            #    nn.Linear(config.hidden_size+n_emb_out+4, 64),\n",
    "            #    nn.ReLU(inplace=True),\n",
    "            #)\n",
    "            #self.classifier_q = nn.Linear(config.hidden_size+n_emb_out+4, 64)  # num_features=4\n",
    "            #self.classifier_q = nn.Sequential(\n",
    "            #    nn.Linear(config.hidden_size+n_emb_out+4, 64),\n",
    "            #    nn.ReLU(inplace=True),\n",
    "            #)\n",
    "            self.classifier_final = nn.Linear(config.hidden_size*2+n_emb_out+4, self.config.num_labels)\n",
    "            #self.classifier_final = nn.Linear(64*3, self.config.num_labels)  # num_features=4\n",
    "            #self.classifier_final = nn.Sequential(\n",
    "            #    nn.BatchNorm1d(64*3),\n",
    "            #    nn.Linear(64*3, self.config.num_labels),\n",
    "            #)\n",
    "            self.init_weights()\n",
    "\n",
    "        def forward(\n",
    "            self,\n",
    "            q_input_ids=None,\n",
    "            q_attention_mask=None,\n",
    "            q_token_type_ids=None,\n",
    "            a_input_ids=None,\n",
    "            a_attention_mask=None,\n",
    "            a_token_type_ids=None,\n",
    "            num_features=None,\n",
    "            cat_features=None,\n",
    "            position_ids=None,\n",
    "            head_mask=None,\n",
    "            inputs_embeds=None,\n",
    "            labels=None,\n",
    "        ):\n",
    "\n",
    "            q_outputs = self.bert(\n",
    "                q_input_ids,\n",
    "                attention_mask=q_attention_mask,\n",
    "                token_type_ids=q_token_type_ids,\n",
    "                position_ids=position_ids,\n",
    "                head_mask=head_mask,\n",
    "                inputs_embeds=inputs_embeds,\n",
    "            )\n",
    "\n",
    "            q_pooled_output = q_outputs[1]\n",
    "            q_pooled_output = self.q_dropout(q_pooled_output)\n",
    "\n",
    "            a_outputs = self.bert(\n",
    "                a_input_ids,\n",
    "                attention_mask=a_attention_mask,\n",
    "                token_type_ids=a_token_type_ids,\n",
    "                position_ids=position_ids,\n",
    "                head_mask=head_mask,\n",
    "                inputs_embeds=inputs_embeds,\n",
    "            )\n",
    "\n",
    "            a_pooled_output = a_outputs[1]\n",
    "            a_pooled_output = self.a_dropout(a_pooled_output)\n",
    "\n",
    "            emb = [\n",
    "                emb_layer(cat_features[:, j]) for j, emb_layer in enumerate(self.embeddings)\n",
    "            ]\n",
    "            emb = self.emb_drop(torch.cat(emb, 1))\n",
    "\n",
    "            num_features = self.num_drop(num_features)\n",
    "\n",
    "            pooled_output = torch.cat([q_pooled_output, a_pooled_output, num_features, emb], 1)\n",
    "            #all_logits = self.classifier_all(pooled_output)\n",
    "            #all_logits = self.dropout_all(all_logits)\n",
    "            logits = self.classifier_final(pooled_output)\n",
    "            \n",
    "            #a_pooled_output = torch.cat([a_pooled_output, num_features, emb], 1)\n",
    "            #a_logits = self.classifier_a(a_pooled_output)\n",
    "            #a_logits = self.dropout_a(a_logits)\n",
    "\n",
    "            #q_pooled_output = torch.cat([q_pooled_output, num_features, emb], 1)\n",
    "            #q_logits = self.classifier_q(q_pooled_output)\n",
    "            #q_logits = self.dropout_q(q_logits)\n",
    "\n",
    "            #concat_logits = torch.cat([all_logits, q_logits, a_logits], 1)\n",
    "            #logits = self.classifier_final(concat_logits)\n",
    "\n",
    "            #logits = torch.cat([q_logits, a_logits], 1)\n",
    "\n",
    "            outputs = (logits,) + q_outputs[2:] + a_outputs[2:]  # add hidden states and attention if they are here\n",
    "            if labels is not None:\n",
    "                if self.num_labels == 1:\n",
    "                    #  We are doing regression\n",
    "                    loss_fct = MSELoss()\n",
    "                    loss = loss_fct(logits.view(-1), labels.view(-1))\n",
    "                else:\n",
    "                    loss_fct = CrossEntropyLoss()\n",
    "                    loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "                outputs = (loss,) + outputs\n",
    "\n",
    "            return outputs  # (loss), logits, (hidden_states), (attentions)\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, optimizer, criterion, scheduler, config):\n",
    "    \n",
    "    model.train()\n",
    "    avg_loss = 0.\n",
    "    avg_loss_1 = 0.\n",
    "    avg_loss_2 = 0.\n",
    "    avg_loss_3 = 0.\n",
    "    avg_loss_4 = 0.\n",
    "    avg_loss_5 = 0.\n",
    "    #tk0 = tqdm(enumerate(train_loader),total =len(train_loader))\n",
    "    optimizer.zero_grad()\n",
    "    for idx, batch in enumerate(train_loader):\n",
    "        if COMBINE_INPUT:\n",
    "            input_ids, input_masks, input_segments, num_features, cat_features, labels, _ = batch\n",
    "            input_ids, input_masks, input_segments, num_features, cat_features, labels = input_ids.to(device), input_masks.to(device), input_segments.to(device), num_features.to(device), cat_features.to(device), labels.to(device)            \n",
    "        \n",
    "            output_train = model(input_ids = input_ids.long(),\n",
    "                             labels = None,\n",
    "                             attention_mask = input_masks,\n",
    "                             token_type_ids = input_segments,\n",
    "                             num_features = num_features,\n",
    "                             cat_features = cat_features,\n",
    "                            )\n",
    "        else:\n",
    "            q_input_ids, q_input_masks, q_input_segments, a_input_ids, a_input_masks, a_input_segments, num_features, cat_features, labels, _ = batch\n",
    "            q_input_ids, q_input_masks, q_input_segments, a_input_ids, a_input_masks, a_input_segments, num_features, cat_features, labels = q_input_ids.to(device), q_input_masks.to(device), q_input_segments.to(device), a_input_ids.to(device), a_input_masks.to(device), a_input_segments.to(device), num_features.to(device), cat_features.to(device), labels.to(device)\n",
    "\n",
    "            output_train = model(q_input_ids = q_input_ids.long(),\n",
    "                             labels = None,\n",
    "                             q_attention_mask = q_input_masks,\n",
    "                             q_token_type_ids = q_input_segments,\n",
    "                             a_input_ids = a_input_ids.long(),\n",
    "                             a_attention_mask = a_input_masks,\n",
    "                             a_token_type_ids = a_input_segments,\n",
    "                             num_features = num_features,\n",
    "                             cat_features = cat_features,\n",
    "                            )\n",
    "        logits = output_train[0] #output preds\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        if (idx + 1) % config.accum_steps == 0:    \n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        avg_loss += loss.item() / (len(train_loader)*config.accum_steps)\n",
    "        if COMBINE_INPUT:\n",
    "            del input_ids, input_masks, input_segments, labels\n",
    "        else:\n",
    "            del q_input_ids, q_input_masks, q_input_segments, a_input_ids, a_input_masks, a_input_segments, labels\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    return avg_loss, avg_loss_1, avg_loss_2, avg_loss_3, avg_loss_4, avg_loss_5\n",
    "\n",
    "\n",
    "def val_model(model, criterion, val_loader, val_shape, batch_size=8):\n",
    "\n",
    "    avg_val_loss = 0.\n",
    "    model.eval() # eval mode\n",
    "    \n",
    "    valid_preds = np.zeros((val_shape, len(target_cols)))\n",
    "    original = np.zeros((val_shape, len(target_cols)))\n",
    "    \n",
    "    #tk0 = tqdm(enumerate(val_loader))\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for idx, batch in enumerate(val_loader):\n",
    "            if COMBINE_INPUT:\n",
    "                input_ids, input_masks, input_segments, num_features, cat_features, labels, _ = batch\n",
    "                input_ids, input_masks, input_segments, num_features, cat_features, labels = input_ids.to(device), input_masks.to(device), input_segments.to(device), num_features.to(device), cat_features.to(device), labels.to(device)            \n",
    "            \n",
    "                output_val = model(input_ids = input_ids.long(),\n",
    "                               labels = None,\n",
    "                               attention_mask = input_masks,\n",
    "                               token_type_ids = input_segments,\n",
    "                               num_features = num_features,\n",
    "                               cat_features = cat_features,\n",
    "                              )\n",
    "            else:\n",
    "                q_input_ids, q_input_masks, q_input_segments, a_input_ids, a_input_masks, a_input_segments, num_features, cat_features, labels, _ = batch\n",
    "                q_input_ids, q_input_masks, q_input_segments, a_input_ids, a_input_masks, a_input_segments, num_features, cat_features, labels = q_input_ids.to(device), q_input_masks.to(device), q_input_segments.to(device), a_input_ids.to(device), a_input_masks.to(device), a_input_segments.to(device), num_features.to(device), cat_features.to(device), labels.to(device)\n",
    "\n",
    "                output_val = model(q_input_ids = q_input_ids.long(),\n",
    "                             labels = None,\n",
    "                             q_attention_mask = q_input_masks,\n",
    "                             q_token_type_ids = q_input_segments,\n",
    "                             a_input_ids = a_input_ids.long(),\n",
    "                             a_attention_mask = a_input_masks,\n",
    "                             a_token_type_ids = a_input_segments,\n",
    "                             num_features = num_features,\n",
    "                             cat_features = cat_features,\n",
    "                            )\n",
    "            logits = output_val[0] #output preds\n",
    "            \n",
    "            avg_val_loss += criterion(logits, labels).item() / len(val_loader)\n",
    "            valid_preds[idx*batch_size : (idx+1)*batch_size] = logits.detach().cpu().squeeze().numpy()\n",
    "            original[idx*batch_size : (idx+1)*batch_size]    = labels.detach().cpu().squeeze().numpy()\n",
    "        \n",
    "        score = 0\n",
    "        preds = torch.sigmoid(torch.tensor(valid_preds)).numpy()\n",
    "        \n",
    "        # np.save(\"preds.npy\", preds)\n",
    "        # np.save(\"actuals.npy\", original)\n",
    "        \n",
    "        rho_val = np.mean([spearmanr(original[:, i], preds[:,i]).correlation for i in range(preds.shape[1])])\n",
    "        print('\\r val_spearman-rho: %s' % (str(round(rho_val, 5))), end = 100*' '+'\\n')\n",
    "        \n",
    "        for i in range(len(target_cols)):\n",
    "            logger.info(f\"{i}, {spearmanr(original[:,i], preds[:,i])}\")\n",
    "            score += np.nan_to_num(spearmanr(original[:, i], preds[:, i]).correlation)\n",
    "        \n",
    "    return avg_val_loss, score/len(target_cols)\n",
    "\n",
    "\n",
    "def predict_valid_result(model, val_loader, val_length, batch_size=32):\n",
    "\n",
    "    val_preds = np.zeros((val_length, len(target_cols)))\n",
    "    original = np.zeros((val_length, len(target_cols)))\n",
    "\n",
    "    model.eval()\n",
    "    tk0 = tqdm(enumerate(val_loader))\n",
    "    for idx, batch in tk0:\n",
    "        if COMBINE_INPUT:\n",
    "            input_ids, input_masks, input_segments, num_features, cat_features, labels, _ = batch\n",
    "            input_ids, input_masks, input_segments, num_features, cat_features, labels = input_ids.to(device), input_masks.to(device), input_segments.to(device), num_features.to(device), cat_features.to(device), labels.to(device)            \n",
    "            with torch.no_grad():\n",
    "                outputs = model(input_ids = input_ids.long(),\n",
    "                            labels = None,\n",
    "                            attention_mask = input_masks,\n",
    "                            token_type_ids = input_segments,\n",
    "                            num_features = num_features,\n",
    "                            cat_features = cat_features,\n",
    "                            )\n",
    "        else:\n",
    "            q_input_ids, q_input_masks, q_input_segments, a_input_ids, a_input_masks, a_input_segments, num_features, cat_features, labels, _ = batch\n",
    "            q_input_ids, q_input_masks, q_input_segments, a_input_ids, a_input_masks, a_input_segments, num_features, cat_features, labels = q_input_ids.to(device), q_input_masks.to(device), q_input_segments.to(device), a_input_ids.to(device), a_input_masks.to(device), a_input_segments.to(device), num_features.to(device), cat_features.to(device), labels.to(device)\n",
    "            with torch.no_grad():\n",
    "                outputs = model(q_input_ids = q_input_ids.long(),\n",
    "                             labels = None,\n",
    "                             q_attention_mask = q_input_masks,\n",
    "                             q_token_type_ids = q_input_segments,\n",
    "                             a_input_ids = a_input_ids.long(),\n",
    "                             a_attention_mask = a_input_masks,\n",
    "                             a_token_type_ids = a_input_segments,\n",
    "                             num_features = num_features,\n",
    "                             cat_features = cat_features,\n",
    "                            )\n",
    "\n",
    "        predictions = outputs[0]\n",
    "        val_preds[idx*batch_size : (idx+1)*batch_size] = predictions.detach().cpu().squeeze().numpy()\n",
    "        original[idx*batch_size : (idx+1)*batch_size] = labels.detach().cpu().squeeze().numpy()\n",
    "\n",
    "    output = torch.sigmoid(torch.tensor(val_preds)).numpy()\n",
    "    return output, original\n",
    "\n",
    "\n",
    "def predict_result(model, test_loader, test_length, batch_size=32):\n",
    "\n",
    "    test_preds = np.zeros((test_length, len(target_cols)))\n",
    "\n",
    "    model.eval()\n",
    "    tk0 = tqdm(enumerate(test_loader))\n",
    "    for idx, x_batch in tk0:\n",
    "        if COMBINE_INPUT:\n",
    "            with torch.no_grad():\n",
    "                outputs = model(input_ids = x_batch[0].to(device),\n",
    "                            labels = None,\n",
    "                            attention_mask = x_batch[1].to(device),\n",
    "                            token_type_ids = x_batch[2].to(device),\n",
    "                            num_features = x_batch[3].to(device),\n",
    "                            cat_features = x_batch[4].to(device),\n",
    "                           )\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                outputs = model(q_input_ids = x_batch[0].to(device),\n",
    "                            labels = None,\n",
    "                            q_attention_mask = x_batch[1].to(device),\n",
    "                            q_token_type_ids = x_batch[2].to(device),\n",
    "                            a_input_ids = x_batch[3].to(device),\n",
    "                            a_attention_mask = x_batch[4].to(device),\n",
    "                            a_token_type_ids = x_batch[5].to(device),\n",
    "                            num_features = x_batch[6].to(device),\n",
    "                            cat_features = x_batch[7].to(device),\n",
    "                           )\n",
    "        predictions = outputs[0]\n",
    "        test_preds[idx*batch_size : (idx+1)*batch_size] = predictions.detach().cpu().squeeze().numpy()\n",
    "\n",
    "    output = torch.sigmoid(torch.tensor(test_preds)).numpy()\n",
    "    return output\n",
    "\n",
    "\n",
    "def add_features(df):\n",
    "    find = re.compile(r\"^[^.]*\")\n",
    "    df['netloc'] = df['url'].apply(lambda x: re.findall(find, urlparse(x).netloc)[0])\n",
    "    df['qa_same_user_page_flag'] = (df['question_user_page']==df['answer_user_page'])*1\n",
    "    df['question_title_num_words'] = df['question_title'].str.count('\\S+')\n",
    "    df['question_body_num_words'] = df['question_body'].str.count('\\S+')\n",
    "    df['answer_num_words'] = df['answer'].str.count('\\S+')\n",
    "    df['question_vs_answer_length'] = df['question_body_num_words']/df['answer_num_words']\n",
    "    df['question_title_num_words'] = np.log1p(df['question_title_num_words'])\n",
    "    df['question_body_num_words'] = np.log1p(df['question_body_num_words'])\n",
    "    df['answer_num_words'] = np.log1p(df['answer_num_words'])\n",
    "    df['question_vs_answer_length'] = np.log1p(df['question_vs_answer_length'])\n",
    "    return df\n",
    "\n",
    "\n",
    "def custom_loss(logits, labels):\n",
    "    #q_loss = nn.BCEWithLogitsLoss()(logits[:,:21], labels[:,:21])\n",
    "    #a_loss = nn.BCEWithLogitsLoss()(logits[:,21:], labels[:,21:])\n",
    "    #custom_loss = 0.5*q_loss + 0.5*a_loss\n",
    "    custom_loss = nn.BCEWithLogitsLoss()(logits, labels)\n",
    "    #loss1 = nn.BCEWithLogitsLoss()(logits[:,0:19], labels[:,0:19])\n",
    "    #loss2 = nn.BCEWithLogitsLoss()(logits[:,20:], labels[:,20:]) # except index=19\n",
    "    #custom_loss = loss1 + loss2\n",
    "    #custom_loss = 0.\n",
    "    #for i in range(len(loss_sample_weights)):\n",
    "    #    custom_loss += loss_sample_weights[i] * nn.BCEWithLogitsLoss()(logits[:,i], labels[:,i])\n",
    "    return custom_loss\n",
    "\n",
    "\n",
    "#===========================================================\n",
    "# main\n",
    "#===========================================================\n",
    "#def main():\n",
    "if True:\n",
    "    \n",
    "    with timer('Data Loading'):\n",
    "        train = pd.read_csv(f\"{ROOT}train.csv\").fillna(\"none\")\n",
    "        for c in ['question_not_really_a_question', 'question_type_consequence', 'question_type_spelling']:\n",
    "            train[c] = np.log1p(train[c].values)\n",
    "        y_train = train[target_cols].values\n",
    "        if config.test:\n",
    "            test = pd.read_csv(f\"{ROOT}test.csv\").fillna(\"none\")\n",
    "            submission = pd.read_csv(f\"{ROOT}sample_submission.csv\")\n",
    "    \n",
    "    with timer('Num features'):\n",
    "        train = add_features(train)\n",
    "        if config.test:\n",
    "            test = add_features(test)\n",
    "        num_features = ['question_title_num_words', 'question_body_num_words', 'answer_num_words', 'question_vs_answer_length']\n",
    "        train_num = train[num_features].values\n",
    "        if config.test:\n",
    "            test_num = test[num_features].values\n",
    "                \n",
    "    with timer('Cat features'):\n",
    "        cat_features = ['netloc', 'category', 'qa_same_user_page_flag']\n",
    "        ce_oe = ce.OrdinalEncoder(cols=cat_features, handle_unknown='return_nan')\n",
    "        ce_oe.fit(train[cat_features])\n",
    "        train_cat_df = ce_oe.transform(train[cat_features])\n",
    "        test_cat_df = ce_oe.transform(test[cat_features]).fillna(0).astype(int)\n",
    "        train_cat = train_cat_df.values\n",
    "        test_cat = test_cat_df.values\n",
    "        cat_dims = []\n",
    "        for col in cat_features:\n",
    "            dim = train[col].nunique()\n",
    "            cat_dims.append((dim+1, dim//2+1)) # for unknown=0\n",
    "        print(cat_dims)\n",
    "\n",
    "    if config.train:\n",
    "        with timer('Create folds'):\n",
    "            folds = train.copy()\n",
    "\n",
    "            kf = MultilabelStratifiedKFold(n_splits=NUM_FOLDS, random_state=SEED)\n",
    "            for fold, (train_index, val_index) in enumerate(kf.split(train.values, y_train)):\n",
    "                folds.loc[val_index, 'fold'] = int(fold)\n",
    "            \"\"\"\n",
    "            # less gap between CV vs LB with GroupKFold\n",
    "            # https://www.kaggle.com/ratthachat/quest-cv-analysis-on-different-splitting-methods\n",
    "            kf = GroupKFold(n_splits=NUM_FOLDS)\n",
    "            for fold, (train_index, val_index) in enumerate(kf.split(X=train.question_body, groups=train.question_body)):\n",
    "                folds.loc[val_index, 'fold'] = int(fold)\n",
    "            \"\"\"\n",
    "            folds['fold'] = folds['fold'].astype(int)\n",
    "            save_cols = [ID] + target_cols + ['fold']\n",
    "            folds[save_cols].to_csv('folds.csv', index=None)\n",
    "\n",
    "    with timer('Prepare Bert config'):\n",
    "        tokenizer = BertTokenizer.from_pretrained(\"../input/pretrained-bert-models-for-pytorch/bert-base-uncased-vocab.txt\", \n",
    "                                                  do_lower_case=True)\n",
    "        input_categories = ['question_title', 'question_body', 'answer']\n",
    "        bert_model_config = '../input/pretrained-bert-models-for-pytorch/bert-base-uncased/bert_config.json'\n",
    "        bert_config = BertConfig.from_json_file(bert_model_config)\n",
    "        bert_config.num_labels = len(target_cols)\n",
    "        bert_model = 'bert-base-uncased'\n",
    "        do_lower_case = 'uncased' in bert_model\n",
    "        output_model_file = 'bert_pytorch.bin'\n",
    "    \n",
    "    if config.train:\n",
    "\n",
    "        BATCH_SIZE = 8\n",
    "        if DEBUG:\n",
    "            epochs = 1\n",
    "        else:\n",
    "            epochs = config.epochs\n",
    "        ACCUM_STEPS = config.accum_steps\n",
    "\n",
    "        with timer('Train Bert'):\n",
    "            \n",
    "            for fold in range(NUM_FOLDS):\n",
    "\n",
    "                logger.info(f\"Current Fold: {fold}\")\n",
    "                train_index = folds[folds.fold != fold].index\n",
    "                val_index = folds[folds.fold == fold].index\n",
    "\n",
    "                train_df, val_df = train.iloc[train_index], train.iloc[val_index]\n",
    "                logger.info(f\"Train Shapes: {train_df.shape}\")\n",
    "                logger.info(f\"Valid Shapes: {val_df.shape}\")\n",
    "            \n",
    "                logger.info(\"Preparing train datasets....\")\n",
    "            \n",
    "                inputs_train = compute_input_arays(train_df, input_categories, tokenizer, max_sequence_length=MAX_SEQUENCE_LENGTH, \n",
    "                                                   num_features=train_num[train_index], cat_features=train_cat[train_index])\n",
    "                outputs_train = compute_output_arrays(train_df, columns=target_cols)\n",
    "                outputs_train = torch.tensor(outputs_train, dtype=torch.float32)\n",
    "                lengths_train = np.argmax(inputs_train[0]==0, axis=1)\n",
    "                lengths_train[lengths_train==0] = inputs_train[0].shape[1]\n",
    "            \n",
    "                logger.info(\"Preparing valid datasets....\")\n",
    "            \n",
    "                inputs_valid = compute_input_arays(val_df, input_categories, tokenizer, max_sequence_length=MAX_SEQUENCE_LENGTH, \n",
    "                                                   num_features=train_num[val_index], cat_features=train_cat[val_index])\n",
    "                outputs_valid = compute_output_arrays(val_df, columns = target_cols)\n",
    "                outputs_valid = torch.tensor(outputs_valid, dtype=torch.float32)\n",
    "                lengths_valid = np.argmax(inputs_valid[0] == 0, axis=1)\n",
    "                lengths_valid[lengths_valid == 0] = inputs_valid[0].shape[1]\n",
    "            \n",
    "                logger.info(\"Preparing Dataloaders Datasets....\")\n",
    "\n",
    "                train_set = QuestDataset(inputs=inputs_train, lengths=lengths_train, labels=outputs_train)\n",
    "                train_sampler = RandomSampler(train_set)\n",
    "                train_loader = DataLoader(train_set, batch_size=BATCH_SIZE,sampler=train_sampler)\n",
    "            \n",
    "                valid_set = QuestDataset(inputs=inputs_valid, lengths=lengths_valid, labels=outputs_valid)\n",
    "                valid_loader = DataLoader(valid_set, batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n",
    "            \n",
    "                model = CustomBert.from_pretrained('../input/pretrained-bert-models-for-pytorch/bert-base-uncased/', config=bert_config, cat_dims=cat_dims)\n",
    "                model.zero_grad()\n",
    "                model.to(device)\n",
    "                torch.cuda.empty_cache()\n",
    "                if config.freeze : ## This is basically using out of the box bert model while training only the classifier head with our data . \n",
    "                    for param in model.bert.parameters():\n",
    "                        param.requires_grad = False\n",
    "                model.train()\n",
    "            \n",
    "                i = 0\n",
    "                best_avg_loss = 100.0\n",
    "                best_score = -1.\n",
    "                best_param_loss = None\n",
    "                best_param_score = None\n",
    "                param_optimizer = list(model.named_parameters())\n",
    "                no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "                optimizer_grouped_parameters = [\n",
    "                    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "                    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "                    ]        \n",
    "                optimizer = AdamW(optimizer_grouped_parameters, lr=config.lr, eps=4e-5)\n",
    "                #optimizer = torch.optim.AdamW(model.parameters(), lr=config.lr, eps=4e-5)\n",
    "                #criterion = nn.BCEWithLogitsLoss()\n",
    "                criterion = custom_loss\n",
    "                scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=config.warmup, num_training_steps=epochs*len(train_loader)//ACCUM_STEPS)\n",
    "                logger.info(\"Training....\")\n",
    "            \n",
    "                for epoch in tqdm(range(epochs)):\n",
    "\n",
    "                    torch.cuda.empty_cache()\n",
    "                \n",
    "                    start_time   = time.time()\n",
    "                    avg_loss, avg_loss_1, avg_loss_2, avg_loss_3, avg_loss_4, avg_loss_5 = train_model(model, train_loader, optimizer, criterion, scheduler, config)\n",
    "                    avg_val_loss, score = val_model(model, criterion, valid_loader, val_shape=val_df.shape[0], batch_size=BATCH_SIZE)\n",
    "                    elapsed_time = time.time() - start_time\n",
    "\n",
    "                    logger.info('Epoch {}/{} \\t loss={:.4f} \\t val_loss={:.4f} \\t train_loss={:.4f} \\t train_loss_1={:.4f} \\t train_loss_2={:.4f} \\t train_loss_3={:.4f} \\t train_loss_4={:.4f}  \\t train_loss_5={:.4f} \\t score={:.6f} \\t time={:.2f}s'.format(\n",
    "                        epoch+1, epochs, avg_loss, avg_val_loss, avg_loss, avg_loss_1, avg_loss_2, avg_loss_3, avg_loss_4, avg_loss_5, score, elapsed_time))\n",
    "\n",
    "                    if best_avg_loss > avg_val_loss:\n",
    "                        i = 0\n",
    "                        best_avg_loss = avg_val_loss \n",
    "                        best_param_loss = model.state_dict()\n",
    "\n",
    "                    if best_score < score:\n",
    "                        best_score = score\n",
    "                        best_param_score = model.state_dict()\n",
    "                        logger.info('best_param_score_{}_{}.pt'.format(config.expname ,fold))\n",
    "                        torch.save(best_param_score, 'best_param_score_{}_{}.pt'.format(config.expname, fold))\n",
    "                    else:\n",
    "                        i += 1\n",
    "\n",
    "            del train_df, val_df, model, optimizer, criterion, scheduler\n",
    "            del valid_loader, train_loader, valid_set, train_set\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "    \n",
    "    if config.cv:\n",
    "\n",
    "        with timer('CV'):\n",
    "\n",
    "            folds = pd.read_csv(f'{MODEL_DIR}folds.csv')\n",
    "            results = np.zeros((len(train), len(target_cols)))\n",
    "            logits = np.zeros((len(train), len(target_cols)))\n",
    "\n",
    "            for fold in range(NUM_FOLDS):\n",
    "                \n",
    "                #train_index = folds[folds.fold != fold].index\n",
    "                val_index = folds[folds.fold == fold].index\n",
    "                #train_df, val_df = train.iloc[train_index], train.iloc[val_index]\n",
    "                val_df = train.iloc[val_index]\n",
    "                \n",
    "                inputs_valid = compute_input_arays(val_df, input_categories, tokenizer, max_sequence_length=MAX_SEQUENCE_LENGTH, \n",
    "                                                   num_features=train_num[val_index], cat_features=train_cat[val_index])\n",
    "                outputs_valid = compute_output_arrays(val_df, columns = target_cols)\n",
    "                outputs_valid = torch.tensor(outputs_valid, dtype=torch.float32)\n",
    "                lengths_valid = np.argmax(inputs_valid[0] == 0, axis=1)\n",
    "                lengths_valid[lengths_valid == 0] = inputs_valid[0].shape[1]\n",
    "                valid_set = QuestDataset(inputs=inputs_valid, lengths=lengths_valid, labels=outputs_valid)\n",
    "                valid_loader = DataLoader(valid_set, batch_size=32, shuffle=False, drop_last=False)\n",
    "                \n",
    "                model = CustomBert.from_pretrained('../input/pretrained-bert-models-for-pytorch/bert-base-uncased/', config=bert_config, cat_dims=cat_dims)\n",
    "                model.zero_grad()\n",
    "                model.to(device)\n",
    "                model.load_state_dict(torch.load(f'{MODEL_DIR}best_param_score_{config.expname}_{fold}.pt'))\n",
    "                result, logit = predict_valid_result(model, valid_loader, len(val_df))  \n",
    "                results[val_index, :] = result\n",
    "                logits[val_index, :] = logit \n",
    "            \n",
    "            rho_val = np.mean([spearmanr(logits[:,i], results[:,i]).correlation for i in range(results.shape[1])])\n",
    "            logger.info(f'CV spearman-rho: {round(rho_val, 5)}')\n",
    "\n",
    "            oof = pd.DataFrame()\n",
    "            for i, col in enumerate(target_cols):\n",
    "                oof[col] = results[:,i]\n",
    "            oof.to_csv(f'oof_{config.expname}.csv', index=False)\n",
    "    \n",
    "    if config.test:\n",
    "\n",
    "        with timer('Inference'):\n",
    "\n",
    "            test_inputs = compute_input_arays(test, input_categories, tokenizer, max_sequence_length=MAX_SEQUENCE_LENGTH, \n",
    "                                              num_features=test_num, cat_features=test_cat)\n",
    "            lengths_test = np.argmax(test_inputs[0] == 0, axis=1)\n",
    "            lengths_test[lengths_test == 0] = test_inputs[0].shape[1]\n",
    "            test_set = QuestDataset(inputs=test_inputs, lengths=lengths_test, labels=None)\n",
    "            test_loader  = DataLoader(test_set, batch_size=32, shuffle=False)\n",
    "            result = np.zeros((len(test), len(target_cols)))\n",
    "\n",
    "            for fold in range(NUM_FOLDS):\n",
    "                model = CustomBert.from_pretrained('../input/pretrained-bert-models-for-pytorch/bert-base-uncased/', config=bert_config, cat_dims=cat_dims)\n",
    "                model.zero_grad()\n",
    "                model.to(device)\n",
    "                model.load_state_dict(torch.load(f'{MODEL_DIR}best_param_score_{config.expname}_{fold}.pt'))\n",
    "                result += predict_result(model, test_loader, len(test)) \n",
    "                if DEBUG:\n",
    "                    break\n",
    "                    \n",
    "            result /= NUM_FOLDS\n",
    "\n",
    "        with timer('Create submission.csv'):\n",
    "            submission.loc[:, 'question_asker_intent_understanding':] = result\n",
    "            submission.to_csv('submission5.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Inference] start\n",
      "15it [00:16,  1.09s/it]\n",
      "15it [00:16,  1.09s/it]\n",
      "15it [00:16,  1.10s/it]\n",
      "15it [00:16,  1.09s/it]\n",
      "15it [00:16,  1.09s/it]\n",
      "[Inference] done in 99 s\n",
      "[Create submission.csv] start\n",
      "[Create submission.csv] done in 0 s\n"
     ]
    }
   ],
   "source": [
    "#===========================================================\n",
    "# Config\n",
    "#===========================================================\n",
    "class PipeLineConfig:\n",
    "    def __init__(self, lr, warmup, accum_steps, epochs, seed, expname, \n",
    "                 head_tail, head, freeze, question_weight, answer_weight, fold, train, cv, test):\n",
    "        self.lr = lr\n",
    "        self.warmup = warmup\n",
    "        self.accum_steps = accum_steps\n",
    "        self.epochs = epochs\n",
    "        self.seed = seed\n",
    "        self.expname = expname\n",
    "        self.head_tail = head_tail\n",
    "        self.head = head\n",
    "        self.freeze = freeze\n",
    "        self.question_weight = question_weight\n",
    "        self.answer_weight = answer_weight\n",
    "        self.fold = fold\n",
    "        self.train = train\n",
    "        self.cv = cv\n",
    "        self.test = test\n",
    "\n",
    "config = PipeLineConfig(lr=1e-4, warmup=0.1, accum_steps=1, epochs=6,\n",
    "                        seed=42, expname='uncased_8', head_tail=True, head=0.5, freeze=False,\n",
    "                        question_weight=0., answer_weight=0., fold=5, train=False, cv=False, test=True)\n",
    "\n",
    "DEBUG = False\n",
    "ID = 'qa_id'\n",
    "target_cols = ['question_asker_intent_understanding', 'question_body_critical', \n",
    "               'question_conversational', 'question_expect_short_answer', \n",
    "               'question_fact_seeking', 'question_has_commonly_accepted_answer', \n",
    "               'question_interestingness_others', 'question_interestingness_self', \n",
    "               'question_multi_intent', 'question_not_really_a_question', \n",
    "               'question_opinion_seeking', 'question_type_choice',\n",
    "               'question_type_compare', 'question_type_consequence',\n",
    "               'question_type_definition', 'question_type_entity', \n",
    "               'question_type_instructions', 'question_type_procedure', \n",
    "               'question_type_reason_explanation', 'question_type_spelling', \n",
    "               'question_well_written', 'answer_helpful',\n",
    "               'answer_level_of_information', 'answer_plausible', \n",
    "               'answer_relevance', 'answer_satisfaction', \n",
    "               'answer_type_instructions', 'answer_type_procedure', \n",
    "               'answer_type_reason_explanation', 'answer_well_written']\n",
    "NUM_FOLDS = config.fold\n",
    "ROOT = '../input/google-quest-challenge/'\n",
    "#ROOT = '../input/'\n",
    "SEED = config.seed\n",
    "seed_everything(SEED)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "MODEL_DIR = '../input/googlequestchallenge-weights7/'\n",
    "#MODEL_DIR = './'\n",
    "COMBINE_INPUT = False\n",
    "T_MAX_LEN = 30\n",
    "Q_MAX_LEN = 479 # 382\n",
    "A_MAX_LEN = 479 # 254 \n",
    "MAX_SEQUENCE_LENGTH = T_MAX_LEN + Q_MAX_LEN + A_MAX_LEN + 4\n",
    "q_max_sequence_length = T_MAX_LEN + Q_MAX_LEN + 3\n",
    "a_max_sequence_length = T_MAX_LEN + A_MAX_LEN + 3\n",
    "\n",
    "#===========================================================\n",
    "# Model\n",
    "#===========================================================\n",
    "def _get_masks(tokens, max_seq_length):\n",
    "    \"\"\"Mask for padding\"\"\"\n",
    "    if len(tokens)>max_seq_length:\n",
    "        print(f'len(tokens): {len(tokens)}')\n",
    "        print(f'max_seq_length: {max_seq_length}')\n",
    "        raise IndexError(\"Token length more than max seq length!\")\n",
    "    return [1]*len(tokens) + [0] * (max_seq_length - len(tokens))\n",
    "\n",
    "\n",
    "def _get_segments(tokens, max_seq_length):\n",
    "    \"\"\"Segments: 0 for the first sequence, 1 for the second\"\"\"\n",
    "    \n",
    "    if len(tokens) > max_seq_length:\n",
    "        raise IndexError(\"Token length more than max seq length!\")\n",
    "        \n",
    "    segments = []\n",
    "    first_sep = True\n",
    "    current_segment_id = 0\n",
    "    \n",
    "    for token in tokens:\n",
    "        segments.append(current_segment_id)\n",
    "        if token == \"[SEP]\":\n",
    "            if first_sep:\n",
    "                first_sep = False \n",
    "            else:\n",
    "                current_segment_id = 1\n",
    "    return segments + [0] * (max_seq_length - len(tokens))\n",
    "\n",
    "\n",
    "def _get_ids(tokens, tokenizer, max_seq_length):\n",
    "    \"\"\"Token ids from Tokenizer vocab\"\"\"\n",
    "    \n",
    "    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    input_ids = token_ids + [0] * (max_seq_length-len(token_ids))\n",
    "    return input_ids\n",
    "\n",
    "\n",
    "def _trim_input(tokenizer, title, question, answer, max_sequence_length, t_max_len, q_max_len, a_max_len):\n",
    "    \n",
    "    # 350+128+30 = 508 +4 = 512\n",
    "    \n",
    "    t = tokenizer.tokenize(title)\n",
    "    q = tokenizer.tokenize(question)\n",
    "    a = tokenizer.tokenize(answer)\n",
    "    \n",
    "    t_len = len(t)\n",
    "    q_len = len(q)\n",
    "    a_len = len(a)\n",
    "\n",
    "    if (t_len+q_len+a_len+4) > max_sequence_length:\n",
    "        \n",
    "        if t_max_len > t_len:\n",
    "            t_new_len = t_len\n",
    "            a_max_len = a_max_len + floor((t_max_len - t_len)/2)\n",
    "            q_max_len = q_max_len + ceil((t_max_len - t_len)/2)\n",
    "        else:\n",
    "            t_new_len = t_max_len\n",
    "      \n",
    "        if a_max_len > a_len:\n",
    "            a_new_len = a_len \n",
    "            q_new_len = q_max_len + (a_max_len - a_len)\n",
    "        elif q_max_len > q_len:\n",
    "            a_new_len = a_max_len + (q_max_len - q_len)\n",
    "            q_new_len = q_len\n",
    "        else:\n",
    "            a_new_len = a_max_len\n",
    "            q_new_len = q_max_len\n",
    "            \n",
    "            \n",
    "        if t_new_len+a_new_len+q_new_len+4 != max_sequence_length:\n",
    "            raise ValueError(\"New sequence length should be %d, but is %d\"%(max_sequence_length, (t_new_len + a_new_len + q_new_len + 4)))\n",
    "        # Head+Tail method \n",
    "        q_len_head = round(q_new_len * config.head)\n",
    "        q_len_tail = -1 * (q_new_len - q_len_head)\n",
    "        a_len_head = round(a_new_len * config.head)\n",
    "        a_len_tail = -1 * (a_new_len - a_len_head)\n",
    "        t_len_head = round(t_new_len * config.head)\n",
    "        t_len_tail = -1 * (t_new_len - t_len_head)  \n",
    "        #t = t[:t_new_len]\n",
    "        if config.head_tail :\n",
    "            q = q[:q_len_head]+q[q_len_tail:]\n",
    "            a = a[:a_len_head]+a[a_len_tail:]\n",
    "            #t = t[:t_len_head]+t[t_len_tail:]\n",
    "            t = t[:t_new_len]\n",
    "        else:\n",
    "            # No Head+Tail , usual processing\n",
    "            q = q[:q_new_len]\n",
    "            a = a[:a_new_len]\n",
    "            t = t[:t_new_len]\n",
    "    \n",
    "    return t, q, a\n",
    "\n",
    "\n",
    "def q_trim_input(tokenizer, title, question, q_max_sequence_length, t_max_len, q_max_len):\n",
    "\n",
    "    t = tokenizer.tokenize(title)\n",
    "    q = tokenizer.tokenize(question)\n",
    "\n",
    "    t_len = len(t)\n",
    "    q_len = len(q)\n",
    "\n",
    "    if (t_len+q_len+3) > q_max_sequence_length:\n",
    "\n",
    "        if t_max_len > t_len:\n",
    "            t_new_len = t_len\n",
    "            q_max_len = q_max_len + (t_max_len - t_len)\n",
    "        else:\n",
    "            t_new_len = t_max_len\n",
    "\n",
    "        if q_max_len > q_len:\n",
    "            q_new_len = q_len\n",
    "            t_new_len = t_max_len + (q_max_len - q_len)\n",
    "        else:\n",
    "            q_new_len = q_max_len\n",
    "\n",
    "        # Head+Tail method\n",
    "        q_len_head = round(q_new_len * config.head)\n",
    "        q_len_tail = -1 * (q_new_len - q_len_head)\n",
    "        t_len_head = round(t_new_len * config.head)\n",
    "        t_len_tail = -1 * (t_new_len - t_len_head)\n",
    "        #t = t[:t_new_len]\n",
    "        if config.head_tail :\n",
    "            q = q[:q_len_head]+q[q_len_tail:]\n",
    "            t = t[:t_len_head]+t[t_len_tail:]\n",
    "            #t = t[:t_new_len]\n",
    "        else:\n",
    "            # No Head+Tail , usual processing\n",
    "            q = q[:q_new_len]\n",
    "            t = t[:t_new_len]\n",
    "\n",
    "    return t, q\n",
    "\n",
    "\"\"\"\n",
    "def a_trim_input(tokenizer, answer, a_max_sequence_length, a_max_len):\n",
    "\n",
    "    a = tokenizer.tokenize(answer)\n",
    "\n",
    "    a_len = len(a)\n",
    "\n",
    "    if (a_len+2) > a_max_sequence_length:\n",
    "\n",
    "        a_new_len = a_max_len\n",
    "\n",
    "        # Head+Tail method\n",
    "        a_len_head = round(a_new_len * config.head)\n",
    "        a_len_tail = -1 * (a_new_len - a_len_head)\n",
    "        if config.head_tail :\n",
    "            a = a[:a_len_head]+a[a_len_tail:]\n",
    "        else:\n",
    "            # No Head+Tail , usual processing\n",
    "            a = a[:a_new_len]\n",
    "\n",
    "    return a\n",
    "\"\"\"\n",
    "\n",
    "def a_trim_input(tokenizer, title, answer, a_max_sequence_length, t_max_len, a_max_len):\n",
    "\n",
    "    t = tokenizer.tokenize(title)\n",
    "    a = tokenizer.tokenize(answer)\n",
    "\n",
    "    t_len = len(t)\n",
    "    a_len = len(a)\n",
    "\n",
    "    if (t_len+a_len+3) > a_max_sequence_length:\n",
    "\n",
    "        if t_max_len > t_len:\n",
    "            t_new_len = t_len\n",
    "            a_max_len = a_max_len + (t_max_len - t_len)\n",
    "        else:\n",
    "            t_new_len = t_max_len\n",
    "\n",
    "        if a_max_len > a_len:\n",
    "            a_new_len = a_len\n",
    "            t_new_len = t_max_len + (a_max_len - a_len)\n",
    "        else:\n",
    "            a_new_len = a_max_len\n",
    "\n",
    "        # Head+Tail method\n",
    "        a_len_head = round(a_new_len * config.head)\n",
    "        a_len_tail = -1 * (a_new_len - a_len_head)\n",
    "        t_len_head = round(t_new_len * config.head)\n",
    "        t_len_tail = -1 * (t_new_len - t_len_head)\n",
    "        #t = t[:t_new_len]\n",
    "        if config.head_tail :\n",
    "            a = a[:a_len_head]+a[a_len_tail:]\n",
    "            t = t[:t_len_head]+t[t_len_tail:]\n",
    "            #t = t[:t_new_len]\n",
    "        else:\n",
    "            # No Head+Tail , usual processing\n",
    "            a = a[:a_new_len]\n",
    "            t = t[:t_new_len]\n",
    "\n",
    "    return t, a\n",
    "\n",
    "\n",
    "def _convert_to_bert_inputs(title_q, title_a, question, answer, tokenizer, max_sequence_length):\n",
    "    \"\"\"Converts tokenized input to ids, masks and segments for BERT\"\"\"\n",
    "    if COMBINE_INPUT:\n",
    "        stoken = [\"[CLS]\"] + title + [\"[QBODY]\"] + question + [\"[ANS]\"] + answer + [\"[SEP]\"]\n",
    "        #stoken = [\"[CLS]\"] + title + [\"[SEP]\"] + question + [\"[SEP]\"] + answer + [\"[SEP]\"]\n",
    "        #stoken = [\"[CLS]\"] + title  + question  + answer + [\"[SEP]\"]\n",
    "    \n",
    "        input_ids = _get_ids(stoken, tokenizer, max_sequence_length)\n",
    "        input_masks = _get_masks(stoken, max_sequence_length)\n",
    "        input_segments = _get_segments(stoken, max_sequence_length)\n",
    "\n",
    "        return [input_ids, input_masks, input_segments]\n",
    "    else:\n",
    "        q_token = [\"[CLS]\"] + title_q + [\"[SEP]\"] + question + [\"[SEP\"]\n",
    "        q_input_ids = _get_ids(q_token, tokenizer, T_MAX_LEN+Q_MAX_LEN+3)\n",
    "        q_input_masks = _get_masks(q_token, T_MAX_LEN+Q_MAX_LEN+3)\n",
    "        q_input_segments = _get_segments(q_token, T_MAX_LEN+Q_MAX_LEN+3)\n",
    "        \n",
    "        #a_token = [\"[CLS]\"] + answer + [\"[SEP]\"]\n",
    "        #a_input_ids = _get_ids(a_token, tokenizer, A_MAX_LEN+2)\n",
    "        #a_input_masks = _get_masks(a_token, A_MAX_LEN+2)\n",
    "        #a_input_segments = _get_segments(a_token, A_MAX_LEN+2)\n",
    "        a_token = [\"[CLS]\"] + title_a + [\"[SEP]\"] + answer + [\"[SEP\"]\n",
    "        a_input_ids = _get_ids(a_token, tokenizer, T_MAX_LEN+A_MAX_LEN+3)\n",
    "        a_input_masks = _get_masks(a_token, T_MAX_LEN+A_MAX_LEN+3)\n",
    "        a_input_segments = _get_segments(a_token, T_MAX_LEN+A_MAX_LEN+3)\n",
    "        \n",
    "        return [q_input_ids, q_input_masks, q_input_segments, a_input_ids, a_input_masks, a_input_segments]\n",
    "\n",
    "\n",
    "def compute_input_arays(df, columns, tokenizer, max_sequence_length, num_features, cat_features, \n",
    "                        t_max_len=T_MAX_LEN, q_max_len=Q_MAX_LEN, a_max_len=A_MAX_LEN):\n",
    "    if COMBINE_INPUT:\n",
    "        input_ids, input_masks, input_segments = [], [], []\n",
    "        for _, instance in df[columns].iterrows():\n",
    "            t, q, a = instance.question_title, instance.question_body, instance.answer\n",
    "            t, q, a = _trim_input(tokenizer, t, q, a, max_sequence_length, t_max_len, q_max_len, a_max_len)\n",
    "            ids, masks, segments = _convert_to_bert_inputs(t, q, a, tokenizer, max_sequence_length)\n",
    "            input_ids.append(ids)\n",
    "            input_masks.append(masks)\n",
    "            input_segments.append(segments)\n",
    "        return [\n",
    "                torch.from_numpy(np.asarray(input_ids, dtype=np.int32)).long(), \n",
    "                torch.from_numpy(np.asarray(input_masks, dtype=np.int32)).long(),\n",
    "                torch.from_numpy(np.asarray(input_segments, dtype=np.int32)).long(),\n",
    "                torch.from_numpy(np.asarray(num_features, dtype=np.float32)).float(),\n",
    "                torch.from_numpy(np.asarray(cat_features, dtype=np.int32)).long(),\n",
    "                ]\n",
    "    else:\n",
    "        q_input_ids, q_input_masks, q_input_segments = [], [], []\n",
    "        a_input_ids, a_input_masks, a_input_segments = [], [], []\n",
    "        for _, instance in df[columns].iterrows():\n",
    "            t, q, a = instance.question_title, instance.question_body, instance.answer\n",
    "            t_q, q = q_trim_input(tokenizer, t, q, q_max_sequence_length, t_max_len, q_max_len)\n",
    "            #a = a_trim_input(tokenizer, a, a_max_sequence_length, a_max_len)\n",
    "            t_a, a = a_trim_input(tokenizer, t, a, a_max_sequence_length, t_max_len, a_max_len)\n",
    "            q_ids, q_masks, q_segments, a_ids, a_masks, a_segments = _convert_to_bert_inputs(t_q, t_a, q, a, tokenizer, max_sequence_length)\n",
    "            q_input_ids.append(q_ids)\n",
    "            q_input_masks.append(q_masks)\n",
    "            q_input_segments.append(q_segments)\n",
    "            a_input_ids.append(a_ids)\n",
    "            a_input_masks.append(a_masks)\n",
    "            a_input_segments.append(a_segments)\n",
    "        return [\n",
    "                torch.from_numpy(np.asarray(q_input_ids, dtype=np.int32)).long(),\n",
    "                torch.from_numpy(np.asarray(q_input_masks, dtype=np.int32)).long(),\n",
    "                torch.from_numpy(np.asarray(q_input_segments, dtype=np.int32)).long(),\n",
    "                torch.from_numpy(np.asarray(a_input_ids, dtype=np.int32)).long(),\n",
    "                torch.from_numpy(np.asarray(a_input_masks, dtype=np.int32)).long(),\n",
    "                torch.from_numpy(np.asarray(a_input_segments, dtype=np.int32)).long(),\n",
    "                torch.from_numpy(np.asarray(num_features, dtype=np.float32)).float(),\n",
    "                torch.from_numpy(np.asarray(cat_features, dtype=np.int32)).long(),\n",
    "                ]\n",
    "\n",
    "\n",
    "def compute_output_arrays(df, columns):\n",
    "    return np.asarray(df[columns])\n",
    "\n",
    "\n",
    "if COMBINE_INPUT:\n",
    "\n",
    "    class QuestDataset(torch.utils.data.Dataset):\n",
    "        def __init__(self, inputs, lengths, labels = None):\n",
    "\n",
    "            self.inputs = inputs\n",
    "            if labels is not None:\n",
    "                self.labels = labels\n",
    "            else:\n",
    "                self.labels = None\n",
    "            self.lengths = lengths\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "\n",
    "            input_ids       = self.inputs[0][idx]\n",
    "            input_masks     = self.inputs[1][idx]\n",
    "            input_segments  = self.inputs[2][idx]\n",
    "            num_features    = self.inputs[3][idx]\n",
    "            cat_features    = self.inputs[4][idx]\n",
    "            lengths         = self.lengths[idx]\n",
    "            if self.labels is not None: # targets\n",
    "                labels = self.labels[idx]\n",
    "                return input_ids, input_masks, input_segments, num_features, cat_features, labels, lengths\n",
    "            return input_ids, input_masks, input_segments, num_features, cat_features, lengths\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.inputs[0])\n",
    "\n",
    "\n",
    "    class CustomBert(BertPreTrainedModel):\n",
    "\n",
    "        def __init__(self, config, cat_dims):\n",
    "            super(CustomBert, self).__init__(config)\n",
    "            self.num_labels = config.num_labels\n",
    "            self.bert = BertModel(config)\n",
    "            self.embeddings = nn.ModuleList([\n",
    "                nn.Embedding(x, y) for x, y in cat_dims\n",
    "            ])\n",
    "            self.emb_drop = nn.Dropout(0.2)\n",
    "            n_emb_out = sum([y for x, y in cat_dims])\n",
    "            self.dropout = nn.Dropout(0.2)\n",
    "            self.classifier_final = nn.Linear(config.hidden_size+n_emb_out+4, self.config.num_labels)  # num_features=4\n",
    "\n",
    "            self.init_weights()\n",
    "\n",
    "        def forward(\n",
    "            self,\n",
    "            input_ids=None,\n",
    "            attention_mask=None,\n",
    "            token_type_ids=None,\n",
    "            num_features=None,\n",
    "            cat_features=None,\n",
    "            position_ids=None,\n",
    "            head_mask=None,\n",
    "            inputs_embeds=None,\n",
    "            labels=None,\n",
    "        ):\n",
    "\n",
    "            outputs = self.bert(\n",
    "                input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                token_type_ids=token_type_ids,\n",
    "                position_ids=position_ids,\n",
    "                head_mask=head_mask,\n",
    "                inputs_embeds=inputs_embeds,\n",
    "            )\n",
    "\n",
    "            pooled_output = outputs[1]\n",
    "            pooled_output = self.dropout(pooled_output)\n",
    "\n",
    "            emb = [\n",
    "                emb_layer(cat_features[:, j]) for j, emb_layer in enumerate(self.embeddings)\n",
    "            ]\n",
    "            emb = self.emb_drop(torch.cat(emb, 1))\n",
    "\n",
    "            pooled_output = torch.cat([pooled_output, num_features, emb], 1)\n",
    "            logits = self.classifier_final(pooled_output)\n",
    "\n",
    "            outputs = (logits,) + outputs[2:]  # add hidden states and attention if they are here\n",
    "            if labels is not None:\n",
    "                if self.num_labels == 1:\n",
    "                    #  We are doing regression\n",
    "                    loss_fct = MSELoss()\n",
    "                    loss = loss_fct(logits.view(-1), labels.view(-1))\n",
    "                else:\n",
    "                    loss_fct = CrossEntropyLoss()\n",
    "                    loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "                outputs = (loss,) + outputs\n",
    "\n",
    "            return outputs  # (loss), logits, (hidden_states), (attentions)\n",
    "\n",
    "else:\n",
    "\n",
    "    class QuestDataset(torch.utils.data.Dataset):\n",
    "        def __init__(self, inputs, lengths, labels = None):\n",
    "\n",
    "            self.inputs = inputs\n",
    "            if labels is not None:\n",
    "                self.labels = labels\n",
    "            else:\n",
    "                self.labels = None\n",
    "            self.lengths = lengths\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "\n",
    "            q_input_ids       = self.inputs[0][idx]\n",
    "            q_input_masks     = self.inputs[1][idx]\n",
    "            q_input_segments  = self.inputs[2][idx]\n",
    "            a_input_ids       = self.inputs[3][idx]\n",
    "            a_input_masks     = self.inputs[4][idx]\n",
    "            a_input_segments  = self.inputs[5][idx]\n",
    "            num_features    = self.inputs[6][idx]\n",
    "            cat_features    = self.inputs[7][idx]\n",
    "            lengths         = self.lengths[idx]\n",
    "            if self.labels is not None: # targets\n",
    "                labels = self.labels[idx]\n",
    "                return q_input_ids, q_input_masks, q_input_segments, a_input_ids, a_input_masks, a_input_segments, num_features, cat_features, labels, lengths\n",
    "            return q_input_ids, q_input_masks, q_input_segments, a_input_ids, a_input_masks, a_input_segments, num_features, cat_features, lengths\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.inputs[0])\n",
    "\n",
    "\n",
    "    class CustomBert(BertPreTrainedModel):\n",
    "\n",
    "        def __init__(self, config, cat_dims):\n",
    "            super(CustomBert, self).__init__(config)\n",
    "            self.num_labels = config.num_labels\n",
    "            self.bert = BertModel(config)\n",
    "            self.embeddings = nn.ModuleList([\n",
    "                nn.Embedding(x, y) for x, y in cat_dims\n",
    "            ])\n",
    "            self.emb_drop = nn.Dropout(0.4)\n",
    "            n_emb_out = sum([y for x, y in cat_dims])\n",
    "            self.num_drop = nn.Dropout(0.4)\n",
    "            self.q_dropout = nn.Dropout(0.4)\n",
    "            self.a_dropout = nn.Dropout(0.4)\n",
    "            #self.dropout_all = nn.Dropout(0.2)\n",
    "            #self.dropout_a = nn.Dropout(0.2)\n",
    "            #self.dropout_q = nn.Dropout(0.2)\n",
    "            #self.classifier_all = nn.Linear(config.hidden_size*2+n_emb_out+4, 64)  # num_features=4\n",
    "            #self.classifier_all = nn.Sequential(\n",
    "            #    nn.Linear(config.hidden_size*2+n_emb_out+4, 64),\n",
    "            #    nn.ReLU(inplace=True),\n",
    "            #)\n",
    "            #self.classifier_a = nn.Linear(config.hidden_size+n_emb_out+4, 64)  # num_features=4\n",
    "            #self.classifier_a = nn.Sequential(\n",
    "            #    nn.Linear(config.hidden_size+n_emb_out+4, 64),\n",
    "            #    nn.ReLU(inplace=True),\n",
    "            #)\n",
    "            #self.classifier_q = nn.Linear(config.hidden_size+n_emb_out+4, 64)  # num_features=4\n",
    "            #self.classifier_q = nn.Sequential(\n",
    "            #    nn.Linear(config.hidden_size+n_emb_out+4, 64),\n",
    "            #    nn.ReLU(inplace=True),\n",
    "            #)\n",
    "            self.classifier_final = nn.Linear(config.hidden_size*2+n_emb_out+4, self.config.num_labels)\n",
    "            #self.classifier_final = nn.Linear(64*3, self.config.num_labels)  # num_features=4\n",
    "            #self.classifier_final = nn.Sequential(\n",
    "            #    nn.BatchNorm1d(64*3),\n",
    "            #    nn.Linear(64*3, self.config.num_labels),\n",
    "            #)\n",
    "            self.init_weights()\n",
    "\n",
    "        def forward(\n",
    "            self,\n",
    "            q_input_ids=None,\n",
    "            q_attention_mask=None,\n",
    "            q_token_type_ids=None,\n",
    "            a_input_ids=None,\n",
    "            a_attention_mask=None,\n",
    "            a_token_type_ids=None,\n",
    "            num_features=None,\n",
    "            cat_features=None,\n",
    "            position_ids=None,\n",
    "            head_mask=None,\n",
    "            inputs_embeds=None,\n",
    "            labels=None,\n",
    "        ):\n",
    "\n",
    "            q_outputs = self.bert(\n",
    "                q_input_ids,\n",
    "                attention_mask=q_attention_mask,\n",
    "                token_type_ids=q_token_type_ids,\n",
    "                position_ids=position_ids,\n",
    "                head_mask=head_mask,\n",
    "                inputs_embeds=inputs_embeds,\n",
    "            )\n",
    "\n",
    "            q_pooled_output = q_outputs[1]\n",
    "            q_pooled_output = self.q_dropout(q_pooled_output)\n",
    "\n",
    "            a_outputs = self.bert(\n",
    "                a_input_ids,\n",
    "                attention_mask=a_attention_mask,\n",
    "                token_type_ids=a_token_type_ids,\n",
    "                position_ids=position_ids,\n",
    "                head_mask=head_mask,\n",
    "                inputs_embeds=inputs_embeds,\n",
    "            )\n",
    "\n",
    "            a_pooled_output = a_outputs[1]\n",
    "            a_pooled_output = self.a_dropout(a_pooled_output)\n",
    "\n",
    "            emb = [\n",
    "                emb_layer(cat_features[:, j]) for j, emb_layer in enumerate(self.embeddings)\n",
    "            ]\n",
    "            emb = self.emb_drop(torch.cat(emb, 1))\n",
    "\n",
    "            num_features = self.num_drop(num_features)\n",
    "\n",
    "            pooled_output = torch.cat([q_pooled_output, a_pooled_output, num_features, emb], 1)\n",
    "            #all_logits = self.classifier_all(pooled_output)\n",
    "            #all_logits = self.dropout_all(all_logits)\n",
    "            logits = self.classifier_final(pooled_output)\n",
    "            \n",
    "            #a_pooled_output = torch.cat([a_pooled_output, num_features, emb], 1)\n",
    "            #a_logits = self.classifier_a(a_pooled_output)\n",
    "            #a_logits = self.dropout_a(a_logits)\n",
    "\n",
    "            #q_pooled_output = torch.cat([q_pooled_output, num_features, emb], 1)\n",
    "            #q_logits = self.classifier_q(q_pooled_output)\n",
    "            #q_logits = self.dropout_q(q_logits)\n",
    "\n",
    "            #concat_logits = torch.cat([all_logits, q_logits, a_logits], 1)\n",
    "            #logits = self.classifier_final(concat_logits)\n",
    "\n",
    "            #logits = torch.cat([q_logits, a_logits], 1)\n",
    "\n",
    "            outputs = (logits,) + q_outputs[2:] + a_outputs[2:]  # add hidden states and attention if they are here\n",
    "            if labels is not None:\n",
    "                if self.num_labels == 1:\n",
    "                    #  We are doing regression\n",
    "                    loss_fct = MSELoss()\n",
    "                    loss = loss_fct(logits.view(-1), labels.view(-1))\n",
    "                else:\n",
    "                    loss_fct = CrossEntropyLoss()\n",
    "                    loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "                outputs = (loss,) + outputs\n",
    "\n",
    "            return outputs  # (loss), logits, (hidden_states), (attentions)\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, optimizer, criterion, scheduler, config):\n",
    "    \n",
    "    model.train()\n",
    "    avg_loss = 0.\n",
    "    avg_loss_1 = 0.\n",
    "    avg_loss_2 = 0.\n",
    "    avg_loss_3 = 0.\n",
    "    avg_loss_4 = 0.\n",
    "    avg_loss_5 = 0.\n",
    "    #tk0 = tqdm(enumerate(train_loader),total =len(train_loader))\n",
    "    optimizer.zero_grad()\n",
    "    for idx, batch in enumerate(train_loader):\n",
    "        if COMBINE_INPUT:\n",
    "            input_ids, input_masks, input_segments, num_features, cat_features, labels, _ = batch\n",
    "            input_ids, input_masks, input_segments, num_features, cat_features, labels = input_ids.to(device), input_masks.to(device), input_segments.to(device), num_features.to(device), cat_features.to(device), labels.to(device)            \n",
    "        \n",
    "            output_train = model(input_ids = input_ids.long(),\n",
    "                             labels = None,\n",
    "                             attention_mask = input_masks,\n",
    "                             token_type_ids = input_segments,\n",
    "                             num_features = num_features,\n",
    "                             cat_features = cat_features,\n",
    "                            )\n",
    "        else:\n",
    "            q_input_ids, q_input_masks, q_input_segments, a_input_ids, a_input_masks, a_input_segments, num_features, cat_features, labels, _ = batch\n",
    "            q_input_ids, q_input_masks, q_input_segments, a_input_ids, a_input_masks, a_input_segments, num_features, cat_features, labels = q_input_ids.to(device), q_input_masks.to(device), q_input_segments.to(device), a_input_ids.to(device), a_input_masks.to(device), a_input_segments.to(device), num_features.to(device), cat_features.to(device), labels.to(device)\n",
    "\n",
    "            output_train = model(q_input_ids = q_input_ids.long(),\n",
    "                             labels = None,\n",
    "                             q_attention_mask = q_input_masks,\n",
    "                             q_token_type_ids = q_input_segments,\n",
    "                             a_input_ids = a_input_ids.long(),\n",
    "                             a_attention_mask = a_input_masks,\n",
    "                             a_token_type_ids = a_input_segments,\n",
    "                             num_features = num_features,\n",
    "                             cat_features = cat_features,\n",
    "                            )\n",
    "        logits = output_train[0] #output preds\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        if (idx + 1) % config.accum_steps == 0:    \n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        avg_loss += loss.item() / (len(train_loader)*config.accum_steps)\n",
    "        if COMBINE_INPUT:\n",
    "            del input_ids, input_masks, input_segments, labels\n",
    "        else:\n",
    "            del q_input_ids, q_input_masks, q_input_segments, a_input_ids, a_input_masks, a_input_segments, labels\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    return avg_loss, avg_loss_1, avg_loss_2, avg_loss_3, avg_loss_4, avg_loss_5\n",
    "\n",
    "\n",
    "def val_model(model, criterion, val_loader, val_shape, batch_size=8):\n",
    "\n",
    "    avg_val_loss = 0.\n",
    "    model.eval() # eval mode\n",
    "    \n",
    "    valid_preds = np.zeros((val_shape, len(target_cols)))\n",
    "    original = np.zeros((val_shape, len(target_cols)))\n",
    "    \n",
    "    #tk0 = tqdm(enumerate(val_loader))\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for idx, batch in enumerate(val_loader):\n",
    "            if COMBINE_INPUT:\n",
    "                input_ids, input_masks, input_segments, num_features, cat_features, labels, _ = batch\n",
    "                input_ids, input_masks, input_segments, num_features, cat_features, labels = input_ids.to(device), input_masks.to(device), input_segments.to(device), num_features.to(device), cat_features.to(device), labels.to(device)            \n",
    "            \n",
    "                output_val = model(input_ids = input_ids.long(),\n",
    "                               labels = None,\n",
    "                               attention_mask = input_masks,\n",
    "                               token_type_ids = input_segments,\n",
    "                               num_features = num_features,\n",
    "                               cat_features = cat_features,\n",
    "                              )\n",
    "            else:\n",
    "                q_input_ids, q_input_masks, q_input_segments, a_input_ids, a_input_masks, a_input_segments, num_features, cat_features, labels, _ = batch\n",
    "                q_input_ids, q_input_masks, q_input_segments, a_input_ids, a_input_masks, a_input_segments, num_features, cat_features, labels = q_input_ids.to(device), q_input_masks.to(device), q_input_segments.to(device), a_input_ids.to(device), a_input_masks.to(device), a_input_segments.to(device), num_features.to(device), cat_features.to(device), labels.to(device)\n",
    "\n",
    "                output_val = model(q_input_ids = q_input_ids.long(),\n",
    "                             labels = None,\n",
    "                             q_attention_mask = q_input_masks,\n",
    "                             q_token_type_ids = q_input_segments,\n",
    "                             a_input_ids = a_input_ids.long(),\n",
    "                             a_attention_mask = a_input_masks,\n",
    "                             a_token_type_ids = a_input_segments,\n",
    "                             num_features = num_features,\n",
    "                             cat_features = cat_features,\n",
    "                            )\n",
    "            logits = output_val[0] #output preds\n",
    "            \n",
    "            avg_val_loss += criterion(logits, labels).item() / len(val_loader)\n",
    "            valid_preds[idx*batch_size : (idx+1)*batch_size] = logits.detach().cpu().squeeze().numpy()\n",
    "            original[idx*batch_size : (idx+1)*batch_size]    = labels.detach().cpu().squeeze().numpy()\n",
    "        \n",
    "        score = 0\n",
    "        preds = torch.sigmoid(torch.tensor(valid_preds)).numpy()\n",
    "        \n",
    "        # np.save(\"preds.npy\", preds)\n",
    "        # np.save(\"actuals.npy\", original)\n",
    "        \n",
    "        rho_val = np.mean([spearmanr(original[:, i], preds[:,i]).correlation for i in range(preds.shape[1])])\n",
    "        print('\\r val_spearman-rho: %s' % (str(round(rho_val, 5))), end = 100*' '+'\\n')\n",
    "        \n",
    "        for i in range(len(target_cols)):\n",
    "            logger.info(f\"{i}, {spearmanr(original[:,i], preds[:,i])}\")\n",
    "            score += np.nan_to_num(spearmanr(original[:, i], preds[:, i]).correlation)\n",
    "        \n",
    "    return avg_val_loss, score/len(target_cols)\n",
    "\n",
    "\n",
    "def predict_valid_result(model, val_loader, val_length, batch_size=32):\n",
    "\n",
    "    val_preds = np.zeros((val_length, len(target_cols)))\n",
    "    original = np.zeros((val_length, len(target_cols)))\n",
    "\n",
    "    model.eval()\n",
    "    tk0 = tqdm(enumerate(val_loader))\n",
    "    for idx, batch in tk0:\n",
    "        if COMBINE_INPUT:\n",
    "            input_ids, input_masks, input_segments, num_features, cat_features, labels, _ = batch\n",
    "            input_ids, input_masks, input_segments, num_features, cat_features, labels = input_ids.to(device), input_masks.to(device), input_segments.to(device), num_features.to(device), cat_features.to(device), labels.to(device)            \n",
    "            with torch.no_grad():\n",
    "                outputs = model(input_ids = input_ids.long(),\n",
    "                            labels = None,\n",
    "                            attention_mask = input_masks,\n",
    "                            token_type_ids = input_segments,\n",
    "                            num_features = num_features,\n",
    "                            cat_features = cat_features,\n",
    "                            )\n",
    "        else:\n",
    "            q_input_ids, q_input_masks, q_input_segments, a_input_ids, a_input_masks, a_input_segments, num_features, cat_features, labels, _ = batch\n",
    "            q_input_ids, q_input_masks, q_input_segments, a_input_ids, a_input_masks, a_input_segments, num_features, cat_features, labels = q_input_ids.to(device), q_input_masks.to(device), q_input_segments.to(device), a_input_ids.to(device), a_input_masks.to(device), a_input_segments.to(device), num_features.to(device), cat_features.to(device), labels.to(device)\n",
    "            with torch.no_grad():\n",
    "                outputs = model(q_input_ids = q_input_ids.long(),\n",
    "                             labels = None,\n",
    "                             q_attention_mask = q_input_masks,\n",
    "                             q_token_type_ids = q_input_segments,\n",
    "                             a_input_ids = a_input_ids.long(),\n",
    "                             a_attention_mask = a_input_masks,\n",
    "                             a_token_type_ids = a_input_segments,\n",
    "                             num_features = num_features,\n",
    "                             cat_features = cat_features,\n",
    "                            )\n",
    "\n",
    "        predictions = outputs[0]\n",
    "        val_preds[idx*batch_size : (idx+1)*batch_size] = predictions.detach().cpu().squeeze().numpy()\n",
    "        original[idx*batch_size : (idx+1)*batch_size] = labels.detach().cpu().squeeze().numpy()\n",
    "\n",
    "    output = torch.sigmoid(torch.tensor(val_preds)).numpy()\n",
    "    return output, original\n",
    "\n",
    "\n",
    "def predict_result(model, test_loader, test_length, batch_size=32):\n",
    "\n",
    "    test_preds = np.zeros((test_length, len(target_cols)))\n",
    "\n",
    "    model.eval()\n",
    "    tk0 = tqdm(enumerate(test_loader))\n",
    "    for idx, x_batch in tk0:\n",
    "        if COMBINE_INPUT:\n",
    "            with torch.no_grad():\n",
    "                outputs = model(input_ids = x_batch[0].to(device),\n",
    "                            labels = None,\n",
    "                            attention_mask = x_batch[1].to(device),\n",
    "                            token_type_ids = x_batch[2].to(device),\n",
    "                            num_features = x_batch[3].to(device),\n",
    "                            cat_features = x_batch[4].to(device),\n",
    "                           )\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                outputs = model(q_input_ids = x_batch[0].to(device),\n",
    "                            labels = None,\n",
    "                            q_attention_mask = x_batch[1].to(device),\n",
    "                            q_token_type_ids = x_batch[2].to(device),\n",
    "                            a_input_ids = x_batch[3].to(device),\n",
    "                            a_attention_mask = x_batch[4].to(device),\n",
    "                            a_token_type_ids = x_batch[5].to(device),\n",
    "                            num_features = x_batch[6].to(device),\n",
    "                            cat_features = x_batch[7].to(device),\n",
    "                           )\n",
    "        predictions = outputs[0]\n",
    "        test_preds[idx*batch_size : (idx+1)*batch_size] = predictions.detach().cpu().squeeze().numpy()\n",
    "\n",
    "    output = torch.sigmoid(torch.tensor(test_preds)).numpy()\n",
    "    return output\n",
    "\n",
    "\n",
    "def add_features(df):\n",
    "    find = re.compile(r\"^[^.]*\")\n",
    "    df['netloc'] = df['url'].apply(lambda x: re.findall(find, urlparse(x).netloc)[0])\n",
    "    df['qa_same_user_page_flag'] = (df['question_user_page']==df['answer_user_page'])*1\n",
    "    df['question_title_num_words'] = df['question_title'].str.count('\\S+')\n",
    "    df['question_body_num_words'] = df['question_body'].str.count('\\S+')\n",
    "    df['answer_num_words'] = df['answer'].str.count('\\S+')\n",
    "    df['question_vs_answer_length'] = df['question_body_num_words']/df['answer_num_words']\n",
    "    df['question_title_num_words'] = np.log1p(df['question_title_num_words'])\n",
    "    df['question_body_num_words'] = np.log1p(df['question_body_num_words'])\n",
    "    df['answer_num_words'] = np.log1p(df['answer_num_words'])\n",
    "    df['question_vs_answer_length'] = np.log1p(df['question_vs_answer_length'])\n",
    "    return df\n",
    "\n",
    "\n",
    "def custom_loss(logits, labels):\n",
    "    #vx = logits - torch.mean(logits)\n",
    "    #vy = labels - torch.mean(labels)\n",
    "    #p_loss = 1 - (torch.sum(vx * vy) / (torch.sqrt(torch.sum(vx ** 2)) * torch.sqrt(torch.sum(vy ** 2))))\n",
    "    #q_loss = nn.BCEWithLogitsLoss()(logits[:,:21], labels[:,:21])\n",
    "    #a_loss = nn.BCEWithLogitsLoss()(logits[:,21:], labels[:,21:])\n",
    "    #custom_loss = 0.5*q_loss + 0.5*a_loss\n",
    "    custom_loss = nn.BCEWithLogitsLoss()(logits, labels)\n",
    "    #loss1 = nn.BCEWithLogitsLoss()(logits[:,0:19], labels[:,0:19])\n",
    "    #loss2 = nn.BCEWithLogitsLoss()(logits[:,20:], labels[:,20:]) # except index=19\n",
    "    #custom_loss = loss1 + loss2\n",
    "    #custom_loss = 0.\n",
    "    #for i in range(len(loss_sample_weights)):\n",
    "    #    custom_loss += loss_sample_weights[i] * nn.BCEWithLogitsLoss()(logits[:,i], labels[:,i])\n",
    "    #custom_loss = 0.5*p_loss + 0.5*b_loss\n",
    "    return custom_loss\n",
    "\n",
    "\n",
    "def min_max(x, axis=None):\n",
    "    _min = x.min(axis=axis, keepdims=True)\n",
    "    _max = x.max(axis=axis, keepdims=True)\n",
    "    result = (x-_min)/(_max-_min)\n",
    "    return result\n",
    "\n",
    "#===========================================================\n",
    "# main\n",
    "#===========================================================\n",
    "#def main():\n",
    "if True:\n",
    "    \"\"\"\n",
    "    with timer('Data Loading'):\n",
    "        train = pd.read_csv(f\"{ROOT}train.csv\").fillna(\"none\")\n",
    "        for c in target_cols:\n",
    "            #train.loc[train[c]==0, c] = 0.2\n",
    "            train[c] = train[c].rank(method='dense').values\n",
    "            train[c] = train[c]/train[c].max()\n",
    "        y_train = train[target_cols].values\n",
    "        if config.test:\n",
    "            test = pd.read_csv(f\"{ROOT}test.csv\").fillna(\"none\")\n",
    "            submission = pd.read_csv(f\"{ROOT}sample_submission.csv\")\n",
    "    \n",
    "    with timer('Num features'):\n",
    "        train = add_features(train)\n",
    "        if config.test:\n",
    "            test = add_features(test)\n",
    "        num_features = ['question_title_num_words', 'question_body_num_words', 'answer_num_words', 'question_vs_answer_length']\n",
    "        train_num = train[num_features].values\n",
    "        if config.test:\n",
    "            test_num = test[num_features].values\n",
    "                \n",
    "    with timer('Cat features'):\n",
    "        cat_features = ['netloc', 'category', 'qa_same_user_page_flag']\n",
    "        ce_oe = ce.OrdinalEncoder(cols=cat_features, handle_unknown='return_nan')\n",
    "        ce_oe.fit(train[cat_features])\n",
    "        train_cat_df = ce_oe.transform(train[cat_features])\n",
    "        test_cat_df = ce_oe.transform(test[cat_features]).fillna(0).astype(int)\n",
    "        train_cat = train_cat_df.values\n",
    "        test_cat = test_cat_df.values\n",
    "        cat_dims = []\n",
    "        for col in cat_features:\n",
    "            dim = train[col].nunique()\n",
    "            cat_dims.append((dim+1, dim//2+1)) # for unknown=0\n",
    "        print(cat_dims)\n",
    "\n",
    "    if config.train:\n",
    "        with timer('Create folds'):\n",
    "            folds = train.copy()\n",
    "\n",
    "            kf = MultilabelStratifiedKFold(n_splits=NUM_FOLDS, random_state=SEED)\n",
    "            for fold, (train_index, val_index) in enumerate(kf.split(train.values, y_train)):\n",
    "                folds.loc[val_index, 'fold'] = int(fold)\n",
    "                \n",
    "            folds['fold'] = folds['fold'].astype(int)\n",
    "            save_cols = [ID] + target_cols + ['fold']\n",
    "            folds[save_cols].to_csv('folds.csv', index=None)\n",
    "\n",
    "    with timer('Prepare Bert config'):\n",
    "        tokenizer = BertTokenizer.from_pretrained(\"../input/pretrained-bert-models-for-pytorch/bert-base-uncased-vocab.txt\", \n",
    "                                                  do_lower_case=True)\n",
    "        input_categories = ['question_title', 'question_body', 'answer']\n",
    "        bert_model_config = '../input/pretrained-bert-models-for-pytorch/bert-base-uncased/bert_config.json'\n",
    "        bert_config = BertConfig.from_json_file(bert_model_config)\n",
    "        bert_config.num_labels = len(target_cols)\n",
    "        bert_model = 'bert-base-uncased'\n",
    "        do_lower_case = 'uncased' in bert_model\n",
    "        output_model_file = 'bert_pytorch.bin'\n",
    "    \n",
    "    if config.train:\n",
    "\n",
    "        BATCH_SIZE = 8\n",
    "        if DEBUG:\n",
    "            epochs = 1\n",
    "        else:\n",
    "            epochs = config.epochs\n",
    "        ACCUM_STEPS = config.accum_steps\n",
    "\n",
    "        with timer('Train Bert'):\n",
    "            \n",
    "            for fold in range(NUM_FOLDS):\n",
    "\n",
    "                logger.info(f\"Current Fold: {fold}\")\n",
    "                train_index = folds[folds.fold != fold].index\n",
    "                val_index = folds[folds.fold == fold].index\n",
    "\n",
    "                train_df, val_df = train.iloc[train_index], train.iloc[val_index]\n",
    "                logger.info(f\"Train Shapes: {train_df.shape}\")\n",
    "                logger.info(f\"Valid Shapes: {val_df.shape}\")\n",
    "            \n",
    "                logger.info(\"Preparing train datasets....\")\n",
    "            \n",
    "                inputs_train = compute_input_arays(train_df, input_categories, tokenizer, max_sequence_length=MAX_SEQUENCE_LENGTH, \n",
    "                                                   num_features=train_num[train_index], cat_features=train_cat[train_index])\n",
    "                outputs_train = compute_output_arrays(train_df, columns=target_cols)\n",
    "                outputs_train = torch.tensor(outputs_train, dtype=torch.float32)\n",
    "                lengths_train = np.argmax(inputs_train[0]==0, axis=1)\n",
    "                lengths_train[lengths_train==0] = inputs_train[0].shape[1]\n",
    "            \n",
    "                logger.info(\"Preparing valid datasets....\")\n",
    "            \n",
    "                inputs_valid = compute_input_arays(val_df, input_categories, tokenizer, max_sequence_length=MAX_SEQUENCE_LENGTH, \n",
    "                                                   num_features=train_num[val_index], cat_features=train_cat[val_index])\n",
    "                outputs_valid = compute_output_arrays(val_df, columns = target_cols)\n",
    "                outputs_valid = torch.tensor(outputs_valid, dtype=torch.float32)\n",
    "                lengths_valid = np.argmax(inputs_valid[0] == 0, axis=1)\n",
    "                lengths_valid[lengths_valid == 0] = inputs_valid[0].shape[1]\n",
    "            \n",
    "                logger.info(\"Preparing Dataloaders Datasets....\")\n",
    "\n",
    "                train_set = QuestDataset(inputs=inputs_train, lengths=lengths_train, labels=outputs_train)\n",
    "                train_sampler = RandomSampler(train_set)\n",
    "                train_loader = DataLoader(train_set, batch_size=BATCH_SIZE,sampler=train_sampler)\n",
    "            \n",
    "                valid_set = QuestDataset(inputs=inputs_valid, lengths=lengths_valid, labels=outputs_valid)\n",
    "                valid_loader = DataLoader(valid_set, batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n",
    "            \n",
    "                model = CustomBert.from_pretrained('../input/pretrained-bert-models-for-pytorch/bert-base-uncased/', config=bert_config, cat_dims=cat_dims)\n",
    "                model.zero_grad()\n",
    "                model.to(device)\n",
    "                torch.cuda.empty_cache()\n",
    "                if config.freeze : ## This is basically using out of the box bert model while training only the classifier head with our data . \n",
    "                    for param in model.bert.parameters():\n",
    "                        param.requires_grad = False\n",
    "                model.train()\n",
    "            \n",
    "                i = 0\n",
    "                best_avg_loss = 100.0\n",
    "                best_score = -1.\n",
    "                best_param_loss = None\n",
    "                best_param_score = None\n",
    "                param_optimizer = list(model.named_parameters())\n",
    "                no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "                optimizer_grouped_parameters = [\n",
    "                    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "                    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "                    ]        \n",
    "                optimizer = AdamW(optimizer_grouped_parameters, lr=config.lr, eps=4e-5)\n",
    "                #optimizer = torch.optim.AdamW(model.parameters(), lr=config.lr, eps=4e-5)\n",
    "                #criterion = nn.BCEWithLogitsLoss()\n",
    "                criterion = custom_loss\n",
    "                scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=config.warmup, num_training_steps=epochs*len(train_loader)//ACCUM_STEPS)\n",
    "                logger.info(\"Training....\")\n",
    "            \n",
    "                for epoch in tqdm(range(epochs)):\n",
    "\n",
    "                    torch.cuda.empty_cache()\n",
    "                \n",
    "                    start_time   = time.time()\n",
    "                    avg_loss, avg_loss_1, avg_loss_2, avg_loss_3, avg_loss_4, avg_loss_5 = train_model(model, train_loader, optimizer, criterion, scheduler, config)\n",
    "                    avg_val_loss, score = val_model(model, criterion, valid_loader, val_shape=val_df.shape[0], batch_size=BATCH_SIZE)\n",
    "                    elapsed_time = time.time() - start_time\n",
    "\n",
    "                    logger.info('Epoch {}/{} \\t loss={:.4f} \\t val_loss={:.4f} \\t train_loss={:.4f} \\t train_loss_1={:.4f} \\t train_loss_2={:.4f} \\t train_loss_3={:.4f} \\t train_loss_4={:.4f}  \\t train_loss_5={:.4f} \\t score={:.6f} \\t time={:.2f}s'.format(\n",
    "                        epoch+1, epochs, avg_loss, avg_val_loss, avg_loss, avg_loss_1, avg_loss_2, avg_loss_3, avg_loss_4, avg_loss_5, score, elapsed_time))\n",
    "\n",
    "                    if best_avg_loss > avg_val_loss:\n",
    "                        i = 0\n",
    "                        best_avg_loss = avg_val_loss \n",
    "                        best_param_loss = model.state_dict()\n",
    "\n",
    "                    if best_score < score:\n",
    "                        best_score = score\n",
    "                        best_param_score = model.state_dict()\n",
    "                        logger.info('best_param_score_{}_{}.pt'.format(config.expname ,fold))\n",
    "                        torch.save(best_param_score, 'best_param_score_{}_{}.pt'.format(config.expname, fold))\n",
    "                    else:\n",
    "                        i += 1\n",
    "\n",
    "            del train_df, val_df, model, optimizer, criterion, scheduler\n",
    "            del valid_loader, train_loader, valid_set, train_set\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "    \n",
    "    if config.cv:\n",
    "\n",
    "        with timer('CV'):\n",
    "\n",
    "            folds = pd.read_csv(f'{MODEL_DIR}folds.csv')\n",
    "            results = np.zeros((len(train), len(target_cols)))\n",
    "            logits = np.zeros((len(train), len(target_cols)))\n",
    "\n",
    "            for fold in range(NUM_FOLDS):\n",
    "                \n",
    "                #train_index = folds[folds.fold != fold].index\n",
    "                val_index = folds[folds.fold == fold].index\n",
    "                #train_df, val_df = train.iloc[train_index], train.iloc[val_index]\n",
    "                val_df = train.iloc[val_index]\n",
    "                \n",
    "                inputs_valid = compute_input_arays(val_df, input_categories, tokenizer, max_sequence_length=MAX_SEQUENCE_LENGTH, \n",
    "                                                   num_features=train_num[val_index], cat_features=train_cat[val_index])\n",
    "                outputs_valid = compute_output_arrays(val_df, columns = target_cols)\n",
    "                outputs_valid = torch.tensor(outputs_valid, dtype=torch.float32)\n",
    "                lengths_valid = np.argmax(inputs_valid[0] == 0, axis=1)\n",
    "                lengths_valid[lengths_valid == 0] = inputs_valid[0].shape[1]\n",
    "                valid_set = QuestDataset(inputs=inputs_valid, lengths=lengths_valid, labels=outputs_valid)\n",
    "                valid_loader = DataLoader(valid_set, batch_size=32, shuffle=False, drop_last=False)\n",
    "                \n",
    "                model = CustomBert.from_pretrained('../input/pretrained-bert-models-for-pytorch/bert-base-uncased/', config=bert_config, cat_dims=cat_dims)\n",
    "                model.zero_grad()\n",
    "                model.to(device)\n",
    "                model.load_state_dict(torch.load(f'{MODEL_DIR}best_param_score_{config.expname}_{fold}.pt'))\n",
    "                result, logit = predict_valid_result(model, valid_loader, len(val_df))  \n",
    "                results[val_index, :] = result\n",
    "                logits[val_index, :] = logit \n",
    "            \n",
    "            rho_val = np.mean([spearmanr(logits[:,i], results[:,i]).correlation for i in range(results.shape[1])])\n",
    "            logger.info(f'CV spearman-rho: {round(rho_val, 5)}')\n",
    "\n",
    "            oof = pd.DataFrame()\n",
    "            for i, col in enumerate(target_cols):\n",
    "                oof[col] = results[:,i]\n",
    "            oof.to_csv(f'oof_{config.expname}.csv', index=False)\n",
    "    \"\"\"\n",
    "    if config.test:\n",
    "\n",
    "        with timer('Inference'):\n",
    "            #===================================\n",
    "            # same as before config\n",
    "            #===================================\n",
    "            #test_inputs = compute_input_arays(test, input_categories, tokenizer, max_sequence_length=MAX_SEQUENCE_LENGTH, \n",
    "            #                                  num_features=test_num, cat_features=test_cat)\n",
    "            #lengths_test = np.argmax(test_inputs[0] == 0, axis=1)\n",
    "            #lengths_test[lengths_test == 0] = test_inputs[0].shape[1]\n",
    "            #test_set = QuestDataset(inputs=test_inputs, lengths=lengths_test, labels=None)\n",
    "            #test_loader  = DataLoader(test_set, batch_size=32, shuffle=False)\n",
    "            result = np.zeros((len(test), len(target_cols)))\n",
    "\n",
    "            for fold in range(NUM_FOLDS):\n",
    "                model = CustomBert.from_pretrained('../input/pretrained-bert-models-for-pytorch/bert-base-uncased/', config=bert_config, cat_dims=cat_dims)\n",
    "                model.zero_grad()\n",
    "                model.to(device)\n",
    "                model.load_state_dict(torch.load(f'{MODEL_DIR}best_param_score_{config.expname}_{fold}.pt'))\n",
    "                result += predict_result(model, test_loader, len(test)) \n",
    "                if DEBUG:\n",
    "                    break\n",
    "                    \n",
    "            result /= NUM_FOLDS\n",
    "\n",
    "        with timer('Create submission.csv'):\n",
    "            submission.loc[:, 'question_asker_intent_understanding':] = result\n",
    "            submission.to_csv('submission7.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Ensemble] start\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oof1 CV spearman-rho1: 0.39754\n",
      "oof2 CV spearman-rho2: 0.39892\n",
      "oof3 CV spearman-rho2: 0.3944\n",
      "oof4 CV spearman-rho2: 0.38585\n",
      "oof5 CV spearman-rho2: 0.39704\n",
      "oof7 CV spearman-rho2: 0.39365\n",
      "oof_lgb CV spearman-rho2: 0.40056\n",
      "CV spearman-rho: 0.42139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Ensemble] done in 2 s\n"
     ]
    }
   ],
   "source": [
    "if True:\n",
    "    if True:\n",
    "        \n",
    "        with timer('Ensemble'):\n",
    "            \n",
    "            oof1 = pd.read_csv('../input/googlequestchallenge-weights1/oof.csv')\n",
    "            oof2 = pd.read_csv('../input/googlequestchallenge-weights2/oof.csv')\n",
    "            oof3 = pd.read_csv('../input/googlequestchallenge-weights3/oof.csv')\n",
    "            oof4 = pd.read_csv('../input/googlequestchallenge-weights4/oof.csv')\n",
    "            oof5 = pd.read_csv('../input/googlequestchallenge-weights5/oof.csv')\n",
    "            #oof6 = pd.read_csv('../input/googlequestchallenge-weights6/oof.csv')\n",
    "            oof7 = pd.read_csv('../input/googlequestchallenge-weights7/oof.csv')\n",
    "            oof_lgb = pd.read_csv('../input/train-lgb-with-bert-features-xentropy/lgb_oof.csv')\n",
    "            #oof_spell = pd.read_csv('../input/googlequestchallenge-spell/oof.csv')\n",
    "            \n",
    "            rho_val = np.mean([spearmanr(train.loc[:,i], oof1.loc[:,i]).correlation for i in target_cols])\n",
    "            print(f'oof1 CV spearman-rho1: {round(rho_val, 5)}')\n",
    "            rho_val = np.mean([spearmanr(train.loc[:,i], oof2.loc[:,i]).correlation for i in target_cols])\n",
    "            print(f'oof2 CV spearman-rho2: {round(rho_val, 5)}')\n",
    "            rho_val = np.mean([spearmanr(train.loc[:,i], oof3.loc[:,i]).correlation for i in target_cols])\n",
    "            print(f'oof3 CV spearman-rho2: {round(rho_val, 5)}')\n",
    "            rho_val = np.mean([spearmanr(train.loc[:,i], oof4.loc[:,i]).correlation for i in target_cols])\n",
    "            print(f'oof4 CV spearman-rho2: {round(rho_val, 5)}')\n",
    "            rho_val = np.mean([spearmanr(train.loc[:,i], oof5.loc[:,i]).correlation for i in target_cols])\n",
    "            print(f'oof5 CV spearman-rho2: {round(rho_val, 5)}')\n",
    "            #rho_val = np.mean([spearmanr(train.loc[:,i], oof6.loc[:,i]).correlation for i in target_cols])\n",
    "            #print(f'oof6 CV spearman-rho2: {round(rho_val, 5)}')\n",
    "            rho_val = np.mean([spearmanr(train.loc[:,i], oof7.loc[:,i]).correlation for i in target_cols])\n",
    "            print(f'oof7 CV spearman-rho2: {round(rho_val, 5)}')\n",
    "            rho_val = np.mean([spearmanr(train.loc[:,i], oof_lgb.loc[:,i]).correlation for i in target_cols])\n",
    "            print(f'oof_lgb CV spearman-rho2: {round(rho_val, 5)}')\n",
    "            #rho_val = spearmanr(train['question_type_spelling'], oof_spell['question_type_spelling']).correlation\n",
    "            #print(f'oof_spell question_type_spelling spearman-rho2: {round(rho_val, 5)}')\n",
    "            \n",
    "            weight_dict = {#'question_asker_intent_understanding': [0.14423382227545373, 0.26403362581825757, 0.21342671281564143, 0.07873806029749497, 0.06163222126953327, 0.19133353793688565, 0.04660201958673338], \n",
    "                           'question_asker_intent_understanding': [0., 0.40328523, 0.15753677,\n",
    "                                                                   0., 0.13640727, 0.22840674, 0.07436399],\n",
    "                           'question_body_critical': [0., 0.06178358, 0.0292628, \n",
    "                                                      0., 0.06584192, 0.45798585, 0.38512585], \n",
    "                           #'question_conversational': [0.10164748898770576, 0.18405234477802096, 0.16808611052544156, 0.106469745260419, 0.35601581737694876, 0.07568642836573576, 0.008042064705728246], \n",
    "                           'question_conversational': [0.08811661, 0.24527189, 0.19405812,\n",
    "                                                      0.05633958, 0.33125105, 0.08496275, 0],\n",
    "                           'question_expect_short_answer': [0.11924842, 0.19755374, 0.,\n",
    "                                                            0.2523071,  0.10030623, 0.33058451, 0], \n",
    "                           'question_fact_seeking': [0.013202065769539835, 0.26707698812113057, 0.0317382709215506, 0.21667859598041936, 0.004755702783864884, 0.32050538984849064, 0.14604298657500414], \n",
    "                           #'question_has_commonly_accepted_answer': [0.13854796335510416, 0.17812783621630807, 0.07317431478650147, 0.12849381596205142, 0.22461229668945618, 0.22545398495625127, 0.031589788034327416],\n",
    "                           'question_has_commonly_accepted_answer': [0.14306743324595933, 0.18393841165175798, 0.07556127959242885,\n",
    "                                                                     0.13268531700139297, 0.23193920707789695, 0.232808351430564, 0],\n",
    "                           'question_interestingness_others': [0., 0.1564567, 0.07727025,\n",
    "                                                               0.20987616, 0., 0.32132959, 0.2350673], \n",
    "                           'question_interestingness_self': [0.0764107430765866, 0.1250758634055338, 0.11271321495327034, 0.03549773316001633, 0.12661862801239315, 0.27569063228935353, 0.24799318510284624],\n",
    "                           #'question_multi_intent': [0.07242022218179081, 0.07402376048693433, 0.05455913079557168, 0.2697231170434426, 0.0812918639251851, 0.23737074429530128, 0.21061116127177426],\n",
    "                           'question_multi_intent': [0., 0.1564567, 0.07727025,\n",
    "                                                     0.20987616, 0., 0.32132959, 0.2350673],\n",
    "                           'question_not_really_a_question': [0.22173249960012137, 0.17070331144640122, 0.03800925013012738, 0.12092933866649534, 0.23763807386006638, 0.15542059707171976, 0.05556692922506842], \n",
    "                           #'question_opinion_seeking': [0.07579225676041464, 0.175130003847942, 0.15289957108380914, 0.31238211059467763, 0.06223106101140025, 0.1232044425977646, 0.09836055410399193],\n",
    "                           'question_opinion_seeking': [0.12353181, 0.1277901, 0.18055271,\n",
    "                                                       0.28815662, 0.02141769, 0.25855108, 0],\n",
    "                           'question_type_choice': [0.13678921128070468, 0.2087929293477967, 0.20735486917872442, 0.1897499074181206, 0.008347306586780264, 0.16089103619963713, 0.08807473998823628],\n",
    "                           #'question_type_compare': [0.026828916116782557, 0.2799801833284125, 0.13942302336501752, 0.06988229992716304, 0.1530419814053994, 0.08227884557595866, 0.24856475028126637],\n",
    "                           'question_type_compare': [0.05881545, 0.45905453, 0.14891039,\n",
    "                                                    0.08329721, 0.1998714, 0.05005102, 0],\n",
    "                           'question_type_consequence': [5.55111512e-17, 2.72271947e-01, 5.92000967e-01,\n",
    "                                                         5.55111512e-17, 5.55111512e-17, 1.35727085e-01, 0],\n",
    "                           #'question_type_definition': [0.029721437749985875, 0.17635250438115008, 0.4397785287791846, 0.013185584358467851, 0.12560976132864476, 0.030970915950460667, 0.18438126745210612], \n",
    "                           'question_type_definition': [1.22679480e-02, 3.47201249e-01, 4.05141867e-01,\n",
    "                                                       1.09169328e-01, 1.26092856e-01, 1.26751081e-04, 0],\n",
    "                           'question_type_entity': [0.0418549535997241, 0.32667789354357957, 0.10596940155493283, 0.05927695959413755, 0.1422348839544507, 0.035617908903690404, 0.2883679988494848], \n",
    "                           'question_type_instructions': [0.13678921128070468, 0.2087929293477967, 0.20735486917872442, 0.1897499074181206, 0.008347306586780264, 0.16089103619963713, 0.08807473998823628],\n",
    "                           #'question_type_procedure': [0.21076627571101972, 0.17070172694839036, 0.06696570361809562, 0.2038405617405412, 0.08418389425529142, 0.2520156600987512, 0.011526177627910493],\n",
    "                           'question_type_procedure': [0.00000000e+00, 3.30383233e-01, 4.16333634e-17,\n",
    "                                                       1.83175459e-01, 0.00000000e+00, 4.86441308e-01, 0.00000000e+00],\n",
    "                           'question_type_reason_explanation': [0.22229041, 0.06843367, 0.21228687,\n",
    "                                                                0.14428088, 0.06463155, 0.28807661, 0],\n",
    "                           'question_type_spelling': [0.19286676590112609, 0.04242283968733379, 0.20523158390448215, 0.11685920202552116, 0.2257236270657778, 0.0008294038366749291, 0.21606657757908418], \n",
    "                           'question_well_written': [0.00000000e+00, 2.70205701e-01, 5.55111512e-17,\n",
    "                                                     5.55111512e-17, 0.00000000e+00, 4.13528567e-01, 3.16265732e-01],\n",
    "                           'answer_helpful': [0.013202065769539835, 0.26707698812113057, 0.0317382709215506, 0.21667859598041936, 0.004755702783864884, 0.32050538984849064, 0.14604298657500414], \n",
    "                           'answer_level_of_information': [0.12867320477143743, 0.11123532800986519, 0.05661504097529127, 0.1587144287756197, 0.002635259339544268, 0.29671865993536584, 0.24540807819287627], \n",
    "                           #'answer_plausible': [0.14423382227545373, 0.26403362581825757, 0.21342671281564143, 0.07873806029749497, 0.06163222126953327, 0.19133353793688565, 0.04660201958673338],\n",
    "                           'answer_plausible': [2.07685008e-01, 1.17305096e-01, 3.25729756e-01,\n",
    "                                                1.11022302e-16, 0.00000000e+00, 3.49280140e-01, 0],\n",
    "                           'answer_relevance': [0.27761237, 0.151558, 0.13026388, \n",
    "                                                0., 0.11851019, 0.32205556, 0], \n",
    "                           'answer_satisfaction': [0.09088489, 0., 0.31911212,\n",
    "                                                   0., 0.29274372, 0.19064694, 0.10661232], \n",
    "                           #'answer_type_instructions': [0.13678921128070468, 0.2087929293477967, 0.20735486917872442, 0.1897499074181206, 0.008347306586780264, 0.16089103619963713, 0.08807473998823628], \n",
    "                           'answer_type_instructions': [2.35330575e-01, 8.71562313e-02, 2.59208132e-01, \n",
    "                                                        2.19053323e-01, 2.22044605e-16, 1.99251739e-01, 0],\n",
    "                           #'answer_type_procedure': [0.3358637794463721, 0.010657239458323304, 0.08600332206081915, 0.25234219821192166, 0.04010421246858745, 0.2530397629419472, 0.021989485412029098], \n",
    "                           'answer_type_procedure': [0.25387628, 0.12267789, 0.14188704, \n",
    "                                                    0.3124254, 0.0196214, 0.149512, 0],\n",
    "                           #'answer_type_reason_explanation': [0.08347908484814268, 0.1597305268386166, 0.14420297234016713, 0.17020547001124728, 0.07517657152542827, 0.19206020951638134, 0.17514516492001675],\n",
    "                           'answer_type_reason_explanation': [0.25138575, 0.14860905, 0.10729707,\n",
    "                                                             0.22231021, 0.04555059, 0.22484732, 0],\n",
    "                           'answer_well_written': [0.09088489, 0., 0.31911212,\n",
    "                                                   0., 0.29274372, 0.19064694, 0.10661232]}\n",
    "            \n",
    "            oof = pd.DataFrame()\n",
    "            for c in target_cols:\n",
    "                weight = weight_dict[c]\n",
    "                oof[c] = weight[0]*oof1[c] + weight[1]*oof2[c] + weight[2]*oof3[c] \\\n",
    "                            + weight[3]*oof4[c] + weight[4]*oof5[c] + weight[5]*oof7[c] + weight[6]*oof_lgb[c]\n",
    "            #oof['question_type_spelling'] = oof_spell['question_type_spelling']\n",
    "            rho_val = np.mean([spearmanr(train.loc[:,i], oof.loc[:,i]).correlation for i in target_cols])\n",
    "            print(f'CV spearman-rho: {round(rho_val, 5)}')\n",
    "            oof.to_csv('oof.csv', index=False)\n",
    "            \n",
    "            sub1 = pd.read_csv('submission1.csv')\n",
    "            sub2 = pd.read_csv('submission2.csv')\n",
    "            sub3 = pd.read_csv('submission3.csv')\n",
    "            sub4 = pd.read_csv('submission4.csv')\n",
    "            sub5 = pd.read_csv('submission5.csv')\n",
    "            #sub6 = pd.read_csv('submission6.csv')\n",
    "            sub7 = pd.read_csv('submission7.csv')\n",
    "            sub_lgb = pd.read_csv('submission_lgb.csv')\n",
    "            #sub_spell = pd.read_csv('submission_spell.csv')\n",
    "            \n",
    "            submission = pd.read_csv(f\"{ROOT}sample_submission.csv\")\n",
    "            for c in target_cols:\n",
    "                weight = weight_dict[c]\n",
    "                submission[c] = weight[0]*sub1[c] + weight[1]*sub2[c] + weight[2]*sub3[c]\\\n",
    "                                    + weight[3]*sub4[c] + weight[4]*sub5[c] + weight[5]*sub7[c] + weight[6]*sub_lgb[c]\n",
    "            #submission['question_type_spelling'] = sub_spell['question_type_spelling']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Post process] start\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rho_val: 0.4213930504075034\n",
      "\n",
      "question_asker_intent_understanding\n",
      "0.39130295236866125\n",
      "0.3960832308562305\n",
      "\n",
      "question_body_critical\n",
      "0.6790887363057522\n",
      "0.682422715502982\n",
      "\n",
      "question_conversational\n",
      "0.4180530698616003\n",
      "0.5173198563036796\n",
      "\n",
      "question_expect_short_answer\n",
      "0.3188913523811039\n",
      "0.3229965222021559\n",
      "\n",
      "question_fact_seeking\n",
      "0.3820288083881838\n",
      "0.3895198034976137\n",
      "\n",
      "question_interestingness_others\n",
      "0.3731809977675753\n",
      "0.37909850585099436\n",
      "\n",
      "question_interestingness_self\n",
      "0.5080258622066897\n",
      "0.5260439360987756\n",
      "\n",
      "question_multi_intent\n",
      "0.6088383936031573\n",
      "0.620502359544521\n",
      "\n",
      "question_not_really_a_question\n",
      "0.08303998324509837\n",
      "0.109441811844358\n",
      "\n",
      "question_opinion_seeking\n",
      "0.5005355833347301\n",
      "0.5036638496515978\n",
      "\n",
      "question_type_choice\n",
      "0.7587243059572895\n",
      "0.7823180189901875\n",
      "\n",
      "question_type_compare\n",
      "0.3600865812222029\n",
      "0.542400043533202\n",
      "\n",
      "question_type_consequence\n",
      "0.17839223325338696\n",
      "0.23579115389524735\n",
      "\n",
      "question_type_definition\n",
      "0.3570649987747841\n",
      "0.6260806771190092\n",
      "\n",
      "question_type_entity\n",
      "0.4611885869484818\n",
      "0.6301652529056948\n",
      "\n",
      "question_type_instructions\n",
      "0.7894577901773894\n",
      "0.8010518040077329\n",
      "\n",
      "question_type_procedure\n",
      "0.358848842367217\n",
      "0.3684465785775644\n",
      "\n",
      "question_type_reason_explanation\n",
      "0.6828653830583097\n",
      "0.6872751101377834\n",
      "\n",
      "question_type_spelling\n",
      "0.07013541561386603\n",
      "0.11579782893292247\n",
      "\n",
      "question_well_written\n",
      "0.5464843574966832\n",
      "0.5507032354062408\n",
      "\n",
      "answer_helpful\n",
      "0.26328810244053963\n",
      "0.27124465044428464\n",
      "\n",
      "answer_level_of_information\n",
      "0.4564715785115994\n",
      "0.4620509675946931\n",
      "\n",
      "answer_plausible\n",
      "0.1697124643341371\n",
      "0.17844968729667834\n",
      "\n",
      "answer_relevance\n",
      "0.18425242194457392\n",
      "0.1920778638534435\n",
      "\n",
      "answer_satisfaction\n",
      "0.3282776801353866\n",
      "0.33571819621015364\n",
      "\n",
      "answer_type_instructions\n",
      "0.7684587558427923\n",
      "0.774676138567074\n",
      "\n",
      "answer_type_procedure\n",
      "0.2953108525156523\n",
      "0.3026347562686987\n",
      "\n",
      "answer_type_reason_explanation\n",
      "0.6963133965961608\n",
      "0.6996059881887067\n",
      "\n",
      "answer_well_written\n",
      "0.21284784462699347\n",
      "0.22239443087592642\n",
      "\n",
      "question_has_commonly_accepted_answer\n",
      "0.44062418094510586\n",
      "0.49856274642196846\n",
      "0.457484590686004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Post process] done in 2 s\n"
     ]
    }
   ],
   "source": [
    "if True:\n",
    "    if True:\n",
    "        \n",
    "        with timer('Post process'):\n",
    "            \n",
    "            #oof = pd.read_csv('oof.csv')\n",
    "            sub = submission.copy()\n",
    "            rho_val = np.mean([spearmanr(train[c], oof[c]).correlation for c in target_cols])\n",
    "            print(f'rho_val: {rho_val}')\n",
    "            \n",
    "            # min_thredhols_samples = 100\n",
    "            col_threshold1 = {#'question_asker_intent_understanding': [0], \n",
    "                              'question_asker_intent_understanding': [0.7798771982510944, 0.7920811109335039, 0.8557452293617155, 0.8782904388166711, 0.902854965507151, 0.9097216513183621, 0.9235148682909096, 0.9377637285504432, 0.9451765158821579, 0.9588248023972451, 0.9653175323909384, 0.9762548886633277, 0.9796716368024583, 0.9819606762586988],\n",
    "                              'question_body_critical': [0.1495922097481511, 0.21278660744188876, 0.23628865235084048, 0.2800908098730057, 0.40498203159739254, 0.46787137110401567, 0.525492636979446, 0.6413243238866531, 0.6708672354281013, 0.7447139221830796, 0.7863341761178992, 0.8420947603145688, 0.8868824178173214, 0.9299641280918173, 0.9545921786495899], \n",
    "                              #'question_conversational': [0.15730855304042254, 0.20423813988542985, 0.2536424055146332, 0.3373688504302154, 0.5221058062019643, 0.740816428624697], \n",
    "                              'question_conversational': [0.1525069766760582, 0.21973333527958397,\n",
    "                                                          0.2501086280984717, 0.29798695846565126,\n",
    "                                                          0.38246143730815246, 0.5331792015729804,\n",
    "                                                          0.7177321753573026, 0.7506112118301018],\n",
    "                              'question_expect_short_answer': [0.2611144109126207, 0.38349116914923986, 0.4656468242600945, 0.5218841911294037, 0.6563936376568509, 0.799586370168953, 0.8348646742971397, 0.8443924042003939, 0.8855802026356525, 0.9345125552918279, 0.9497116097962152, 0.9683335385975709],\n",
    "                              'question_fact_seeking': [0.3031777585182752, 0.38923299438492953, 0.5141984122575314, 0.5956082724766223, 0.6716870783315136, 0.8723047191802196, 0.9417597045808908, 0.9561960519695563, 0.9626713507691174, 0.9746346737387603], \n",
    "                              'question_interestingness_others': [0.44431287572719097, 0.4982307314319283, 0.5144904023708171, 0.5416725846068623, 0.6036645848631093, 0.6294690584877651, 0.7197414180193262, 0.7456757663769422, 0.7497264557912451, 0.7573632343682353, 0.7821305934341662],\n",
    "                              'question_interestingness_self': [0.42498338810662023, 0.4703352513449347, 0.5266966945624868, 0.5614019215799894, 0.6286043708485128, 0.6696881575056536, 0.6974721297586757, 0.8208525195006722], \n",
    "                              #'question_multi_intent': [0.2002102301339863, 0.24164563322001426, 0.27837575301138995, 0.32605090736484393, 0.44463146451870367, 0.6347091999212645, 0.7574897330637935, 0.8343621386815206],\n",
    "                              'question_multi_intent': [0.21574519340526252, 0.24455823906844346, 0.2882639741661787, 0.3175744065853631, 0.3728078616670225, 0.437773812330742, 0.45636612809188054, 0.6131775748570878, 0.6520070969720705, 0.7848995633232405, 0.8452294317089482, 0.9186552533350643, 0.9344598516526684, 0.9452438987246035],\n",
    "                              'question_not_really_a_question': [0.03939258439095743, 0.04976097847575716, 0.09891222789537282],\n",
    "                              #'question_opinion_seeking': [0.11192085372578475, 0.19644661036700625, 0.25217730232909336, 0.4722478013564555, 0.5385586884122188, 0.6146146571699068, 0.7088051861050262, 0.7601569084293851, 0.8710683264407086, 0.9234604611980788], \n",
    "                              'question_opinion_seeking': [0.13289964719018157, 0.20618607043554066,\n",
    "                                                           0.2604280423363276, 0.3069630972895149, \n",
    "                                                           0.5348804402114498, 0.5987618913550097, \n",
    "                                                           0.7052586456484844, 0.7422339214461927, \n",
    "                                                           0.7846790466669848, 0.8743318962006583, \n",
    "                                                           0.9201453647137279, 0.939505738787939, \n",
    "                                                           0.9623826709445682],\n",
    "                              'question_type_choice': [0.16189209863769405, 0.18041577490958538, 0.20375853233399585, 0.28264195949237053, 0.3174840123555569, 0.43230607635119594, 0.5614214858568691, 0.6778646218309891, 0.7760264291410569, 0.8869531382380472, 0.9136651272394836, 0.9545537136899828], \n",
    "                              #'question_type_compare': [0.20054487492113204, 0.3470358764633537], \n",
    "                              'question_type_compare': [0.18990925922981028, 0.40123795613313257, \n",
    "                                                        0.5589969999807299, 0.6779192540256538, \n",
    "                                                        0.742416114956296, 0.7936005539683534, \n",
    "                                                        0.8421920381249596], \n",
    "                              'question_type_consequence': [0.06038017543220174, 0.0702390460947302, 0.09607382454868457, 0.20571409772171417, 0.21407418656760632],\n",
    "                              #'question_type_definition': [0.15763240556375235, 0.3526881294414419],\n",
    "                              'question_type_definition': [0.11180949041294634, 0.24956385868209166, \n",
    "                                                           0.32807333040120734, 0.41919767202577446,\n",
    "                                                           0.7795623708837238, 0.8149852210754654], \n",
    "                              'question_type_entity': [0.18025714657180678, 0.29494450802633304, 0.47341091797993157, 0.7023626874921312, 0.9073733560868775], \n",
    "                              'question_type_instructions': [0.11545000730019972, 0.19799267408587765, 0.23550907733286972, 0.3017076630685107, 0.40247920086274774, 0.4489292237364733, 0.5159060765432031, 0.6665511093010561, 0.9636617010717549], \n",
    "                              #'question_type_procedure': [0.06609997032258343, 0.09008359578030727, 0.11837768309646264, 0.14599606547596422, 0.16901712218515597, 0.3758457628027094, 0.4414868926563922], \n",
    "                              'question_type_procedure': [0.11004606575356937, 0.13441472465425375, 0.17708316018945558, 0.20675465774674012, 0.2309187642730335, 0.33345429362849754, 0.38374262465375975, 0.4168174490935639, 0.4389489980778297, 0.4846348516432625, 0.5280430388808683, 0.5770837436375043],\n",
    "                              'question_type_reason_explanation': [0.18568368940670632, 0.24868573236528568, 0.2954520477866478, 0.34726564658051806, 0.46702325384626997, 0.5946945524603777, 0.6698542085890401, 0.753313352465967, 0.7811815673471862, 0.8526310272505137, 0.90231676523946, 0.9217352725526369, 0.9601912064297389, 0.9765686258376298, 0.9830687754020904],\n",
    "                              #'question_type_spelling': [0.22019172620024594], \n",
    "                              'question_type_spelling': [0.0029089999],\n",
    "                              'question_well_written': [0.4207639178763126, 0.5610948503866817, 0.6662563857158016, 0.737045736721135, 0.7998574481149376, 0.8442311848896967, 0.8976912762705032, 0.9206109761152341, 0.9551348125781118, 0.9578814586064871, 0.9623856082601339, 0.9712609208706265, 0.975746144221896], \n",
    "                              'answer_helpful': [0.6984034089413959, 0.7983687269189049, 0.8655643088812476, 0.9001466801353707, 0.935729499047471, 0.9579834480656675, 0.9755939591623526, 0.9777704009081776, 0.9810771630408655],\n",
    "                              'answer_level_of_information': [0.3715275553789707, 0.396338749814461, 0.4587861594679614, 0.5148828192403583, 0.5315244715903833, 0.5594341091161936, 0.6084747296859907, 0.6336569576359905, 0.6595364221929101, 0.6846156176541431, 0.7047383742278317, 0.7167593572520277, 0.7455446587073238, 0.7817465842259476, 0.7983501095328426, 0.8119815931181447],\n",
    "                              #'answer_plausible': [0.8970473473995253, 0.9319313682149294, 0.9607598067122134, 0.9720762804218548, 0.9774262965010801, 0.9794692016815529, 0.988353238268427], \n",
    "                              'answer_plausible': [0.9094755568746322, 0.928591183031122, 0.9473102584235416, 0.9628147848474388, 0.9784702155497013, 0.9843023378554436, 0.9896587797725087, 0.9900177500223624],\n",
    "                              'answer_relevance': [0.8948966045153495, 0.9435168086799155, 0.9545568302156637, 0.9622565427863874, 0.9830454076528758, 0.9911405860892273, 0.9921045218942421, 0.9923499406262755],\n",
    "                              'answer_satisfaction': [0.6293044939394735, 0.7121716066041376, 0.7561107046712046, 0.7914277468387129, 0.8298867650435837, 0.883850436430629, 0.9117851387565115, 0.9210891741477153, 0.9378713261848577, 0.9508567331490615, 0.9585898967280264],\n",
    "                              #'answer_type_instructions': [0.10378756640878524, 0.12650578354436098, 0.1835660532428279, 0.2536069520137287, 0.30462117785830223, 0.444751757369504, 0.510977014118346, 0.5584100618542558, 0.6760724267021406, 0.9563513617530714], \n",
    "                              'answer_type_instructions': [0.12914539018869778, 0.2547374021123783, 0.3004133928631577, 0.3430771537748245, 0.47344102727934695, 0.5123180067788267, 0.6986051519696712, 0.9490846566348909, 0.9611641358159742, 0.9634507955866556, 0.9643617783194274],\n",
    "                              #'answer_type_procedure': [0.07283263806783744, 0.08673782158466102, 0.10797326502772303, 0.14305042199760273, 0.18652053202933983, 0.2589035024488969, 0.3142348956297047, 0.3884851946263015], \n",
    "                              'answer_type_procedure': [0.05262500290391218, 0.068643509793028, \n",
    "                                                        0.0875365114214299, 0.12193730591853633, \n",
    "                                                        0.12859167991175083, 0.1749735190696107, \n",
    "                                                        0.23268544708591288, 0.2853473228116521, \n",
    "                                                        0.3081688498590655, 0.3726692764096011, \n",
    "                                                        0.43835658963783947],\n",
    "                              #'answer_type_reason_explanation': [0.0719726353157682, 0.09497584034183688, 0.1922202264108104, 0.2927100843316804, 0.32682952727302356, 0.4546942172344034, 0.5492092734450725, 0.6414310974814582, 0.7712054310472717, 0.8032054448895015, 0.8963335925276644, 0.9275577809848018, 0.9604266579895407, 0.98558127380008], \n",
    "                              'answer_type_reason_explanation': [0.08911567475013671, 0.19586031922677827,\n",
    "                                                                 0.3244037433600754, 0.40260246629932767, \n",
    "                                                                 0.49025563870497624, 0.5843397430488557, \n",
    "                                                                 0.6629220619382283, 0.7781651886175698, \n",
    "                                                                 0.8225671776234381, 0.8974398999014135, \n",
    "                                                                 0.9245137338400247, 0.9565124730882079, \n",
    "                                                                 0.9669367106467958],\n",
    "                              'answer_well_written': [0.7790048186124875, 0.816189790201333, 0.8647582261050355, 0.8993758617055834, 0.9161059950859819, 0.9291888306671663, 0.9405892016200885, 0.9427343677958661, 0.9636116603983861]}\n",
    "\n",
    "            for col, thresholds in col_threshold1.items():\n",
    "                print('')\n",
    "                print(col)\n",
    "                score = spearmanr(train[col], oof[col]).correlation\n",
    "                print(score)\n",
    "                for i in range(len(thresholds)):\n",
    "                    if i==0:\n",
    "                        oof.loc[(oof[col]<=thresholds[0]), col] = 0\n",
    "                        submission.loc[(submission[col]<=thresholds[0]), col] = 0\n",
    "                    else:\n",
    "                        oof.loc[(oof[col]>thresholds[i-1]) & (oof[col]<=thresholds[i]), col] = thresholds[i-1]\n",
    "                        submission.loc[(submission[col]>thresholds[i-1]) & (submission[col]<=thresholds[i]), col] = thresholds[i-1]\n",
    "                score = spearmanr(train[col], oof[col]).correlation\n",
    "                print(score)\n",
    "            \n",
    "            col_threshold2 = {'question_has_commonly_accepted_answer': [0.7842707733624578],}\n",
    "            \n",
    "            for col, thresholds in col_threshold2.items():\n",
    "                print('')\n",
    "                print(col)\n",
    "                score = spearmanr(train[col], oof[col]).correlation\n",
    "                print(score)\n",
    "                for i in range(len(thresholds)):\n",
    "                    if i==0:\n",
    "                        oof.loc[(oof[col]>=thresholds[0]), col] = 1\n",
    "                        submission.loc[(submission[col]>=thresholds[0]), col] = 1\n",
    "                score = spearmanr(train[col], oof[col]).correlation\n",
    "                print(score)\n",
    "                \n",
    "            rho_val = np.mean([spearmanr(train[c], oof[c]).correlation for c in target_cols])\n",
    "            print(rho_val)\n",
    "            \n",
    "            ### To Avoid Error\n",
    "            if submission['question_type_spelling'].nunique()==1:\n",
    "                v1 = sub['question_type_spelling'].max()\n",
    "                v2 = sub[sub['question_type_spelling']!=v1]['question_type_spelling'].max()\n",
    "                print(v1, v2)\n",
    "                index1 = sub[sub['question_type_spelling']==v1].index\n",
    "                index2 = sub[sub['question_type_spelling']==v2].index\n",
    "                print(index1, index2)\n",
    "                submission.loc[index1, 'question_type_spelling'] = 0.66666667\n",
    "                submission.loc[index2, 'question_type_spelling'] = 0.33333333\n",
    "            \n",
    "            def min_max(x, axis=None):\n",
    "                _min = x.min(axis=axis, keepdims=True)\n",
    "                _max = x.max(axis=axis, keepdims=True)\n",
    "                result = (x-_min)/(_max-_min)\n",
    "                return result\n",
    "    \n",
    "            # 0 ~ 1\n",
    "            for col in target_cols:\n",
    "                submission[col] = min_max(submission[col].values, axis=None)\n",
    "                submission.loc[submission[col]==0, col] = 0.0000000000000000001\n",
    "                submission.loc[submission[col]==1, col] = 0.9999999999999999999\n",
    "                \n",
    "            submission.to_csv('submission.csv', index=False, float_format='%.20f')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
